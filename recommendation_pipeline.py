# -*- coding: utf-8 -*-
"""–ü–æ–ª–Ω–∞—è –≥–∏–±—Ä–∏–¥–Ω–∞—è —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ç–µ–ª—å–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ —Å –Ω–µ–π—Ä–æ—Å–µ—Ç–µ–≤—ã–º–∏ –º–µ—Ç–æ–¥–∞–º–∏"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import MinMaxScaler, StandardScaler
from sklearn.decomposition import TruncatedSVD
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.neighbors import NearestNeighbors
import warnings
warnings.filterwarnings('ignore')

# –î–ª—è –Ω–µ–π—Ä–æ—Å–µ—Ç–µ–π
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader, TensorDataset
from functools import lru_cache
import json
import pickle
import hashlib
from datetime import datetime
from wordcloud import WordCloud

print("=" * 100)
print("–ü–û–õ–ù–ê–Ø –ì–ò–ë–†–ò–î–ù–ê–Ø –†–ï–ö–û–ú–ï–ù–î–ê–¢–ï–õ–¨–ù–ê–Ø –°–ò–°–¢–ï–ú–ê –° –ù–ï–ô–†–û–°–ï–¢–ï–í–´–ú–ò –ú–ï–¢–û–î–ê–ú–ò")
print("=" * 100)
# ========================================================================
# 0. –ö–õ–ê–°–° –ö–≠–®–ò–†–û–í–ê–ù–ò–Ø –ò –í–ò–ó–£–ê–õ–ò–ó–ê–¶–ò–ò
# ========================================================================

class ComputationCache:
    """–ö–ª–∞—Å—Å –¥–ª—è –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –≤—ã—á–∏—Å–ª–µ–Ω–∏–π"""
    
    def __init__(self):
        self.cache = {}
        self.stats = {'hits': 0, 'misses': 0}
    
    def get_or_compute(self, key, compute_func, *args, **kwargs):
        """–ü–æ–ª—É—á–∏—Ç—å –∑–Ω–∞—á–µ–Ω–∏–µ –∏–∑ –∫—ç—à–∞ –∏–ª–∏ –≤—ã—á–∏—Å–ª–∏—Ç—å –∏ —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å"""
        if key in self.cache:
            self.stats['hits'] += 1
            return self.cache[key]
        else:
            self.stats['misses'] += 1
            result = compute_func(*args, **kwargs)
            self.cache[key] = result
            return result
    
    def clear(self):
        """–û—á–∏—Å—Ç–∏—Ç—å –∫—ç—à"""
        self.cache.clear()
        self.stats = {'hits': 0, 'misses': 0}
    
    def get_stats(self):
        """–ü–æ–ª—É—á–∏—Ç—å —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –∫—ç—à–∞"""
        total = self.stats['hits'] + self.stats['misses']
        hit_rate = self.stats['hits'] / total if total > 0 else 0
        return {
            'hits': self.stats['hits'],
            'misses': self.stats['misses'],
            'total': total,
            'hit_rate': hit_rate,
            'size': len(self.cache)
        }

class DataVisualizer:
    """–ö–ª–∞—Å—Å –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–π"""
    
    @staticmethod
    def create_subplot_grid(n_plots, title="", figsize=(15, 10)):
        """–°–æ–∑–¥–∞–µ—Ç —Å–µ—Ç–∫—É –≥—Ä–∞—Ñ–∏–∫–æ–≤"""
        rows = int(np.ceil(np.sqrt(n_plots)))
        cols = int(np.ceil(n_plots / rows))
        fig, axes = plt.subplots(rows, cols, figsize=figsize)
        if n_plots == 1:
            axes = np.array([axes])
        axes = axes.flatten()
        
        # –£–±–∏—Ä–∞–µ–º –ª–∏—à–Ω–∏–µ –æ—Å–∏
        for i in range(n_plots, len(axes)):
            fig.delaxes(axes[i])
        
        fig.suptitle(title, fontsize=16, fontweight='bold')
        return fig, axes[:n_plots]
    
    @staticmethod
    def plot_distribution(data, ax, title, xlabel, ylabel="–ß–∞—Å—Ç–æ—Ç–∞", color='skyblue', log_scale=False):
        """–ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è"""
        if log_scale:
            ax.hist(data, bins=50, edgecolor='black', alpha=0.7, color=color, log=True)
        else:
            ax.hist(data, bins=50, edgecolor='black', alpha=0.7, color=color)
        ax.set_title(title, fontsize=12, fontweight='bold')
        ax.set_xlabel(xlabel, fontsize=10)
        ax.set_ylabel(ylabel, fontsize=10)
        ax.grid(True, alpha=0.3)
    
    @staticmethod
    def plot_bar(values, labels, ax, title, xlabel="", ylabel="", color=None):
        """–ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ —Å—Ç–æ–ª–±—á–∞—Ç–æ–π –¥–∏–∞–≥—Ä–∞–º–º—ã"""
        if color is None:
            color = plt.cm.Set3(range(len(values)))
        
        bars = ax.bar(range(len(values)), values, color=color, edgecolor='black')
        ax.set_title(title, fontsize=12, fontweight='bold')
        ax.set_xlabel(xlabel, fontsize=10)
        ax.set_ylabel(ylabel, fontsize=10)
        ax.set_xticks(range(len(values)))
        ax.set_xticklabels(labels, rotation=45, ha='right', fontsize=8)
        ax.grid(True, alpha=0.3, axis='y')
        
        # –î–æ–±–∞–≤–ª—è–µ–º –∑–Ω–∞—á–µ–Ω–∏—è –Ω–∞ —Å—Ç–æ–ª–±—Ü—ã
        for bar, value in zip(bars, values):
            height = bar.get_height()
            ax.text(bar.get_x() + bar.get_width()/2., height + max(values)*0.01,
                   f'{value}', ha='center', va='bottom', fontsize=8)
        
        return bars
    
    @staticmethod
    def plot_correlation_matrix(df, ax, title="–ú–∞—Ç—Ä–∏—Ü–∞ –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–π"):
        """–ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ —Ç–µ–ø–ª–æ–≤–æ–π –∫–∞—Ä—Ç—ã –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–π"""
        correlation = df.corr()
        sns.heatmap(correlation, annot=True, fmt='.2f', cmap='coolwarm', 
                   center=0, ax=ax, cbar_kws={'label': '–ö–æ—Ä—Ä–µ–ª—è—Ü–∏—è'})
        ax.set_title(title, fontsize=12, fontweight='bold')

# –ì–ª–æ–±–∞–ª—å–Ω—ã–π –∫—ç—à –¥–ª—è –ø–æ–≤—Ç–æ—Ä–Ω–æ–≥–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –≤—ã—á–∏—Å–ª–µ–Ω–∏–π
global_cache = ComputationCache()
visualizer = DataVisualizer()

# ========================================================================
# 1. –ó–ê–ì–†–£–ó–ö–ê –ò –†–ê–ó–í–ï–î–û–ß–ù–´–ô –ê–ù–ê–õ–ò–ó –î–ê–ù–ù–´–•
# ========================================================================

print("\n1. –ó–ê–ì–†–£–ó–ö–ê –ò –ê–ù–ê–õ–ò–ó –î–ê–ù–ù–´–•")
print("-" * 60)

def load_and_preprocess_data():
    """–ó–∞–≥—Ä—É–∑–∫–∞ –∏ –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –¥–∞–Ω–Ω—ã—Ö —Å –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ–º"""
    cache_key = "loaded_data"
    
    def load_data():
        ratings = pd.read_csv('goodbooks-10k/ratings.csv')
        books = pd.read_csv('goodbooks-10k/books.csv')
        book_tags = pd.read_csv('goodbooks-10k/book_tags.csv')
        tags = pd.read_csv('goodbooks-10k/tags.csv')
        to_read = pd.read_csv('goodbooks-10k/to_read.csv')
        
        # –§–∏–∫—Å–∏—Ä—É–µ–º —Å—Ç—Ä—É–∫—Ç—É—Ä—É books
        if 'id' in books.columns and 'book_id' not in books.columns:
            books = books.rename(columns={'id': 'book_id'})
        
        if 'goodreads_book_id' in book_tags.columns:
            book_tags = book_tags.rename(columns={'goodreads_book_id': 'book_id'})
        
        return ratings, books, book_tags, tags, to_read
    
    return global_cache.get_or_compute(cache_key, load_data)

# –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö —Å –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ–º
ratings, books, book_tags, tags, to_read = load_and_preprocess_data()

print(f"‚úì –ó–∞–≥—Ä—É–∂–µ–Ω–æ –¥–∞–Ω–Ω—ã—Ö:")
print(f"  ‚Ä¢ –û—Ü–µ–Ω–æ–∫: {len(ratings):,} –∑–∞–ø–∏—Å–µ–π")
print(f"  ‚Ä¢ –ö–Ω–∏–≥: {len(books):,} –∑–∞–ø–∏—Å–µ–π")
print(f"  ‚Ä¢ –¢–µ–≥–æ–≤ –∫–Ω–∏–≥: {len(book_tags):,} –∑–∞–ø–∏—Å–µ–π")
print(f"  ‚Ä¢ –ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π: {ratings['user_id'].nunique():,}")
print(f"  ‚Ä¢ –£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –∫–Ω–∏–≥: {ratings['book_id'].nunique():,}")

# ========================================================================
# 1.1 –í–ò–ó–£–ê–õ–ò–ó–ê–¶–ò–ò –ü–û–°–õ–ï –ó–ê–ì–†–£–ó–ö–ò –î–ê–ù–ù–´–•
# ========================================================================

print("\n1.1 –í–ò–ó–£–ê–õ–ò–ó–ê–¶–ò–ò –ü–û–°–õ–ï –ó–ê–ì–†–£–ó–ö–ò –î–ê–ù–ù–´–•")
print("-" * 60)

def visualize_initial_data():
    """–í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –Ω–∞—á–∞–ª—å–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö"""
    cache_key = "initial_visualizations"
    
    def create_visualizations():
        print("üé® –°–æ–∑–¥–∞–Ω–∏–µ –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–π –∑–∞–≥—Ä—É–∂–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö...")
        
        # –°–æ–∑–¥–∞–µ–º —Ñ–∏–≥—É—Ä—É —Å –Ω–µ—Å–∫–æ–ª—å–∫–∏–º–∏ –≥—Ä–∞—Ñ–∏–∫–∞–º–∏
        fig, axes = visualizer.create_subplot_grid(4, "–ê–ù–ê–õ–ò–ó –ó–ê–ì–†–£–ñ–ï–ù–ù–´–• –î–ê–ù–ù–´–•", figsize=(16, 10))
        
        # 1. –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –æ—Ü–µ–Ω–æ–∫
        rating_counts = ratings['rating'].value_counts().sort_index()
        visualizer.plot_bar(rating_counts.values, rating_counts.index.astype(str), 
                          axes[0], "–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –æ—Ü–µ–Ω–æ–∫", "–û—Ü–µ–Ω–∫–∞", "–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ")
        
        # 2. –ê–∫—Ç–∏–≤–Ω–æ—Å—Ç—å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π (–ª–æ–≥–∞—Ä–∏—Ñ–º–∏—á–µ—Å–∫–∞—è —à–∫–∞–ª–∞)
        user_activity = ratings.groupby('user_id').size()
        axes[1].hist(user_activity, bins=50, edgecolor='black', alpha=0.7, color='lightgreen', log=True)
        axes[1].set_title('–ê–∫—Ç–∏–≤–Ω–æ—Å—Ç—å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π (log scale)', fontsize=12, fontweight='bold')
        axes[1].set_xlabel('–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –æ—Ü–µ–Ω–æ–∫', fontsize=10)
        axes[1].set_ylabel('–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π', fontsize=10)
        axes[1].grid(True, alpha=0.3)
        
        # 3. –ü–æ–ø—É–ª—è—Ä–Ω–æ—Å—Ç—å –∫–Ω–∏–≥ (–ª–æ–≥–∞—Ä–∏—Ñ–º–∏—á–µ—Å–∫–∞—è —à–∫–∞–ª–∞)
        book_popularity = ratings.groupby('book_id').size()
        axes[2].hist(book_popularity, bins=50, edgecolor='black', alpha=0.7, color='salmon', log=True)
        axes[2].set_title('–ü–æ–ø—É–ª—è—Ä–Ω–æ—Å—Ç—å –∫–Ω–∏–≥ (log scale)', fontsize=12, fontweight='bold')
        axes[2].set_xlabel('–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –æ—Ü–µ–Ω–æ–∫', fontsize=10)
        axes[2].set_ylabel('–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–Ω–∏–≥', fontsize=10)
        axes[2].grid(True, alpha=0.3)
        
        # 4. Box plot –æ—Ü–µ–Ω–æ–∫
        rating_data = ratings['rating'].values
        box = axes[3].boxplot(rating_data, patch_artist=True, 
                            boxprops=dict(facecolor='lightblue', color='darkblue'),
                            medianprops=dict(color='red', linewidth=2))
        axes[3].set_title('Box plot –æ—Ü–µ–Ω–æ–∫', fontsize=12, fontweight='bold')
        axes[3].set_ylabel('–û—Ü–µ–Ω–∫–∞', fontsize=10)
        axes[3].grid(True, alpha=0.3)
        
        # –î–æ–±–∞–≤–ª—è–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
        stats_text = f"–ú–µ–¥–∏–∞–Ω–∞: {np.median(rating_data):.2f}\n–°—Ä–µ–¥–Ω–µ–µ: {np.mean(rating_data):.2f}"
        axes[3].text(0.7, 0.95, stats_text, transform=axes[3].transAxes, 
                    fontsize=9, verticalalignment='top',
                    bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))
        
        plt.tight_layout()
        plt.show()
        
        # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏
        print("\nüìä –°–¢–ê–¢–ò–°–¢–ò–ö–ò –î–ê–ù–ù–´–•:")
        print(f"  ‚Ä¢ –°—Ä–µ–¥–Ω—è—è –æ—Ü–µ–Ω–∫–∞: {ratings['rating'].mean():.2f}")
        print(f"  ‚Ä¢ –ú–µ–¥–∏–∞–Ω–Ω–∞—è –æ—Ü–µ–Ω–∫–∞: {ratings['rating'].median():.2f}")
        print(f"  ‚Ä¢ –°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–µ –æ—Ç–∫–ª–æ–Ω–µ–Ω–∏–µ –æ—Ü–µ–Ω–æ–∫: {ratings['rating'].std():.2f}")
        print(f"  ‚Ä¢ –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –æ—Ü–µ–Ω–∫–∞: {ratings['rating'].max()}")
        print(f"  ‚Ä¢ –ú–∏–Ω–∏–º–∞–ª—å–Ω–∞—è –æ—Ü–µ–Ω–∫–∞: {ratings['rating'].min()}")
        
        # –ê–Ω–∞–ª–∏–∑ —Ä–∞–∑—Ä–µ–∂–µ–Ω–Ω–æ—Å—Ç–∏
        total_possible_ratings = ratings['user_id'].nunique() * ratings['book_id'].nunique()
        actual_ratings = len(ratings)
        sparsity = 1 - (actual_ratings / total_possible_ratings)
        
        print(f"\nüî¢ –ê–ù–ê–õ–ò–ó –†–ê–ó–†–ï–ñ–ï–ù–ù–û–°–¢–ò:")
        print(f"  ‚Ä¢ –í—Å–µ–≥–æ –≤–æ–∑–º–æ–∂–Ω—ã—Ö –æ—Ü–µ–Ω–æ–∫: {total_possible_ratings:,}")
        print(f"  ‚Ä¢ –§–∞–∫—Ç–∏—á–µ—Å–∫–∏—Ö –æ—Ü–µ–Ω–æ–∫: {actual_ratings:,}")
        print(f"  ‚Ä¢ –ó–∞–ø–æ–ª–Ω–µ–Ω–Ω–æ—Å—Ç—å –º–∞—Ç—Ä–∏—Ü—ã: {actual_ratings/total_possible_ratings*100:.6f}%")
        print(f"  ‚Ä¢ –†–∞–∑—Ä–µ–∂–µ–Ω–Ω–æ—Å—Ç—å: {sparsity*100:.6f}%")
        
        return True
    
    return global_cache.get_or_compute(cache_key, create_visualizations)

# –°–æ–∑–¥–∞–µ–º –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏
visualize_initial_data()


# ========================================================================
# 1.3 –°–¢–ê–¢–ò–°–¢–ò–ß–ï–°–ö–ò–ô –ê–ù–ê–õ–ò–ó
# ========================================================================

print("\n\n1.3 –°–¢–ê–¢–ò–°–¢–ò–ß–ï–°–ö–ò–ô –ê–ù–ê–õ–ò–ó –î–ê–ù–ù–´–•")
print("-" * 60)

def perform_statistical_analysis():
    """–í—ã–ø–æ–ª–Ω–µ–Ω–∏–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞"""
    cache_key = "statistical_analysis"
    
    def analyze():
        print("üìà –ü—Ä–æ–≤–µ–¥–µ–Ω–∏–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞...")
        
        analysis_results = {}
        
        # 1. –û—Å–Ω–æ–≤–Ω—ã–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –æ—Ü–µ–Ω–æ–∫
        rating_stats = ratings['rating'].describe()
        analysis_results['rating_stats'] = rating_stats
        
        print(f"\nüìä –û–°–ù–û–í–ù–´–ï –°–¢–ê–¢–ò–°–¢–ò–ö–ò –û–¶–ï–ù–û–ö:")
        print(f"  ‚Ä¢ –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ: {rating_stats['count']:,}")
        print(f"  ‚Ä¢ –°—Ä–µ–¥–Ω–µ–µ: {rating_stats['mean']:.2f}")
        print(f"  ‚Ä¢ –°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–µ –æ—Ç–∫–ª–æ–Ω–µ–Ω–∏–µ: {rating_stats['std']:.2f}")
        print(f"  ‚Ä¢ –ú–∏–Ω–∏–º—É–º: {rating_stats['min']}")
        print(f"  ‚Ä¢ 25% –ø–µ—Ä—Ü–µ–Ω—Ç–∏–ª—å: {rating_stats['25%']}")
        print(f"  ‚Ä¢ –ú–µ–¥–∏–∞–Ω–∞: {rating_stats['50%']}")
        print(f"  ‚Ä¢ 75% –ø–µ—Ä—Ü–µ–Ω—Ç–∏–ª—å: {rating_stats['75%']}")
        print(f"  ‚Ä¢ –ú–∞–∫—Å–∏–º—É–º: {rating_stats['max']}")
        
        # 2. –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –∞–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π
        user_activity = ratings.groupby('user_id').size()
        user_stats = user_activity.describe()
        analysis_results['user_stats'] = user_stats
        
        print(f"\nüë• –°–¢–ê–¢–ò–°–¢–ò–ö–ò –ê–ö–¢–ò–í–ù–û–°–¢–ò –ü–û–õ–¨–ó–û–í–ê–¢–ï–õ–ï–ô:")
        print(f"  ‚Ä¢ –í—Å–µ–≥–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π: {len(user_activity):,}")
        print(f"  ‚Ä¢ –°—Ä–µ–¥–Ω–µ–µ –æ—Ü–µ–Ω–æ–∫ –Ω–∞ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è: {user_stats['mean']:.1f}")
        print(f"  ‚Ä¢ –ú–µ–¥–∏–∞–Ω–∞ –æ—Ü–µ–Ω–æ–∫ –Ω–∞ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è: {user_stats['50%']:.0f}")
        print(f"  ‚Ä¢ –ú–∞–∫—Å–∏–º—É–º –æ—Ü–µ–Ω–æ–∫: {user_stats['max']:,}")
        print(f"  ‚Ä¢ –ú–∏–Ω–∏–º—É–º –æ—Ü–µ–Ω–æ–∫: {user_stats['min']:,}")
        
        # 3. –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –ø–æ–ø—É–ª—è—Ä–Ω–æ—Å—Ç–∏ –∫–Ω–∏–≥
        book_popularity = ratings.groupby('book_id').size()
        book_stats = book_popularity.describe()
        analysis_results['book_stats'] = book_stats
        
        print(f"\nüìö –°–¢–ê–¢–ò–°–¢–ò–ö–ò –ü–û–ü–£–õ–Ø–†–ù–û–°–¢–ò –ö–ù–ò–ì:")
        print(f"  ‚Ä¢ –í—Å–µ–≥–æ –∫–Ω–∏–≥: {len(book_popularity):,}")
        print(f"  ‚Ä¢ –°—Ä–µ–¥–Ω–µ–µ –æ—Ü–µ–Ω–æ–∫ –Ω–∞ –∫–Ω–∏–≥—É: {book_stats['mean']:.1f}")
        print(f"  ‚Ä¢ –ú–µ–¥–∏–∞–Ω–∞ –æ—Ü–µ–Ω–æ–∫ –Ω–∞ –∫–Ω–∏–≥—É: {book_stats['50%']:.0f}")
        print(f"  ‚Ä¢ –ú–∞–∫—Å–∏–º—É–º –æ—Ü–µ–Ω–æ–∫: {book_stats['max']:,}")
        print(f"  ‚Ä¢ –ú–∏–Ω–∏–º—É–º –æ—Ü–µ–Ω–æ–∫: {book_stats['min']:,}")
        
        # 4. –ê–Ω–∞–ª–∏–∑ —Ö–æ–ª–æ–¥–Ω–æ–≥–æ —Å—Ç–∞—Ä—Ç–∞
        # –ö–Ω–∏–≥–∏ —Å –º–∞–ª—ã–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ–º –æ—Ü–µ–Ω–æ–∫ (–ø—Ä–æ–±–ª–µ–º–∞ —Ö–æ–ª–æ–¥–Ω–æ–≥–æ —Å—Ç–∞—Ä—Ç–∞)
        cold_start_books = book_popularity[book_popularity <= 5]
        cold_start_ratio = len(cold_start_books) / len(book_popularity) * 100
        analysis_results['cold_start'] = {
            'count': len(cold_start_books),
            'ratio': cold_start_ratio
        }
        
        print(f"\n‚ùÑÔ∏è –ê–ù–ê–õ–ò–ó –ü–†–û–ë–õ–ï–ú–´ –•–û–õ–û–î–ù–û–ì–û –°–¢–ê–†–¢–ê:")
        print(f"  ‚Ä¢ –ö–Ω–∏–≥ —Å ‚â§5 –æ—Ü–µ–Ω–∫–∞–º–∏: {len(cold_start_books):,}")
        print(f"  ‚Ä¢ –î–æ–ª—è —Ç–∞–∫–∏—Ö –∫–Ω–∏–≥: {cold_start_ratio:.1f}%")
        
        # 5. –ê–Ω–∞–ª–∏–∑ —Ä–∞–∑—Ä–µ–∂–µ–Ω–Ω–æ—Å—Ç–∏ –º–∞—Ç—Ä–∏—Ü—ã
        total_possible_ratings = ratings['user_id'].nunique() * ratings['book_id'].nunique()
        actual_ratings = len(ratings)
        sparsity = 1 - (actual_ratings / total_possible_ratings)
        density = actual_ratings / total_possible_ratings * 100
        
        analysis_results['matrix_stats'] = {
            'total_possible': total_possible_ratings,
            'actual': actual_ratings,
            'sparsity': sparsity,
            'density': density
        }
        
        print(f"\nüî¢ –°–¢–ê–¢–ò–°–¢–ò–ö–ò –ú–ê–¢–†–ò–¶–´ –û–¶–ï–ù–û–ö:")
        print(f"  ‚Ä¢ –í—Å–µ–≥–æ –≤–æ–∑–º–æ–∂–Ω—ã—Ö –æ—Ü–µ–Ω–æ–∫: {total_possible_ratings:,}")
        print(f"  ‚Ä¢ –§–∞–∫—Ç–∏—á–µ—Å–∫–∏—Ö –æ—Ü–µ–Ω–æ–∫: {actual_ratings:,}")
        print(f"  ‚Ä¢ –ó–∞–ø–æ–ª–Ω–µ–Ω–Ω–æ—Å—Ç—å –º–∞—Ç—Ä–∏—Ü—ã: {density:.6f}%")
        print(f"  ‚Ä¢ –†–∞–∑—Ä–µ–∂–µ–Ω–Ω–æ—Å—Ç—å: {sparsity*100:.6f}%")
        
        # 6. –ê–Ω–∞–ª–∏–∑ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –ø–æ –≤—Ä–µ–º–µ–Ω–∏ (–µ—Å–ª–∏ –µ—Å—Ç—å –≤—Ä–µ–º–µ–Ω–Ω—ã–µ –º–µ—Ç–∫–∏)
        # –í –Ω–∞—à–µ–º –Ω–∞–±–æ—Ä–µ –¥–∞–Ω–Ω—ã—Ö –∏—Ö –Ω–µ—Ç, –Ω–æ –µ—Å–ª–∏ –±—ã –±—ã–ª–∏:
        # if 'timestamp' in ratings.columns:
        #     ratings['date'] = pd.to_datetime(ratings['timestamp'], unit='s')
        #     monthly_ratings = ratings.set_index('date').resample('M').size()
        
        return analysis_results
    
    return global_cache.get_or_compute(cache_key, analyze)

# –í—ã–ø–æ–ª–Ω—è–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–∏–π –∞–Ω–∞–ª–∏–∑
stat_analysis = perform_statistical_analysis()


# ========================================================================
# 1.4 –ê–ù–ê–õ–ò–ó –ê–ù–û–ú–ê–õ–ò–ô –ò –í–´–ë–†–û–°–û–í
# ========================================================================

print("\n\n1.4 –ê–ù–ê–õ–ò–ó –ê–ù–û–ú–ê–õ–ò–ô –ò –í–´–ë–†–û–°–û–í")
print("-" * 60)

def analyze_anomalies():
    """–ê–Ω–∞–ª–∏–∑ –∞–Ω–æ–º–∞–ª–∏–π –∏ –≤—ã–±—Ä–æ—Å–æ–≤ –≤ –¥–∞–Ω–Ω—ã—Ö"""
    print("üîç –ê–Ω–∞–ª–∏–∑ –∞–Ω–æ–º–∞–ª–∏–π –∏ –≤—ã–±—Ä–æ—Å–æ–≤...")
    
    # 1. –ê–Ω–æ–º–∞–ª—å–Ω—ã–µ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–∏ (—Å–ª–∏—à–∫–æ–º –º–Ω–æ–≥–æ/–º–∞–ª–æ –æ—Ü–µ–Ω–æ–∫)
    user_activity = ratings.groupby('user_id').size()
    Q1_user = user_activity.quantile(0.25)
    Q3_user = user_activity.quantile(0.75)
    IQR_user = Q3_user - Q1_user
    user_outliers = user_activity[
        (user_activity < (Q1_user - 1.5 * IQR_user)) | 
        (user_activity > (Q3_user + 1.5 * IQR_user))
    ]
    
    print(f"\nüë§ –ê–ù–û–ú–ê–õ–¨–ù–´–ï –ü–û–õ–¨–ó–û–í–ê–¢–ï–õ–ò (–º–µ—Ç–æ–¥ IQR):")
    print(f"  ‚Ä¢ –í—ã—è–≤–ª–µ–Ω–æ –∞–Ω–æ–º–∞–ª–∏–π: {len(user_outliers):,}")
    print(f"  ‚Ä¢ –î–æ–ª—è –∞–Ω–æ–º–∞–ª—å–Ω—ã—Ö –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π: {len(user_outliers)/len(user_activity)*100:.2f}%")
    
    if len(user_outliers) > 0:
        print(f"  ‚Ä¢ –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –∞–∫—Ç–∏–≤–Ω–æ—Å—Ç—å —É –∞–Ω–æ–º–∞–ª–∏–∏: {user_outliers.max():,} –æ—Ü–µ–Ω–æ–∫")
        print(f"  ‚Ä¢ –ú–∏–Ω–∏–º–∞–ª—å–Ω–∞—è –∞–∫—Ç–∏–≤–Ω–æ—Å—Ç—å —É –∞–Ω–æ–º–∞–ª–∏–∏: {user_outliers.min():,} –æ—Ü–µ–Ω–æ–∫")
    
    # 2. –ê–Ω–æ–º–∞–ª—å–Ω—ã–µ –∫–Ω–∏–≥–∏ (—Å–ª–∏—à–∫–æ–º –º–Ω–æ–≥–æ/–º–∞–ª–æ –æ—Ü–µ–Ω–æ–∫)
    book_popularity = ratings.groupby('book_id').size()
    Q1_book = book_popularity.quantile(0.25)
    Q3_book = book_popularity.quantile(0.75)
    IQR_book = Q3_book - Q1_book
    book_outliers = book_popularity[
        (book_popularity < (Q1_book - 1.5 * IQR_book)) | 
        (book_popularity > (Q3_book + 1.5 * IQR_book))
    ]
    
    print(f"\nüìö –ê–ù–û–ú–ê–õ–¨–ù–´–ï –ö–ù–ò–ì–ò (–º–µ—Ç–æ–¥ IQR):")
    print(f"  ‚Ä¢ –í—ã—è–≤–ª–µ–Ω–æ –∞–Ω–æ–º–∞–ª–∏–π: {len(book_outliers):,}")
    print(f"  ‚Ä¢ –î–æ–ª—è –∞–Ω–æ–º–∞–ª—å–Ω—ã—Ö –∫–Ω–∏–≥: {len(book_outliers)/len(book_popularity)*100:.2f}%")
    
    # 3. –ê–Ω–æ–º–∞–ª—å–Ω—ã–µ –æ—Ü–µ–Ω–∫–∏
    rating_values = ratings['rating'].value_counts().sort_index()
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –æ—Ü–µ–Ω–∫–∏ –≤–Ω–µ –¥–æ–ø—É—Å—Ç–∏–º–æ–≥–æ –¥–∏–∞–ø–∞–∑–æ–Ω–∞ (0-5)
    invalid_ratings = ratings[~ratings['rating'].between(0, 5)]
    
    print(f"\n‚≠ê –ê–ù–ê–õ–ò–ó –û–¶–ï–ù–û–ö:")
    print(f"  ‚Ä¢ –í—Å–µ–≥–æ –æ—Ü–µ–Ω–æ–∫: {len(ratings):,}")
    if not invalid_ratings.empty:
        print(f"  ‚Ä¢ –ù–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã—Ö –æ—Ü–µ–Ω–æ–∫: {len(invalid_ratings):,}")
        print(f"  ‚Ä¢ –î–æ–ª—è –Ω–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã—Ö –æ—Ü–µ–Ω–æ–∫: {len(invalid_ratings)/len(ratings)*100:.4f}%")
    else:
        print(f"  ‚Ä¢ –ù–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã—Ö –æ—Ü–µ–Ω–æ–∫: 0 (–≤—Å–µ –≤ –¥–∏–∞–ø–∞–∑–æ–Ω–µ 0-5)")
    
    # 4. –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –≤—ã–±—Ä–æ—Å–æ–≤
    fig, axes = plt.subplots(1, 2, figsize=(14, 6))
    
    # Box plot –∞–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π
    axes[0].boxplot(user_activity.values, vert=False)
    axes[0].set_title('–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∞–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π', fontsize=14, fontweight='bold')
    axes[0].set_xlabel('–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –æ—Ü–µ–Ω–æ–∫', fontsize=12)
    axes[0].grid(True, alpha=0.3)
    
    # Box plot –ø–æ–ø—É–ª—è—Ä–Ω–æ—Å—Ç–∏ –∫–Ω–∏–≥
    axes[1].boxplot(book_popularity.values, vert=False)
    axes[1].set_title('–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø–æ–ø—É–ª—è—Ä–Ω–æ—Å—Ç–∏ –∫–Ω–∏–≥', fontsize=14, fontweight='bold')
    axes[1].set_xlabel('–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –æ—Ü–µ–Ω–æ–∫', fontsize=12)
    axes[1].grid(True, alpha=0.3)
    
    plt.suptitle('–ê–ù–ê–õ–ò–ó –í–´–ë–†–û–°–û–í –í –î–ê–ù–ù–´–•', fontsize=16, fontweight='bold')
    plt.tight_layout()
    plt.show()
    
    return {
        'user_outliers': user_outliers,
        'book_outliers': book_outliers,
        'invalid_ratings': invalid_ratings
    }

# –í—ã–ø–æ–ª–Ω—è–µ–º –∞–Ω–∞–ª–∏–∑ –∞–Ω–æ–º–∞–ª–∏–π
anomalies = analyze_anomalies()

# ========================================================================
# 1.5 –ê–ù–ê–õ–ò–ó –ö–ê–ß–ï–°–¢–í–ê –î–ê–ù–ù–´–• –ò –ü–†–û–ü–£–©–ï–ù–ù–´–• –ó–ù–ê–ß–ï–ù–ò–ô
# ========================================================================

print("\n\n1.5 –ê–ù–ê–õ–ò–ó –ö–ê–ß–ï–°–¢–í–ê –î–ê–ù–ù–´–• –ò –ü–†–û–ü–£–©–ï–ù–ù–´–• –ó–ù–ê–ß–ï–ù–ò–ô")
print("-" * 60)

def analyze_data_quality():
    """–ê–Ω–∞–ª–∏–∑ –∫–∞—á–µ—Å—Ç–≤–∞ –¥–∞–Ω–Ω—ã—Ö –∏ –ø—Ä–æ–ø—É—â–µ–Ω–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π"""
    print("üß™ –ê–Ω–∞–ª–∏–∑ –∫–∞—á–µ—Å—Ç–≤–∞ –¥–∞–Ω–Ω—ã—Ö...")
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ –ø—Ä–æ–ø—É—â–µ–Ω–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π –≤ –∫–∞–∂–¥–æ–º –¥–∞—Ç–∞—Å–µ—Ç–µ
    datasets = {
        'ratings': ratings,
        'books': books,
        'book_tags': book_tags,
        'tags': tags,
        'to_read': to_read
    }
    
    quality_report = {}
    
    for name, df in datasets.items():
        print(f"\nüìã –ê–Ω–∞–ª–∏–∑ –¥–∞—Ç–∞—Å–µ—Ç–∞ '{name}':")
        print(f"  ‚Ä¢ –†–∞–∑–º–µ—Ä: {df.shape[0]} —Å—Ç—Ä–æ–∫ √ó {df.shape[1]} –∫–æ–ª–æ–Ω–æ–∫")
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –ø—Ä–æ–ø—É—â–µ–Ω–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è
        missing_values = df.isnull().sum()
        missing_total = missing_values.sum()
        missing_percentage = (missing_total / (df.shape[0] * df.shape[1])) * 100
        
        print(f"  ‚Ä¢ –í—Å–µ–≥–æ –ø—Ä–æ–ø—É—â–µ–Ω–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π: {missing_total:,}")
        print(f"  ‚Ä¢ –î–æ–ª—è –ø—Ä–æ–ø—É—â–µ–Ω–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π: {missing_percentage:.2f}%")
        
        if missing_total > 0:
            print(f"  ‚Ä¢ –ö–æ–ª–æ–Ω–∫–∏ —Å –ø—Ä–æ–ø—É—Å–∫–∞–º–∏:")
            for col, count in missing_values[missing_values > 0].items():
                perc = (count / df.shape[0]) * 100
                print(f"    - {col}: {count:,} ({perc:.2f}%)")
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –¥—É–±–ª–∏–∫–∞—Ç—ã
        duplicates = df.duplicated().sum()
        print(f"  ‚Ä¢ –î—É–±–ª–∏–∫–∞—Ç–æ–≤: {duplicates:,}")
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π
        print(f"  ‚Ä¢ –£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π –ø–æ –∫–æ–ª–æ–Ω–∫–∞–º:")
        for col in df.columns[:5]:  # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º —Ç–æ–ª—å–∫–æ –ø–µ—Ä–≤—ã–µ 5 –∫–æ–ª–æ–Ω–æ–∫
            unique_count = df[col].nunique()
            print(f"    - {col}: {unique_count:,}")
        
        quality_report[name] = {
            'shape': df.shape,
            'missing_total': missing_total,
            'missing_percentage': missing_percentage,
            'duplicates': duplicates
        }
    
    # –ê–Ω–∞–ª–∏–∑ —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç–∏ –¥–∞–Ω–Ω—ã—Ö –º–µ–∂–¥—É –¥–∞—Ç–∞—Å–µ—Ç–∞–º–∏
    print(f"\nüîó –ê–ù–ê–õ–ò–ó –°–û–ì–õ–ê–°–û–í–ê–ù–ù–û–°–¢–ò –î–ê–ù–ù–´–• –ú–ï–ñ–î–£ –î–ê–¢–ê–°–ï–¢–ê–ú–ò:")
    
    # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç–∏ book_id –º–µ–∂–¥—É ratings –∏ books
    books_in_ratings = set(ratings['book_id'].unique())
    books_in_books = set(books['book_id'].unique()) if 'book_id' in books.columns else set()
    
    if books_in_books:
        common_books = books_in_ratings.intersection(books_in_books)
        only_in_ratings = books_in_ratings - books_in_books
        only_in_books = books_in_books - books_in_ratings
        
        print(f"  ‚Ä¢ –û–±—â–∏–µ –∫–Ω–∏–≥–∏ –≤ ratings –∏ books: {len(common_books):,}")
        print(f"  ‚Ä¢ –ö–Ω–∏–≥–∏ —Ç–æ–ª—å–∫–æ –≤ ratings: {len(only_in_ratings):,}")
        print(f"  ‚Ä¢ –ö–Ω–∏–≥–∏ —Ç–æ–ª—å–∫–æ –≤ books: {len(only_in_books):,}")
        print(f"  ‚Ä¢ Coverage (books –≤ ratings / books –≤ books): {len(common_books)/len(books_in_books)*100:.2f}%")
    
    # –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –∫–∞—á–µ—Å—Ç–≤–∞ –¥–∞–Ω–Ω—ã—Ö
    fig, axes = plt.subplots(2, 2, figsize=(14, 10))
    
    # 1. –ü—Ä–æ–ø—É—â–µ–Ω–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è –ø–æ –¥–∞—Ç–∞—Å–µ—Ç–∞–º
    ax1 = axes[0, 0]
    dataset_names = list(quality_report.keys())
    missing_percentages = [quality_report[name]['missing_percentage'] for name in dataset_names]
    
    bars1 = ax1.bar(dataset_names, missing_percentages, color=plt.cm.tab10(range(len(dataset_names))))
    ax1.set_title('–î–æ–ª—è –ø—Ä–æ–ø—É—â–µ–Ω–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π –ø–æ –¥–∞—Ç–∞—Å–µ—Ç–∞–º', fontsize=14, fontweight='bold')
    ax1.set_ylabel('–ü—Ä–æ—Ü–µ–Ω—Ç –ø—Ä–æ–ø—É—Å–∫–æ–≤ (%)', fontsize=12)
    ax1.set_xticklabels(dataset_names, rotation=45, ha='right')
    ax1.grid(True, alpha=0.3, axis='y')
    
    for bar, perc in zip(bars1, missing_percentages):
        height = bar.get_height()
        ax1.text(bar.get_x() + bar.get_width()/2., height + 0.5,
                f'{perc:.2f}%', ha='center', va='bottom', fontsize=10)
    
    # 2. –†–∞–∑–º–µ—Ä—ã –¥–∞—Ç–∞—Å–µ—Ç–æ–≤ (–ª–æ–≥–∞—Ä–∏—Ñ–º–∏—á–µ—Å–∫–∞—è —à–∫–∞–ª–∞)
    ax2 = axes[0, 1]
    dataset_sizes = [quality_report[name]['shape'][0] for name in dataset_names]
    
    bars2 = ax2.bar(dataset_names, dataset_sizes, color=plt.cm.Set2(range(len(dataset_names))))
    ax2.set_title('–†–∞–∑–º–µ—Ä—ã –¥–∞—Ç–∞—Å–µ—Ç–æ–≤ (–ª–æ–≥–∞—Ä–∏—Ñ–º–∏—á–µ—Å–∫–∞—è —à–∫–∞–ª–∞)', fontsize=14, fontweight='bold')
    ax2.set_ylabel('–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å—Ç—Ä–æ–∫', fontsize=12)
    ax2.set_xticklabels(dataset_names, rotation=45, ha='right')
    ax2.set_yscale('log')
    ax2.grid(True, alpha=0.3, axis='y')
    
    for bar, size in zip(bars2, dataset_sizes):
        height = bar.get_height()
        ax2.text(bar.get_x() + bar.get_width()/2., height * 1.05,
                f'{size:,}', ha='center', va='bottom', fontsize=10)
    
    # 3. –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤
    ax3 = axes[1, 0]
    duplicates_counts = [quality_report[name]['duplicates'] for name in dataset_names]
    
    bars3 = ax3.bar(dataset_names, duplicates_counts, color=plt.cm.Set3(range(len(dataset_names))))
    ax3.set_title('–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤ –ø–æ –¥–∞—Ç–∞—Å–µ—Ç–∞–º', fontsize=14, fontweight='bold')
    ax3.set_ylabel('–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤', fontsize=12)
    ax3.set_xticklabels(dataset_names, rotation=45, ha='right')
    ax3.grid(True, alpha=0.3, axis='y')
    
    for bar, count in zip(bars3, duplicates_counts):
        height = bar.get_height()
        ax3.text(bar.get_x() + bar.get_width()/2., height + max(duplicates_counts)*0.01,
                f'{count:,}', ha='center', va='bottom', fontsize=10)
    
    # 4. –ì–∏—Å—Ç–æ–≥—Ä–∞–º–º–∞ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π –≤ ratings
    ax4 = axes[1, 1]
    ratings_nunique = ratings.nunique()
    top_columns = ratings_nunique.nlargest(10)
    
    bars4 = ax4.bar(top_columns.index, top_columns.values, color=plt.cm.Pastel1(range(len(top_columns))))
    ax4.set_title('–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π –≤ ratings', fontsize=14, fontweight='bold')
    ax4.set_ylabel('–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π', fontsize=12)
    ax4.set_xticklabels(top_columns.index, rotation=45, ha='right')
    ax4.grid(True, alpha=0.3, axis='y')
    
    for bar, value in zip(bars4, top_columns.values):
        height = bar.get_height()
        ax4.text(bar.get_x() + bar.get_width()/2., height + max(top_columns.values)*0.01,
                f'{value:,}', ha='center', va='bottom', fontsize=9, rotation=0)
    
    plt.suptitle('–ê–ù–ê–õ–ò–ó –ö–ê–ß–ï–°–¢–í–ê –î–ê–ù–ù–´–•', fontsize=16, fontweight='bold')
    plt.tight_layout()
    plt.show()
    
    return quality_report

# –í—ã–ø–æ–ª–Ω—è–µ–º –∞–Ω–∞–ª–∏–∑ –∫–∞—á–µ—Å—Ç–≤–∞ –¥–∞–Ω–Ω—ã—Ö
quality_report = analyze_data_quality()

print("\n" + "="*100)
print("‚úÖ –ê–ù–ê–õ–ò–ó –î–ê–ù–ù–´–• –ó–ê–í–ï–†–®–ï–ù!")
print("="*100)

# ========================================================================
# 2. –°–û–ó–î–ê–ù–ò–ï –†–ê–°–®–ò–†–ï–ù–ù–´–• –ü–†–ò–ó–ù–ê–ö–û–í
# ========================================================================

print("\n\n2. –°–û–ó–î–ê–ù–ò–ï –†–ê–°–®–ò–†–ï–ù–ù–´–• –ü–†–ò–ó–ù–ê–ö–û–í")
print("-" * 60)

class FeatureBuilder:
    """–ö–ª–∞—Å—Å –¥–ª—è –ø–æ—Å—Ç—Ä–æ–µ–Ω–∏—è –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ —Å –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ–º –ø—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω—ã—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤"""
    
    def __init__(self, ratings, books, book_tags, tags, to_read):
        self.ratings = ratings
        self.books = books
        self.book_tags = book_tags
        self.tags = tags
        self.to_read = to_read
        self.cache = {}
        
    def build_book_features(self):
        """–ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–ª—è –∫–Ω–∏–≥ —Å –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ–º"""
        cache_key = "book_features"
        if cache_key in self.cache:
            return self.cache[cache_key]
        
        print("–°–æ–∑–¥–∞–Ω–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–ª—è –∫–Ω–∏–≥...")
        
        # –ë–∞–∑–æ–≤—ã–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –∏–∑ ratings
        book_stats = self.ratings.groupby('book_id').agg({
            'rating': ['mean', 'std', 'count', 'min', 'max'],
            'user_id': 'nunique'
        }).reset_index()
        book_stats.columns = ['book_id', 'avg_rating', 'rating_std', 'rating_count', 'min_rating', 'max_rating', 'unique_users']
        
        # –û–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ —Å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π –æ –∫–Ω–∏–≥–∞—Ö
        if 'book_id' in self.books.columns:
            book_info_cols = []
            for col in ['title', 'authors', 'original_publication_year', 'language_code', 'average_rating', 'ratings_count']:
                if col in self.books.columns:
                    book_info_cols.append(col)
            
            if book_info_cols:
                book_info = self.books[['book_id'] + book_info_cols].copy()
                book_info = book_info.drop_duplicates(subset='book_id')
                book_stats = pd.merge(book_stats, book_info, on='book_id', how='left')
        
        # –î–æ–±–∞–≤–ª–µ–Ω–∏–µ —Ç–µ–≥–æ–≤
        if 'book_id' in self.book_tags.columns:
            book_tags_merged = pd.merge(self.book_tags, self.tags, on='tag_id', how='left')
            top_tags_per_book = book_tags_merged.groupby('book_id').apply(
                lambda x: ' '.join(x.nlargest(10, 'count')['tag_name'].fillna('').tolist())
            ).reset_index(name='top_tags')
            book_stats = pd.merge(book_stats, top_tags_per_book, on='book_id', how='left')
        
        book_stats['top_tags'] = book_stats['top_tags'].fillna('')
        
        # TF-IDF –ø—Ä–∏–∑–Ω–∞–∫–∏ –∏–∑ –Ω–∞–∑–≤–∞–Ω–∏–π (–∫—ç—à–∏—Ä—É–µ–º –æ—Ç–¥–µ–ª—å–Ω–æ)
        tfidf_features = self._build_tfidf_features(book_stats)
        book_stats = pd.concat([book_stats, tfidf_features], axis=1)
        
        # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è —á–∏—Å–ª–æ–≤—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
        book_stats = self._normalize_features(book_stats, prefix='book')
        
        self.cache[cache_key] = book_stats.fillna(0)
        
        print(f"‚úì –°–æ–∑–¥–∞–Ω–æ {len(book_stats.columns)} –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–ª—è {len(book_stats)} –∫–Ω–∏–≥")
        
        return self.cache[cache_key]
    
    def _build_tfidf_features(self, book_stats):
        """–ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ TF-IDF –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ —Å –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ–º"""
        cache_key = f"tfidf_features_{hash(str(book_stats['book_id'].tolist()[:10]))}"
        
        if cache_key in self.cache:
            return self.cache[cache_key]
        
        print("  –°–æ–∑–¥–∞–Ω–∏–µ TF-IDF –ø—Ä–∏–∑–Ω–∞–∫–æ–≤...")
        
        tfidf_results = pd.DataFrame(index=book_stats.index)
        
        # –ü—Ä–∏–∑–Ω–∞–∫–∏ –∏–∑ –Ω–∞–∑–≤–∞–Ω–∏–π
        if 'title' in book_stats.columns and len(book_stats['title'].dropna()) > 0:
            titles = book_stats['title'].fillna('').astype(str)
            tfidf = TfidfVectorizer(max_features=50, stop_words='english')
            title_tfidf = tfidf.fit_transform(titles)
            svd = TruncatedSVD(n_components=10, random_state=42)
            title_features = svd.fit_transform(title_tfidf)
            
            for i in range(10):
                tfidf_results[f'title_svd_{i}'] = title_features[:, i]
        
        # –ü—Ä–∏–∑–Ω–∞–∫–∏ –∏–∑ —Ç–µ–≥–æ–≤
        if 'top_tags' in book_stats.columns and len(book_stats['top_tags'].dropna()) > 0:
            tags_text = book_stats['top_tags'].fillna('').astype(str)
            tfidf_tags = TfidfVectorizer(max_features=30, stop_words='english')
            tags_tfidf = tfidf_tags.fit_transform(tags_text)
            svd_tags = TruncatedSVD(n_components=10, random_state=42)
            tags_features = svd_tags.fit_transform(tags_tfidf)
            
            for i in range(10):
                tfidf_results[f'tags_svd_{i}'] = tags_features[:, i]
        
        self.cache[cache_key] = tfidf_results
        return tfidf_results
    
    def _normalize_features(self, df, prefix=''):
        """–ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è —á–∏—Å–ª–æ–≤—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤"""
        numeric_cols = []
        for col in df.columns:
            if df[col].dtype in [np.float64, np.int64] and 'svd_' not in col and 'scaled' not in col:
                if col not in ['book_id', 'user_id']:
                    numeric_cols.append(col)
        
        if numeric_cols and len(df) > 1:
            scaler = StandardScaler()
            valid_cols = []
            data_to_scale = []
            
            for col in numeric_cols:
                if col in df.columns and len(df[col].dropna()) > 0:
                    mean_val = df[col].mean()
                    if not pd.isna(mean_val):
                        df[col] = df[col].fillna(mean_val)
                        valid_cols.append(col)
                        data_to_scale.append(df[col].values)
            
            if valid_cols:
                data_to_scale = np.column_stack(data_to_scale)
                scaled = scaler.fit_transform(data_to_scale)
                
                for i, col in enumerate(valid_cols):
                    df[f'{col}_scaled'] = scaled[:, i]
        
        return df
    
    def build_user_features(self):
        """–ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–ª—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π —Å –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ–º"""
        cache_key = "user_features"
        if cache_key in self.cache:
            return self.cache[cache_key]
        
        print("–°–æ–∑–¥–∞–Ω–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–ª—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π...")
        
        # –ë–∞–∑–æ–≤—ã–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏
        user_stats = self.ratings.groupby('user_id').agg({
            'rating': ['mean', 'std', 'count', 'min', 'max'],
            'book_id': 'nunique'
        }).reset_index()
        user_stats.columns = ['user_id', 'mean_rating', 'rating_std', 'total_ratings', 'min_rating', 'max_rating', 'unique_books']
        
        # –î–æ–±–∞–≤–ª–µ–Ω–∏–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ –∫–Ω–∏–≥–∞—Ö "to read"
        if 'to_read' in locals() and 'user_id' in self.to_read.columns:
            to_read_counts = self.to_read.groupby('user_id').size().reset_index(name='to_read_count')
            user_stats = pd.merge(user_stats, to_read_counts, on='user_id', how='left')
            user_stats['to_read_count'] = user_stats['to_read_count'].fillna(0)
        
        # –ü—Ä–æ—Ñ–∏–ª—å –∏–Ω—Ç–µ—Ä–µ—Å–æ–≤ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
        if 'book_tags' in locals() and 'book_id' in self.book_tags.columns:
            book_tags_merged = pd.merge(self.book_tags, self.tags, on='tag_id', how='left')
            user_book_ratings = pd.merge(self.ratings[['user_id', 'book_id', 'rating']], 
                                       book_tags_merged[['book_id', 'tag_name']],
                                       on='book_id', how='left')
            
            user_top_tags = user_book_ratings.groupby('user_id')['tag_name'].apply(
                lambda x: ' '.join(x.dropna().value_counts().head(10).index.tolist())
            ).reset_index()
            user_top_tags.columns = ['user_id', 'top_tags']
            
            user_stats = pd.merge(user_stats, user_top_tags, on='user_id', how='left')
            user_stats['top_tags'] = user_stats['top_tags'].fillna('')
        
        # TF-IDF –ø—Ä–∏–∑–Ω–∞–∫–∏ –∏–∑ —Ç–µ–≥–æ–≤ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π
        if 'top_tags' in user_stats.columns and len(user_stats['top_tags'].dropna()) > 0:
            tfidf_user_tags = TfidfVectorizer(max_features=20, stop_words='english')
            user_tags_text = user_stats['top_tags'].fillna('').astype(str)
            user_tags_tfidf = tfidf_user_tags.fit_transform(user_tags_text)
            svd_user_tags = TruncatedSVD(n_components=10, random_state=42)
            user_tags_features = svd_user_tags.fit_transform(user_tags_tfidf)
            
            for i in range(10):
                user_stats[f'user_tags_svd_{i}'] = user_tags_features[:, i]
        
        # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è
        user_stats = self._normalize_features(user_stats, prefix='user')
        
        self.cache[cache_key] = user_stats.fillna(0)
        
        print(f"‚úì –°–æ–∑–¥–∞–Ω–æ {len(user_stats.columns)} –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–ª—è {len(user_stats)} –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π")
        
        return self.cache[cache_key]

# –°–æ–∑–¥–∞–µ–º –∏ –∏—Å–ø–æ–ª—å–∑—É–µ–º –±–∏–ª–¥–µ—Ä –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
feature_builder = FeatureBuilder(ratings, books, book_tags, tags, to_read)
book_stats = feature_builder.build_book_features()
user_stats = feature_builder.build_user_features()

# ========================================================================
# 2.1 –í–ò–ó–£–ê–õ–ò–ó–ê–¶–ò–ò –ü–û–°–õ–ï –°–û–ó–î–ê–ù–ò–Ø –ü–†–ò–ó–ù–ê–ö–û–í
# ========================================================================

print("\n2.1 –í–ò–ó–£–ê–õ–ò–ó–ê–¶–ò–ò –ü–†–ò–ó–ù–ê–ö–û–í")
print("-" * 60)

def visualize_features():
    """–í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è —Å–æ–∑–¥–∞–Ω–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤"""
    cache_key = "feature_visualizations"
    
    def create_visualizations():
        print("üìä –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –ø—Ä–∏–∑–Ω–∞–∫–æ–≤...")
        
        # –í—ã–±–∏—Ä–∞–µ–º —á–∏—Å–ª–æ–≤—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ –¥–ª—è –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏
        book_numeric_features = []
        user_numeric_features = []
        
        for col in book_stats.columns:
            if book_stats[col].dtype in [np.float64, np.int64] and 'book_id' not in col:
                if len(book_stats[col].unique()) > 5:
                    book_numeric_features.append(col)
        
        for col in user_stats.columns:
            if user_stats[col].dtype in [np.float64, np.int64] and 'user_id' not in col:
                if len(user_stats[col].unique()) > 5:
                    user_numeric_features.append(col)
        
        # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–ª—è –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏
        book_numeric_features = book_numeric_features[:6]
        user_numeric_features = user_numeric_features[:6]
        
        # –°–æ–∑–¥–∞–µ–º —Ñ–∏–≥—É—Ä—É
        fig, axes = plt.subplots(2, 3, figsize=(15, 8))
        axes = axes.flatten()
        
        # –í–∏–∑—É–∞–ª–∏–∑–∏—Ä—É–µ–º –ø—Ä–∏–∑–Ω–∞–∫–∏ –∫–Ω–∏–≥
        for i, feature in enumerate(book_numeric_features[:3]):
            ax = axes[i]
            if i < len(book_numeric_features):
                ax.hist(book_stats[feature].dropna(), bins=30, edgecolor='black', alpha=0.7, color='skyblue')
                ax.set_title(f'–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ {feature}', fontsize=11, fontweight='bold')
                ax.set_xlabel(feature, fontsize=9)
                ax.set_ylabel('–ß–∞—Å—Ç–æ—Ç–∞', fontsize=9)
                ax.grid(True, alpha=0.3)
        
        # –í–∏–∑—É–∞–ª–∏–∑–∏—Ä—É–µ–º –ø—Ä–∏–∑–Ω–∞–∫–∏ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π
        for i, feature in enumerate(user_numeric_features[:3]):
            ax = axes[i + 3]
            if i < len(user_numeric_features):
                ax.hist(user_stats[feature].dropna(), bins=30, edgecolor='black', alpha=0.7, color='lightgreen')
                ax.set_title(f'–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ {feature}', fontsize=11, fontweight='bold')
                ax.set_xlabel(feature, fontsize=9)
                ax.set_ylabel('–ß–∞—Å—Ç–æ—Ç–∞', fontsize=9)
                ax.grid(True, alpha=0.3)
        
        # –£–±–∏—Ä–∞–µ–º –ª–∏—à–Ω–∏–µ –æ—Å–∏
        for i in range(len(book_numeric_features[:3]) + len(user_numeric_features[:3]), len(axes)):
            fig.delaxes(axes[i])
        
        fig.suptitle('–†–ê–°–ü–†–ï–î–ï–õ–ï–ù–ò–ï –ü–†–ò–ó–ù–ê–ö–û–í –ö–ù–ò–ì –ò –ü–û–õ–¨–ó–û–í–ê–¢–ï–õ–ï–ô', fontsize=14, fontweight='bold')
        plt.tight_layout()
        plt.show()
        
        # –ö–æ—Ä—Ä–µ–ª—è—Ü–∏–æ–Ω–Ω–∞—è –º–∞—Ç—Ä–∏—Ü–∞ –¥–ª—è –∫–Ω–∏–≥
        if len(book_numeric_features) > 2:
            print("\nüîó –ö–æ—Ä—Ä–µ–ª—è—Ü–∏–æ–Ω–Ω–∞—è –º–∞—Ç—Ä–∏—Ü–∞ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –∫–Ω–∏–≥:")
            
            # –í—ã–±–∏—Ä–∞–µ–º —Ç–æ–ø –∫–æ—Ä—Ä–µ–ª–∏—Ä—É—é—â–∏—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
            book_corr = book_stats[book_numeric_features[:8]].corr()
            
            fig, ax = plt.subplots(figsize=(10, 8))
            sns.heatmap(book_corr, annot=True, fmt='.2f', cmap='coolwarm', 
                       center=0, ax=ax, cbar_kws={'label': '–ö–æ—Ä—Ä–µ–ª—è—Ü–∏—è'})
            ax.set_title('–ö–æ—Ä—Ä–µ–ª—è—Ü–∏—è –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –∫–Ω–∏–≥', fontsize=14, fontweight='bold')
            plt.tight_layout()
            plt.show()
            
            # –ù–∞—Ö–æ–¥–∏–º –Ω–∞–∏–±–æ–ª–µ–µ –∫–æ—Ä—Ä–µ–ª–∏—Ä—É—é—â–∏–µ –ø–∞—Ä—ã
            print("\nüìà –¢–æ–ø-5 –Ω–∞–∏–±–æ–ª–µ–µ –∫–æ—Ä—Ä–µ–ª–∏—Ä—É—é—â–∏—Ö –ø–∞—Ä –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –∫–Ω–∏–≥:")
            corr_pairs = []
            for i in range(len(book_corr.columns)):
                for j in range(i+1, len(book_corr.columns)):
                    corr_val = abs(book_corr.iloc[i, j])
                    if corr_val > 0.5:  # –ü–æ—Ä–æ–≥ –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–∏
                        corr_pairs.append((book_corr.columns[i], book_corr.columns[j], corr_val))
            
            corr_pairs.sort(key=lambda x: x[2], reverse=True)
            for i, (feat1, feat2, corr) in enumerate(corr_pairs[:5]):
                print(f"  {i+1}. {feat1} ‚Üî {feat2}: {corr:.3f}")
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
        print("\nüìä –°–¢–ê–¢–ò–°–¢–ò–ö–ê –ü–†–ò–ó–ù–ê–ö–û–í:")
        print(f"  ‚Ä¢ –ü—Ä–∏–∑–Ω–∞–∫–æ–≤ –∫–Ω–∏–≥: {len(book_stats.columns)}")
        print(f"  ‚Ä¢ –ü—Ä–∏–∑–Ω–∞–∫–æ–≤ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π: {len(user_stats.columns)}")
        print(f"  ‚Ä¢ –ß–∏—Å–ª–æ–≤—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –∫–Ω–∏–≥: {len([c for c in book_stats.columns if book_stats[c].dtype in [np.float64, np.int64]])}")
        print(f"  ‚Ä¢ –ß–∏—Å–ª–æ–≤—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π: {len([c for c in user_stats.columns if user_stats[c].dtype in [np.float64, np.int64]])}")
        
        return True
    
    return global_cache.get_or_compute(cache_key, create_visualizations)

# –°–æ–∑–¥–∞–µ–º –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
visualize_features()

# ========================================================================
# 3. –†–ê–ó–î–ï–õ–ï–ù–ò–ï –î–ê–ù–ù–´–• –ò –ü–û–î–ì–û–¢–û–í–ö–ê –ú–ê–¢–†–ò–¶
# ========================================================================

print("\n\n3. –†–ê–ó–î–ï–õ–ï–ù–ò–ï –î–ê–ù–ù–´–•")
print("-" * 60)

def prepare_data_matrices():
    """–ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –º–∞—Ç—Ä–∏—Ü –¥–∞–Ω–Ω—ã—Ö —Å –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ–º"""
    cache_key = "data_matrices"
    
    def prepare_matrices():
        print("–†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –Ω–∞ train –∏ test...")
        train_data, test_data = train_test_split(ratings, test_size=0.2, random_state=42, 
                                                stratify=ratings['user_id'])
        
        # –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –¥–ª—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏
        user_counts = train_data['user_id'].value_counts()
        active_users = user_counts[user_counts >= 5].index
        
        book_counts = train_data['book_id'].value_counts()
        popular_books = book_counts[book_counts >= 10].index
        
        train_filtered = train_data[
            train_data['user_id'].isin(active_users) & 
            train_data['book_id'].isin(popular_books)
        ]
        
        # –°–æ–∑–¥–∞–Ω–∏–µ –º–∞—Ç—Ä–∏—Ü
        train_matrix = train_filtered.pivot_table(
            index='user_id',
            columns='book_id',
            values='rating',
            fill_value=0
        )
        
        item_user_matrix = train_filtered.pivot_table(
            index='book_id',
            columns='user_id',
            values='rating',
            fill_value=0
        )
        
        # –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è —Ç–µ—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö
        test_filtered = test_data[
            test_data['user_id'].isin(train_filtered['user_id']) & 
            test_data['book_id'].isin(train_filtered['book_id'])
        ]
        
        return {
            'train_data': train_data,
            'test_data': test_data,
            'train_filtered': train_filtered,
            'test_filtered': test_filtered,
            'train_matrix': train_matrix,
            'item_user_matrix': item_user_matrix
        }
    
    return global_cache.get_or_compute(cache_key, prepare_matrices)

data_matrices = prepare_data_matrices()
train_data = data_matrices['train_data']
test_data = data_matrices['test_data']
train_filtered = data_matrices['train_filtered']
test_filtered = data_matrices['test_filtered']
train_matrix = data_matrices['train_matrix']
item_user_matrix = data_matrices['item_user_matrix']

print(f"‚úì –î–∞–Ω–Ω—ã–µ –ø–æ–¥–≥–æ—Ç–æ–≤–ª–µ–Ω—ã:")
print(f"  ‚Ä¢ Train: {len(train_data):,} –∑–∞–ø–∏—Å–µ–π")
print(f"  ‚Ä¢ Test: {len(test_data):,} –∑–∞–ø–∏—Å–µ–π")
print(f"  ‚Ä¢ Train (—Ñ–∏–ª—å—Ç—Ä.): {len(train_filtered):,} –∑–∞–ø–∏—Å–µ–π")
print(f"  ‚Ä¢ –ú–∞—Ç—Ä–∏—Ü–∞ train: {train_matrix.shape}")


# ========================================================================
# 3.1 –í–ò–ó–£–ê–õ–ò–ó–ê–¶–ò–ò –ü–û–°–õ–ï –†–ê–ó–î–ï–õ–ï–ù–ò–Ø –î–ê–ù–ù–´–•
# ========================================================================

print("\n3.1 –í–ò–ó–£–ê–õ–ò–ó–ê–¶–ò–ò –†–ê–ó–î–ï–õ–ï–ù–ù–´–• –î–ê–ù–ù–´–•")
print("-" * 60)

def visualize_split_data():
    """–í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è —Ä–∞–∑–¥–µ–ª–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö"""
    cache_key = "split_data_visualizations"
    
    def create_visualizations():
        print("üìä –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è —Ä–∞–∑–¥–µ–ª–µ–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö...")
        
        # –°–æ–∑–¥–∞–µ–º —Ñ–∏–≥—É—Ä—É
        fig, axes = plt.subplots(2, 2, figsize=(12, 10))
        axes = axes.flatten()
        
        # 1. –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –æ—Ü–µ–Ω–æ–∫ –≤ train –∏ test
        axes[0].hist(train_data['rating'], bins=5, alpha=0.7, label='Train', color='skyblue', edgecolor='black')
        axes[0].hist(test_data['rating'], bins=5, alpha=0.7, label='Test', color='salmon', edgecolor='black')
        axes[0].set_title('–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –æ—Ü–µ–Ω–æ–∫ –≤ train/test', fontsize=12, fontweight='bold')
        axes[0].set_xlabel('–û—Ü–µ–Ω–∫–∞', fontsize=10)
        axes[0].set_ylabel('–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ', fontsize=10)
        axes[0].legend()
        axes[0].grid(True, alpha=0.3)
        
        # 2. –†–∞–∑–º–µ—Ä—ã –¥–∞–Ω–Ω—ã—Ö
        sizes = [len(train_data), len(test_data), len(train_filtered), len(test_filtered)]
        labels = ['Train (–≤–µ—Å—å)', 'Test (–≤–µ—Å—å)', 'Train (—Ñ–∏–ª—å—Ç—Ä.)', 'Test (—Ñ–∏–ª—å—Ç—Ä.)']
        
        bars = axes[1].bar(range(len(sizes)), sizes, color=plt.cm.Set3(range(len(sizes))))
        axes[1].set_title('–†–∞–∑–º–µ—Ä—ã –≤—ã–±–æ—Ä–æ–∫ –¥–∞–Ω–Ω—ã—Ö', fontsize=12, fontweight='bold')
        axes[1].set_xlabel('–í—ã–±–æ—Ä–∫–∞', fontsize=10)
        axes[1].set_ylabel('–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∑–∞–ø–∏—Å–µ–π', fontsize=10)
        axes[1].set_xticks(range(len(sizes)))
        axes[1].set_xticklabels(labels, rotation=45, ha='right', fontsize=9)
        axes[1].grid(True, alpha=0.3, axis='y')
        
        # –î–æ–±–∞–≤–ª—è–µ–º –∑–Ω–∞—á–µ–Ω–∏—è –Ω–∞ —Å—Ç–æ–ª–±—Ü—ã
        for bar, size in zip(bars, sizes):
            height = bar.get_height()
            axes[1].text(bar.get_x() + bar.get_width()/2., height + max(sizes)*0.01,
                        f'{size:,}', ha='center', va='bottom', fontsize=9)
        
        # 3. –ü–æ–∫—Ä—ã—Ç–∏–µ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π –∏ –∫–Ω–∏–≥
        if train_matrix is not None:
            coverage_data = [
                train_matrix.shape[0],  # –ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–∏ –≤ train
                train_matrix.shape[1],  # –ö–Ω–∏–≥–∏ –≤ train
                len(set(train_filtered['user_id']).intersection(set(test_filtered['user_id']))),  # –û–±—â–∏–µ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–∏
                len(set(train_filtered['book_id']).intersection(set(test_filtered['book_id'])))   # –û–±—â–∏–µ –∫–Ω–∏–≥–∏
            ]
            
            coverage_labels = ['–ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–∏ –≤ train', '–ö–Ω–∏–≥–∏ –≤ train', '–û–±—â–∏–µ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–∏', '–û–±—â–∏–µ –∫–Ω–∏–≥–∏']
            
            bars2 = axes[2].bar(range(len(coverage_data)), coverage_data, color=plt.cm.Set2(range(len(coverage_data))))
            axes[2].set_title('–ü–æ–∫—Ä—ã—Ç–∏–µ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π –∏ –∫–Ω–∏–≥', fontsize=12, fontweight='bold')
            axes[2].set_xlabel('–ö–∞—Ç–µ–≥–æ—Ä–∏—è', fontsize=10)
            axes[2].set_ylabel('–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ', fontsize=10)
            axes[2].set_xticks(range(len(coverage_data)))
            axes[2].set_xticklabels(coverage_labels, rotation=45, ha='right', fontsize=9)
            axes[2].grid(True, alpha=0.3, axis='y')
            
            # –î–æ–±–∞–≤–ª—è–µ–º –∑–Ω–∞—á–µ–Ω–∏—è –Ω–∞ —Å—Ç–æ–ª–±—Ü—ã
            for bar, value in zip(bars2, coverage_data):
                height = bar.get_height()
                axes[2].text(bar.get_x() + bar.get_width()/2., height + max(coverage_data)*0.01,
                           f'{value:,}', ha='center', va='bottom', fontsize=9)
        
        # 4. –†–∞–∑—Ä–µ–∂–µ–Ω–Ω–æ—Å—Ç—å –º–∞—Ç—Ä–∏—Ü—ã train
        if train_matrix is not None:
            total_cells = train_matrix.shape[0] * train_matrix.shape[1]
            non_zero = np.count_nonzero(train_matrix.values)
            sparsity = 1 - (non_zero / total_cells)
            
            labels_sparsity = ['–ó–∞–ø–æ–ª–Ω–µ–Ω–Ω—ã–µ', '–ü—É—Å—Ç—ã–µ']
            sizes_sparsity = [non_zero, total_cells - non_zero]
            colors_sparsity = ['lightgreen', 'lightcoral']
            
            axes[3].pie(sizes_sparsity, labels=labels_sparsity, colors=colors_sparsity, 
                       autopct='%1.1f%%', startangle=90)
            axes[3].set_title(f'–†–∞–∑—Ä–µ–∂–µ–Ω–Ω–æ—Å—Ç—å –º–∞—Ç—Ä–∏—Ü—ã\n({non_zero/total_cells*100:.3f}% –∑–∞–ø–æ–ª–Ω–µ–Ω–æ)', 
                            fontsize=12, fontweight='bold')
        
        fig.suptitle('–ê–ù–ê–õ–ò–ó –†–ê–ó–î–ï–õ–ï–ù–ù–´–• –î–ê–ù–ù–´–•', fontsize=14, fontweight='bold')
        plt.tight_layout()
        plt.show()
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ —Ä–∞–∑–¥–µ–ª–µ–Ω–∏—è
        print("\nüìä –°–¢–ê–¢–ò–°–¢–ò–ö–ê –†–ê–ó–î–ï–õ–ï–ù–ò–Ø –î–ê–ù–ù–´–•:")
        print(f"  ‚Ä¢ Train/Test split: {len(train_data):,}/{len(test_data):,} –∑–∞–ø–∏—Å–µ–π")
        print(f"  ‚Ä¢ –ü–æ—Å–ª–µ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏: {len(train_filtered):,}/{len(test_filtered):,} –∑–∞–ø–∏—Å–µ–π")
        print(f"  ‚Ä¢ –ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π –≤ train: {train_filtered['user_id'].nunique():,}")
        print(f"  ‚Ä¢ –ö–Ω–∏–≥ –≤ train: {train_filtered['book_id'].nunique():,}")
        
        if train_matrix is not None:
            print(f"  ‚Ä¢ –†–∞–∑–º–µ—Ä –º–∞—Ç—Ä–∏—Ü—ã train: {train_matrix.shape}")
            print(f"  ‚Ä¢ –ó–∞–ø–æ–ª–Ω–µ–Ω–Ω–æ—Å—Ç—å –º–∞—Ç—Ä–∏—Ü—ã: {non_zero/total_cells*100:.3f}%")
            print(f"  ‚Ä¢ –†–∞–∑—Ä–µ–∂–µ–Ω–Ω–æ—Å—Ç—å: {sparsity*100:.3f}%")
        
        return True
    
    return global_cache.get_or_compute(cache_key, create_visualizations)

# –°–æ–∑–¥–∞–µ–º –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏ —Ä–∞–∑–¥–µ–ª–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
visualize_split_data()


# ========================================================================
# 4. –ë–ê–ó–û–í–´–ï –ú–û–î–ï–õ–ò –†–ï–ö–û–ú–ï–ù–î–ê–¶–ò–ô
# ========================================================================

print("\n\n4. –ë–ê–ó–û–í–´–ï –ú–û–î–ï–õ–ò –†–ï–ö–û–ú–ï–ù–î–ê–¶–ò–ô")
print("-" * 60)

class ModelFactory:
    """–§–∞–±—Ä–∏–∫–∞ –º–æ–¥–µ–ª–µ–π —Å –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ–º"""
    
    def __init__(self, train_filtered, book_stats, train_matrix, item_user_matrix):
        self.train_filtered = train_filtered
        self.book_stats = book_stats
        self.train_matrix = train_matrix
        self.item_user_matrix = item_user_matrix
        self.models = {}
        
    def get_popularity_model(self):
        """–ú–æ–¥–µ–ª—å –ø–æ–ø—É–ª—è—Ä–Ω–æ—Å—Ç–∏ —Å –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ–º"""
        if 'popularity' in self.models:
            return self.models['popularity']
        
        print("4.1 –ú–æ–¥–µ–ª—å –ø–æ–ø—É–ª—è—Ä–Ω–æ—Å—Ç–∏...")
        popularity_scores = self.train_filtered.groupby('book_id').agg({
            'rating': ['mean', 'count']
        }).reset_index()
        popularity_scores.columns = ['book_id', 'avg_rating', 'rating_count']
        
        # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è
        pop_scaler = MinMaxScaler()
        popularity_scores['norm_rating'] = pop_scaler.fit_transform(popularity_scores[['avg_rating']])
        popularity_scores['norm_count'] = pop_scaler.fit_transform(popularity_scores[['rating_count']])
        popularity_scores['popularity_score'] = 0.7 * popularity_scores['norm_rating'] + 0.3 * popularity_scores['norm_count']
        popularity_scores = popularity_scores.sort_values('popularity_score', ascending=False)
        
        self.models['popularity'] = popularity_scores
        print(f"‚úì –ú–æ–¥–µ–ª—å –ø–æ–ø—É–ª—è—Ä–Ω–æ—Å—Ç–∏ —Å–æ–∑–¥–∞–Ω–∞: {len(popularity_scores)} –∫–Ω–∏–≥")
        return popularity_scores
    
    def get_content_model(self):
        """–ö–æ–Ω—Ç–µ–Ω—Ç–Ω–∞—è –º–æ–¥–µ–ª—å —Å –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ–º"""
        if 'content' in self.models:
            return self.models['content']
        
        print("\n4.2 –ö–æ–Ω—Ç–µ–Ω—Ç–Ω–∞—è –º–æ–¥–µ–ª—å...")
        
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º —É–∂–µ —Å–æ–∑–¥–∞–Ω–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏
        content_features_list = []
        for col in self.book_stats.columns:
            if 'svd_' in col or 'scaled' in col:
                content_features_list.append(col)
        
        if content_features_list:
            book_ids_in_train = set(self.train_filtered['book_id'])
            book_stats_filtered = self.book_stats[self.book_stats['book_id'].isin(book_ids_in_train)]
            
            if len(book_stats_filtered) > 0:
                content_features_filtered = []
                for col in content_features_list:
                    if col in book_stats_filtered.columns:
                        content_features_filtered.append(book_stats_filtered[col].values)
                
                if content_features_filtered:
                    content_features_filtered = np.column_stack(content_features_filtered)
                    
                    n_neighbors = min(51, len(book_stats_filtered))
                    content_knn = NearestNeighbors(n_neighbors=n_neighbors, 
                                                 metric='cosine', algorithm='auto')
                    content_knn.fit(content_features_filtered)
                    
                    content_book_ids = book_stats_filtered['book_id'].tolist()
                    content_book_id_to_idx = {book_id: idx for idx, book_id in enumerate(content_book_ids)}
                    
                    model_data = {
                        'knn': content_knn,
                        'book_ids': content_book_ids,
                        'id_to_idx': content_book_id_to_idx,
                        'features': content_features_filtered
                    }
                    
                    self.models['content'] = model_data
                    print(f"‚úì –ö–æ–Ω—Ç–µ–Ω—Ç–Ω–∞—è –º–æ–¥–µ–ª—å —Å–æ–∑–¥–∞–Ω–∞")
                    return model_data
        
        self.models['content'] = None
        return None
    
    def get_item_based_model(self):
        """Item-Based –º–æ–¥–µ–ª—å —Å –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ–º"""
        if 'item_based' in self.models:
            return self.models['item_based']
        
        print("\n4.3 Item-Based Collaborative Filtering...")
        
        if self.item_user_matrix is not None and len(self.item_user_matrix) > 1:
            n_books_for_sim = min(500, len(self.item_user_matrix))
            popular_books_for_sim = self.item_user_matrix.index[:n_books_for_sim]
            item_user_matrix_filtered = self.item_user_matrix.loc[popular_books_for_sim]
            
            item_similarity = cosine_similarity(item_user_matrix_filtered.values)
            item_similarity_df = pd.DataFrame(
                item_similarity,
                index=item_user_matrix_filtered.index,
                columns=item_user_matrix_filtered.index
            )
            
            self.models['item_based'] = item_similarity_df
            print(f"‚úì Item-Based –º–æ–¥–µ–ª—å —Å–æ–∑–¥–∞–Ω–∞: {item_similarity_df.shape}")
            return item_similarity_df
        
        self.models['item_based'] = None
        return None
    
    def get_svd_model(self):
        """SVD –º–æ–¥–µ–ª—å —Å –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ–º"""
        if 'svd' in self.models:
            return self.models['svd']
        
        print("\n4.4 –ú–∞—Ç—Ä–∏—á–Ω–∞—è —Ñ–∞–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏—è (SVD)...")
        
        if self.train_matrix is not None and len(self.train_matrix) > 1:
            n_components = min(50, min(self.train_matrix.shape) - 1)
            if n_components > 0:
                svd = TruncatedSVD(n_components=n_components, random_state=42)
                train_matrix_svd = svd.fit_transform(self.train_matrix.values)
                
                svd_user_ids = self.train_matrix.index.tolist()
                svd_book_ids = self.train_matrix.columns.tolist()
                
                model_data = {
                    'svd': svd,
                    'matrix': train_matrix_svd,
                    'user_ids': svd_user_ids,
                    'book_ids': svd_book_ids
                }
                
                self.models['svd'] = model_data
                print(f"‚úì SVD –º–æ–¥–µ–ª—å —Å–æ–∑–¥–∞–Ω–∞: {train_matrix_svd.shape}")
                return model_data
        
        self.models['svd'] = None
        return None

# –°–æ–∑–¥–∞–µ–º –∏ –∏—Å–ø–æ–ª—å–∑—É–µ–º —Ñ–∞–±—Ä–∏–∫—É –º–æ–¥–µ–ª–µ–π
model_factory = ModelFactory(train_filtered, book_stats, train_matrix, item_user_matrix)
popularity_scores = model_factory.get_popularity_model()
content_model = model_factory.get_content_model()
item_similarity_df = model_factory.get_item_based_model()
svd_model_data = model_factory.get_svd_model()


# ========================================================================
# 5. –ì–ò–ë–†–ò–î–ù–ê–Ø –ú–û–î–ï–õ–¨ –° –û–ü–¢–ò–ú–ò–ó–ò–†–û–í–ê–ù–ù–´–ú–ò –í–´–ß–ò–°–õ–ï–ù–ò–Ø–ú–ò
# ========================================================================

print("\n\n5. –ì–ò–ë–†–ò–î–ù–ê–Ø –ú–û–î–ï–õ–¨ –° –û–ü–¢–ò–ú–ò–ó–ò–†–û–í–ê–ù–ù–´–ú–ò –í–´–ß–ò–°–õ–ï–ù–ò–Ø–ú–ò")
print("-" * 60)

class OptimizedHybridModel:
    """
    –û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –≥–∏–±—Ä–∏–¥–Ω–∞—è –º–æ–¥–µ–ª—å —Å —É—Å—Ç—Ä–∞–Ω–µ–Ω–∏–µ–º –ø–æ–≤—Ç–æ—Ä–Ω—ã—Ö –≤—ã—á–∏—Å–ª–µ–Ω–∏–π
    """
    
    def __init__(self, model_factory, book_stats, user_stats):
        self.model_factory = model_factory
        self.book_stats = book_stats
        self.user_stats = user_stats
        self.cache = {}
        
        # –ü—Ä–µ–¥–∑–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–µ–π
        self.popularity_model = model_factory.get_popularity_model()
        self.content_model = model_factory.get_content_model()
        self.item_based_model = model_factory.get_item_based_model()
        self.svd_model = model_factory.get_svd_model()
        
    def _get_cache_key(self, func_name, *args, **kwargs):
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –∫–ª—é—á–∞ –¥–ª—è –∫—ç—à–∞"""
        key_parts = [func_name]
        for arg in args:
            if isinstance(arg, (int, float, str)):
                key_parts.append(str(arg))
            elif isinstance(arg, (list, tuple)):
                key_parts.append(str(arg[:3]))
        return hashlib.md5('_'.join(key_parts).encode()).hexdigest()
    
    @lru_cache(maxsize=1000)
    def predict_popularity_cached(self, book_id):
        """–ö—ç—à–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –ø–æ–ø—É–ª—è—Ä–Ω–æ—Å—Ç–∏"""
        if self.popularity_model is not None:
            book_scores = self.popularity_model.set_index('book_id')['popularity_score']
            return book_scores.get(book_id, 0.0)
        return 0.0
    
    @lru_cache(maxsize=1000)
    def predict_content_cached(self, book_id, n_neighbors=10):
        """–ö—ç—à–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ –∫–æ–Ω—Ç–µ–Ω—Ç–Ω–æ–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ"""
        try:
            if self.content_model is not None:
                book_ids = self.content_model['book_ids']
                id_to_idx = self.content_model['id_to_idx']
                features = self.content_model['features']
                
                if book_id in id_to_idx:
                    idx = id_to_idx[book_id]
                    book_vector = features[idx].reshape(1, -1)
                    similarities = cosine_similarity(book_vector, features)[0]
                    
                    similar_indices = np.argsort(similarities)[-n_neighbors-1:-1]
                    avg_similarity = np.mean(similarities[similar_indices])
                    
                    return avg_similarity
        except Exception as e:
            print(f"–û—à–∏–±–∫–∞ –≤ predict_content: {e}")
        return 0.0
    
    @lru_cache(maxsize=1000)
    def predict_item_based_cached(self, book_id, n_neighbors=10):
        """–ö—ç—à–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ item-based –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ"""
        try:
            if self.item_based_model is not None and book_id in self.item_based_model.index:
                similarities = self.item_based_model.loc[book_id].values
                similar_indices = np.argsort(similarities)[-n_neighbors-1:-1]
                avg_similarity = np.mean(similarities[similar_indices])
                return avg_similarity
        except Exception as e:
            print(f"–û—à–∏–±–∫–∞ –≤ predict_item_based: {e}")
        return 0.0
    
    def predict_svd_cached(self, user_id, book_id):
        """–ö—ç—à–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ SVD –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ"""
        cache_key = f"svd_{user_id}_{book_id}"
        
        if cache_key in self.cache:
            return self.cache[cache_key]
        
        try:
            if self.svd_model is not None:
                user_ids = self.svd_model['user_ids']
                book_ids = self.svd_model['book_ids']
                
                if user_id in user_ids and book_id in book_ids:
                    user_idx = user_ids.index(user_id)
                    book_idx = book_ids.index(book_id)
                    
                    svd_matrix = self.svd_model['matrix']
                    if len(svd_matrix.shape) == 2:
                        # –£–ø—Ä–æ—â–µ–Ω–Ω–æ–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ
                        score = svd_matrix[user_idx, book_idx % svd_matrix.shape[1]]
                        self.cache[cache_key] = score
                        return score
        except Exception as e:
            print(f"–û—à–∏–±–∫–∞ –≤ predict_svd: {e}")
        
        self.cache[cache_key] = 0.0
        return 0.0
    
    def hybrid_predict(self, user_id, book_id, weights):
        """
        –ì–∏–±—Ä–∏–¥–Ω–æ–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ —Å –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ–º
        """
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫—ç—à
        cache_key = f"hybrid_{user_id}_{book_id}_{hash(str(weights))}"
        if cache_key in self.cache:
            return self.cache[cache_key]
        
        predictions = []
        
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º –∫—ç—à–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è
        if weights.get('popularity', 0) > 0:
            pop_score = self.predict_popularity_cached(book_id)
            predictions.append(pop_score * weights['popularity'])
        
        if weights.get('content', 0) > 0:
            content_score = self.predict_content_cached(book_id)
            predictions.append(content_score * weights['content'])
        
        if weights.get('item_based', 0) > 0:
            item_cf_score = self.predict_item_based_cached(book_id)
            predictions.append(item_cf_score * weights['item_based'])
        
        if weights.get('svd', 0) > 0:
            svd_score = self.predict_svd_cached(user_id, book_id)
            predictions.append(svd_score * weights['svd'])
        
        result = sum(predictions) if predictions else 0.0
        
        # –ö—ç—à–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç
        self.cache[cache_key] = result
        return result
    
    def evaluate_weights(self, weights, sample_size=500):
        """
        –ë—ã—Å—Ç—Ä–∞—è –æ—Ü–µ–Ω–∫–∞ –≤–µ—Å–æ–≤ —Å –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ–º
        """
        # –ö—ç—à–∏—Ä—É–µ–º –æ—Ü–µ–Ω–∫—É –¥–ª—è –¥–∞–Ω–Ω—ã—Ö –≤–µ—Å–æ–≤
        weights_hash = hash(json.dumps(weights, sort_keys=True))
        cache_key = f"evaluate_{weights_hash}_{sample_size}"
        
        if cache_key in self.cache:
            return self.cache[cache_key]
        
        try:
            if len(test_filtered) > sample_size:
                sample = test_filtered.sample(sample_size, random_state=42)
            else:
                sample = test_filtered
            
            predictions = []
            actuals = []
            
            # –í–µ–∫—Ç–æ—Ä–∏–∑–æ–≤–∞–Ω–Ω—ã–µ –≤—ã—á–∏—Å–ª–µ–Ω–∏—è, –≥–¥–µ –≤–æ–∑–º–æ–∂–Ω–æ
            for _, row in sample.iterrows():
                user_id = row['user_id']
                book_id = row['book_id']
                actual_rating = row['rating']
                
                pred_rating = self.hybrid_predict(user_id, book_id, weights)
                
                if pred_rating > 0:
                    pred_rating = min(5, max(0, pred_rating * 5))
                
                predictions.append(pred_rating)
                actuals.append(actual_rating)
            
            mse = np.mean([(p - a) ** 2 for p, a in zip(predictions, actuals)])
            rmse = np.sqrt(mse)
            
            self.cache[cache_key] = rmse
            return rmse
        
        except Exception as e:
            print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ—Ü–µ–Ω–∫–µ –≤–µ—Å–æ–≤: {e}")
            return float('inf')
    
    def optimize_weights_quick(self, n_iter=20):
        """
        –ë—ã—Å—Ç—Ä–∞—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –≤–µ—Å–æ–≤
        """
        print("\nüîç –ë—ã—Å—Ç—Ä–∞—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –≤–µ—Å–æ–≤...")
        
        best_weights = None
        best_rmse = float('inf')
        
        # –ü—Ä–æ–±—É–µ–º –Ω–µ—Å–∫–æ–ª—å–∫–æ —Å—Ç—Ä–∞—Ç–µ–≥–∏–π –≤–µ—Å–æ–≤
        weight_strategies = [
            {'popularity': 0.2, 'content': 0.3, 'item_based': 0.3, 'svd': 0.2},
            {'popularity': 0.1, 'content': 0.4, 'item_based': 0.3, 'svd': 0.2},
            {'popularity': 0.15, 'content': 0.25, 'item_based': 0.35, 'svd': 0.25},
            {'popularity': 0.3, 'content': 0.2, 'item_based': 0.25, 'svd': 0.25},
        ]
        
        # –î–æ–±–∞–≤–ª—è–µ–º —Å–ª—É—á–∞–π–Ω—ã–µ —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏
        for i in range(n_iter):
            if i >= len(weight_strategies):
                w1, w2, w3, w4 = np.random.dirichlet(np.ones(4), 1)[0]
                weight_strategies.append({
                    'popularity': w1,
                    'content': w2,
                    'item_based': w3,
                    'svd': w4
                })
        
        # –û—Ü–µ–Ω–∏–≤–∞–µ–º –≤—Å–µ —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏
        evaluation_results = []
        for i, weights in enumerate(weight_strategies):
            rmse = self.evaluate_weights(weights, sample_size=300)
            evaluation_results.append((weights, rmse))
            
            if rmse < best_rmse:
                best_rmse = rmse
                best_weights = weights.copy()
            
            if (i + 1) % 5 == 0:
                print(f"  –ü—Ä–æ–≤–µ—Ä–µ–Ω–æ {i+1}/{len(weight_strategies)} —Å—Ç—Ä–∞—Ç–µ–≥–∏–π...")
        
        # –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏
        self._visualize_optimization_results(evaluation_results)
        
        print(f"‚úì –í–µ—Å–∞ –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω—ã: {best_weights}")
        print(f"  –õ—É—á—à–∏–π RMSE: {best_rmse:.4f}")
        
        return best_weights, best_rmse
    
    def _visualize_optimization_results(self, evaluation_results):
        """–í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –≤–µ—Å–æ–≤"""
        print("\nüìà –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏...")
        
        # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏
        rmses = [rmse for _, rmse in evaluation_results]
        weights_data = []
        
        for weights, rmse in evaluation_results:
            weights_data.append({
                'popularity': weights['popularity'],
                'content': weights['content'],
                'item_based': weights['item_based'],
                'svd': weights['svd'],
                'rmse': rmse
            })
        
        weights_df = pd.DataFrame(weights_data)
        
        # –°–æ–∑–¥–∞–µ–º —Ñ–∏–≥—É—Ä—É
        fig, axes = plt.subplots(2, 2, figsize=(14, 10))
        axes = axes.flatten()
        
        # 1. –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ RMSE
        axes[0].hist(rmses, bins=20, edgecolor='black', alpha=0.7, color='skyblue')
        axes[0].set_title('–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ RMSE', fontsize=12, fontweight='bold')
        axes[0].set_xlabel('RMSE', fontsize=10)
        axes[0].set_ylabel('–ß–∞—Å—Ç–æ—Ç–∞', fontsize=10)
        axes[0].grid(True, alpha=0.3)
        
        # –î–æ–±–∞–≤–ª—è–µ–º –ª—É—á—à–∏–π RMSE
        best_rmse = min(rmses)
        axes[0].axvline(x=best_rmse, color='red', linestyle='--', linewidth=2)
        axes[0].text(best_rmse, axes[0].get_ylim()[1]*0.9, f'–õ—É—á—à–∏–π: {best_rmse:.3f}',
                    color='red', fontsize=9, ha='right')
        
        # 2. –ö–æ—Ä—Ä–µ–ª—è—Ü–∏—è –≤–µ—Å–æ–≤ —Å RMSE
        correlation_cols = ['popularity', 'content', 'item_based', 'svd']
        correlations = []
        for col in correlation_cols:
            corr = np.corrcoef(weights_df[col], weights_df['rmse'])[0, 1]
            correlations.append(abs(corr))
        
        bars = axes[1].bar(range(len(correlation_cols)), correlations, 
                          color=plt.cm.Set2(range(len(correlation_cols))))
        axes[1].set_title('–ö–æ—Ä—Ä–µ–ª—è—Ü–∏—è –≤–µ—Å–æ–≤ —Å RMSE', fontsize=12, fontweight='bold')
        axes[1].set_xlabel('–í–µ—Å –º–æ–¥–µ–ª–∏', fontsize=10)
        axes[1].set_ylabel('|–ö–æ—Ä—Ä–µ–ª—è—Ü–∏—è —Å RMSE|', fontsize=10)
        axes[1].set_xticks(range(len(correlation_cols)))
        axes[1].set_xticklabels(correlation_cols, rotation=45, ha='right', fontsize=9)
        axes[1].grid(True, alpha=0.3, axis='y')
        
        # 3. Scatter plot: –≤–µ—Å –ø–æ–ø—É–ª—è—Ä–Ω–æ—Å—Ç–∏ vs RMSE
        axes[2].scatter(weights_df['popularity'], weights_df['rmse'], 
                       alpha=0.6, s=30, c=weights_df['rmse'], cmap='viridis')
        axes[2].set_title('–í–µ—Å –ø–æ–ø—É–ª—è—Ä–Ω–æ—Å—Ç–∏ vs RMSE', fontsize=12, fontweight='bold')
        axes[2].set_xlabel('–í–µ—Å –ø–æ–ø—É–ª—è—Ä–Ω–æ—Å—Ç–∏', fontsize=10)
        axes[2].set_ylabel('RMSE', fontsize=10)
        axes[2].grid(True, alpha=0.3)
        
        # –õ–∏–Ω–µ–π–Ω–∞—è —Ä–µ–≥—Ä–µ—Å—Å–∏—è
        if len(weights_df) > 1:
            z = np.polyfit(weights_df['popularity'], weights_df['rmse'], 1)
            p = np.poly1d(z)
            axes[2].plot(weights_df['popularity'], p(weights_df['popularity']), 
                        "r--", alpha=0.8, linewidth=2)
        
        # 4. –õ—É—á—à–∏–µ –≤–µ—Å–∞ (—Ä–∞–¥–∞—Ä–Ω–∞—è –¥–∏–∞–≥—Ä–∞–º–º–∞)
        best_idx = weights_df['rmse'].idxmin()
        best_weights = weights_df.loc[best_idx, correlation_cols].values
        
        angles = np.linspace(0, 2*np.pi, len(correlation_cols), endpoint=False).tolist()
        best_weights = np.concatenate((best_weights, [best_weights[0]]))
        angles += angles[:1]
        
        ax4 = fig.add_subplot(2, 2, 4, polar=True)
        ax4.plot(angles, best_weights, linewidth=2, linestyle='solid', color='green')
        ax4.fill(angles, best_weights, alpha=0.25, color='green')
        ax4.set_xticks(angles[:-1])
        ax4.set_xticklabels(correlation_cols, fontsize=9)
        ax4.set_title('–õ—É—á—à–∏–µ –≤–µ—Å–∞ –º–æ–¥–µ–ª–µ–π', fontsize=12, fontweight='bold', pad=20)
        ax4.grid(True)
        
        fig.suptitle('–†–ï–ó–£–õ–¨–¢–ê–¢–´ –û–ü–¢–ò–ú–ò–ó–ê–¶–ò–ò –í–ï–°–û–í –ì–ò–ë–†–ò–î–ù–û–ô –ú–û–î–ï–õ–ò', 
                    fontsize=14, fontweight='bold')
        plt.tight_layout()
        plt.show()
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏
        print("\nüìä –°–¢–ê–¢–ò–°–¢–ò–ö–ê –û–ü–¢–ò–ú–ò–ó–ê–¶–ò–ò:")
        print(f"  ‚Ä¢ –ü—Ä–æ–≤–µ—Ä–µ–Ω–æ —Å—Ç—Ä–∞—Ç–µ–≥–∏–π: {len(evaluation_results)}")
        print(f"  ‚Ä¢ –õ—É—á—à–∏–π RMSE: {best_rmse:.4f}")
        print(f"  ‚Ä¢ –°—Ä–µ–¥–Ω–∏–π RMSE: {np.mean(rmses):.4f}")
        print(f"  ‚Ä¢ –°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–µ –æ—Ç–∫–ª–æ–Ω–µ–Ω–∏–µ RMSE: {np.std(rmses):.4f}")
        
        # –ê–Ω–∞–ª–∏–∑ –≤–ª–∏—è–Ω–∏—è –≤–µ—Å–æ–≤
        print(f"\nüìà –í–õ–ò–Ø–ù–ò–ï –í–ï–°–û–í –ù–ê –ö–ê–ß–ï–°–¢–í–û:")
        for col in correlation_cols:
            corr = np.corrcoef(weights_df[col], weights_df['rmse'])[0, 1]
            print(f"  ‚Ä¢ {col}: –∫–æ—Ä—Ä–µ–ª—è—Ü–∏—è —Å RMSE = {corr:.3f}")
        
        return True

# –°–æ–∑–¥–∞–µ–º –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—É—é –≥–∏–±—Ä–∏–¥–Ω—É—é –º–æ–¥–µ–ª—å
print("–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–π –≥–∏–±—Ä–∏–¥–Ω–æ–π –º–æ–¥–µ–ª–∏...")
optimized_hybrid = OptimizedHybridModel(model_factory, book_stats, user_stats)

# –ë—ã—Å—Ç—Ä–∞—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –≤–µ—Å–æ–≤
weights, rmse = optimized_hybrid.optimize_weights_quick(n_iter=15)

print(f"\nüéØ –§–∏–Ω–∞–ª—å–Ω—ã–µ –≤–µ—Å–∞ –≥–∏–±—Ä–∏–¥–Ω–æ–π –º–æ–¥–µ–ª–∏:")
for model_name, weight in weights.items():
    print(f"  ‚Ä¢ {model_name}: {weight:.3f}")
print(f"  –û–∂–∏–¥–∞–µ–º—ã–π RMSE: {rmse:.4f}")




# ========================================================================
# 5.1 –í–ò–ó–£–ê–õ–ò–ó–ê–¶–ò–ò –†–ê–ë–û–¢–´ –ì–ò–ë–†–ò–î–ù–û–ô –ú–û–î–ï–õ–ò
# ========================================================================

print("\n5.1 –í–ò–ó–£–ê–õ–ò–ó–ê–¶–ò–ò –†–ê–ë–û–¢–´ –ì–ò–ë–†–ò–î–ù–û–ô –ú–û–î–ï–õ–ò")
print("-" * 60)

def visualize_hybrid_model_performance():
    """–í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è —Ä–∞–±–æ—Ç—ã –≥–∏–±—Ä–∏–¥–Ω–æ–π –º–æ–¥–µ–ª–∏"""
    cache_key = "hybrid_model_visualizations"
    
    def create_visualizations():
        print("üìä –ê–Ω–∞–ª–∏–∑ —Ä–∞–±–æ—Ç—ã –≥–∏–±—Ä–∏–¥–Ω–æ–π –º–æ–¥–µ–ª–∏...")
        
        # –¢–µ—Å—Ç–∏—Ä—É–µ–º –Ω–∞ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è—Ö
        test_users_sample = test_filtered['user_id'].unique()[:5]
        
        # –°–æ–±–∏—Ä–∞–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π
        all_predictions = []
        all_actuals = []
        user_stats_list = []
        
        for user_id in test_users_sample[:3]:  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–ª—è —Å–∫–æ—Ä–æ—Å—Ç–∏
            user_ratings = test_filtered[test_filtered['user_id'] == user_id]
            if len(user_ratings) > 0:
                for _, row in user_ratings.head(5).iterrows():  # –ë–µ—Ä–µ–º –ø–µ—Ä–≤—ã–µ 5 –æ—Ü–µ–Ω–æ–∫
                    book_id = row['book_id']
                    actual_rating = row['rating']
                    
                    # –ü–æ–ª—É—á–∞–µ–º –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –æ—Ç –∫–∞–∂–¥–æ–π –º–æ–¥–µ–ª–∏
                    pop_pred = optimized_hybrid.predict_popularity_cached(book_id)
                    content_pred = optimized_hybrid.predict_content_cached(book_id)
                    item_pred = optimized_hybrid.predict_item_based_cached(book_id)
                    svd_pred = optimized_hybrid.predict_svd_cached(user_id, book_id)
                    hybrid_pred = optimized_hybrid.hybrid_predict(user_id, book_id, weights)
                    
                    all_predictions.append({
                        'user_id': user_id,
                        'book_id': book_id,
                        'pop': pop_pred,
                        'content': content_pred,
                        'item': item_pred,
                        'svd': svd_pred,
                        'hybrid': hybrid_pred,
                        'actual': actual_rating
                    })
                    
                    all_actuals.append(actual_rating)
        
        if not all_predictions:
            print("  ‚ö† –ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏")
            return False
        
        predictions_df = pd.DataFrame(all_predictions)
        
        # –°–æ–∑–¥–∞–µ–º —Ñ–∏–≥—É—Ä—É
        fig, axes = plt.subplots(2, 2, figsize=(14, 10))
        axes = axes.flatten()
        
        # 1. –°—Ä–∞–≤–Ω–µ–Ω–∏–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π –º–æ–¥–µ–ª–µ–π
        model_names = ['pop', 'content', 'item', 'svd', 'hybrid']
        model_errors = []
        
        for model in model_names:
            if model in predictions_df.columns:
                errors = abs(predictions_df[model] * 5 - predictions_df['actual'])  # –ú–∞—Å—à—Ç–∞–±–∏—Ä—É–µ–º –∫ 0-5
                model_errors.append(np.mean(errors))
            else:
                model_errors.append(0)
        
        bars = axes[0].bar(range(len(model_names)), model_errors, 
                          color=plt.cm.tab10(range(len(model_names))))
        axes[0].set_title('–°—Ä–µ–¥–Ω—è—è –∞–±—Å–æ–ª—é—Ç–Ω–∞—è –æ—à–∏–±–∫–∞ (MAE) –º–æ–¥–µ–ª–µ–π', fontsize=12, fontweight='bold')
        axes[0].set_xlabel('–ú–æ–¥–µ–ª—å', fontsize=10)
        axes[0].set_ylabel('MAE', fontsize=10)
        axes[0].set_xticks(range(len(model_names)))
        axes[0].set_xticklabels(['–ü–æ–ø—É–ª.', '–ö–æ–Ω—Ç.', 'Item', 'SVD', '–ì–∏–±—Ä–∏–¥'], 
                               rotation=45, ha='right', fontsize=9)
        axes[0].grid(True, alpha=0.3, axis='y')
        
        # –î–æ–±–∞–≤–ª—è–µ–º –∑–Ω–∞—á–µ–Ω–∏—è –Ω–∞ —Å—Ç–æ–ª–±—Ü—ã
        for bar, error in zip(bars, model_errors):
            height = bar.get_height()
            axes[0].text(bar.get_x() + bar.get_width()/2., height + max(model_errors)*0.01,
                        f'{error:.3f}', ha='center', va='bottom', fontsize=9)
        
        # 2. –í–∫–ª–∞–¥ –∫–∞–∂–¥–æ–π –º–æ–¥–µ–ª–∏ –≤ –≥–∏–±—Ä–∏–¥–Ω–æ–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ
        weights_array = [weights['popularity'], weights['content'], 
                        weights['item_based'], weights['svd']]
        weight_labels = ['–ü–æ–ø—É–ª.', '–ö–æ–Ω—Ç.', 'Item', 'SVD']
        
        axes[1].pie(weights_array, labels=weight_labels, autopct='%1.1f%%',
                   colors=plt.cm.Set3(range(len(weights_array))))
        axes[1].set_title('–í–∫–ª–∞–¥ –º–æ–¥–µ–ª–µ–π –≤ –≥–∏–±—Ä–∏–¥–Ω–æ–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ', fontsize=12, fontweight='bold')
        
        # 3. Scatter plot: –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è vs —Ñ–∞–∫—Ç–∏—á–µ—Å–∫–∏–µ –∑–Ω–∞—á–µ–Ω–∏—è
        hybrid_scaled = predictions_df['hybrid'] * 5  # –ú–∞—Å—à—Ç–∞–±–∏—Ä—É–µ–º –∫ 0-5
        axes[2].scatter(predictions_df['actual'], hybrid_scaled, 
                       alpha=0.6, s=30, c='green', edgecolors='black', linewidth=0.5)
        axes[2].plot([0, 5], [0, 5], 'r--', alpha=0.5, linewidth=2)  # –ò–¥–µ–∞–ª—å–Ω–∞—è –ª–∏–Ω–∏—è
        axes[2].set_title('–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è vs –§–∞–∫—Ç–∏—á–µ—Å–∫–∏–µ –∑–Ω–∞—á–µ–Ω–∏—è', fontsize=12, fontweight='bold')
        axes[2].set_xlabel('–§–∞–∫—Ç–∏—á–µ—Å–∫–∞—è –æ—Ü–µ–Ω–∫–∞', fontsize=10)
        axes[2].set_ylabel('–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω–∞—è –æ—Ü–µ–Ω–∫–∞', fontsize=10)
        axes[2].grid(True, alpha=0.3)
        axes[2].set_xlim([0, 5.5])
        axes[2].set_ylim([0, 5.5])
        
        # 4. –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –æ—à–∏–±–æ–∫ –≥–∏–±—Ä–∏–¥–Ω–æ–π –º–æ–¥–µ–ª–∏
        errors = hybrid_scaled - predictions_df['actual']
        axes[3].hist(errors, bins=20, edgecolor='black', alpha=0.7, color='purple')
        axes[3].set_title('–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –æ—à–∏–±–æ–∫ –≥–∏–±—Ä–∏–¥–Ω–æ–π –º–æ–¥–µ–ª–∏', fontsize=12, fontweight='bold')
        axes[3].set_xlabel('–û—à–∏–±–∫–∞ (–ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ - —Ñ–∞–∫—Ç)', fontsize=10)
        axes[3].set_ylabel('–ß–∞—Å—Ç–æ—Ç–∞', fontsize=10)
        axes[3].grid(True, alpha=0.3)
        axes[3].axvline(x=0, color='red', linestyle='--', linewidth=2, alpha=0.7)
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –æ—à–∏–±–æ–∫
        mean_error = np.mean(errors)
        std_error = np.std(errors)
        axes[3].text(0.7, 0.95, f'–°—Ä–µ–¥–Ω–µ–µ: {mean_error:.3f}\n–°—Ç–¥: {std_error:.3f}',
                    transform=axes[3].transAxes, fontsize=9, verticalalignment='top',
                    bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))
        
        fig.suptitle('–ê–ù–ê–õ–ò–ó –†–ê–ë–û–¢–´ –ì–ò–ë–†–ò–î–ù–û–ô –ú–û–î–ï–õ–ò', fontsize=14, fontweight='bold')
        plt.tight_layout()
        plt.show()
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
        print("\nüìä –ü–†–û–ò–ó–í–û–î–ò–¢–ï–õ–¨–ù–û–°–¢–¨ –ì–ò–ë–†–ò–î–ù–û–ô –ú–û–î–ï–õ–ò:")
        
        # –í—ã—á–∏—Å–ª—è–µ–º –º–µ—Ç—Ä–∏–∫–∏
        mae_hybrid = np.mean(abs(errors))
        rmse_hybrid = np.sqrt(np.mean(errors**2))
        
        print(f"  ‚Ä¢ –°—Ä–µ–¥–Ω—è—è –∞–±—Å–æ–ª—é—Ç–Ω–∞—è –æ—à–∏–±–∫–∞ (MAE): {mae_hybrid:.4f}")
        print(f"  ‚Ä¢ –°—Ä–µ–¥–Ω–µ–∫–≤–∞–¥—Ä–∞—Ç–∏—á–Ω–∞—è –æ—à–∏–±–∫–∞ (RMSE): {rmse_hybrid:.4f}")
        print(f"  ‚Ä¢ –°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–µ –æ—Ç–∫–ª–æ–Ω–µ–Ω–∏–µ –æ—à–∏–±–æ–∫: {std_error:.4f}")
        
        # –ü—Ä–æ—Ü–µ–Ω—Ç –ø—Ä–∞–≤–∏–ª—å–Ω—ã—Ö –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π –≤ –ø—Ä–µ–¥–µ–ª–∞—Ö –¥–∏–∞–ø–∞–∑–æ–Ω–∞
        within_05 = np.sum(abs(errors) <= 0.5) / len(errors) * 100
        within_10 = np.sum(abs(errors) <= 1.0) / len(errors) * 100
        within_15 = np.sum(abs(errors) <= 1.5) / len(errors) * 100
        
        print(f"  ‚Ä¢ –í –ø—Ä–µ–¥–µ–ª–∞—Ö 0.5 –±–∞–ª–ª–∞: {within_05:.1f}%")
        print(f"  ‚Ä¢ –í –ø—Ä–µ–¥–µ–ª–∞—Ö 1.0 –±–∞–ª–ª–∞: {within_10:.1f}%")
        print(f"  ‚Ä¢ –í –ø—Ä–µ–¥–µ–ª–∞—Ö 1.5 –±–∞–ª–ª–æ–≤: {within_15:.1f}%")
        
        # –°—Ä–∞–≤–Ω–µ–Ω–∏–µ —Å –∏–Ω–¥–∏–≤–∏–¥—É–∞–ª—å–Ω—ã–º–∏ –º–æ–¥–µ–ª—è–º–∏
        print(f"\nüîç –°–†–ê–í–ù–ï–ù–ò–ï –° –ò–ù–î–ò–í–ò–î–£–ê–õ–¨–ù–´–ú–ò –ú–û–î–ï–õ–Ø–ú–ò:")
        for i, (model_name, model_label) in enumerate(zip(model_names[:-1], ['–ü–æ–ø—É–ª.', '–ö–æ–Ω—Ç.', 'Item', 'SVD'])):
            if model_name in predictions_df.columns:
                model_errors_i = abs(predictions_df[model_name] * 5 - predictions_df['actual'])
                mae_i = np.mean(model_errors_i)
                improvement = (mae_i - mae_hybrid) / mae_i * 100 if mae_i > 0 else 0
                print(f"  ‚Ä¢ {model_label}: MAE = {mae_i:.4f} ({improvement:+.1f}% —É–ª—É—á—à–µ–Ω–∏–µ)")
        
        return True
    
    return global_cache.get_or_compute(cache_key, create_visualizations)

# –°–æ–∑–¥–∞–µ–º –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏ –≥–∏–±—Ä–∏–¥–Ω–æ–π –º–æ–¥–µ–ª–∏
visualize_hybrid_model_performance()


# ========================================================================
# 6. –û–ü–¢–ò–ú–ò–ó–ò–†–û–í–ê–ù–ù–ê–Ø –î–ï–ú–û–ù–°–¢–†–ê–¶–ò–Ø –°–ò–°–¢–ï–ú–´
# ========================================================================

print("\n\n6. –û–ü–¢–ò–ú–ò–ó–ò–†–û–í–ê–ù–ù–ê–Ø –î–ï–ú–û–ù–°–¢–†–ê–¶–ò–Ø –°–ò–°–¢–ï–ú–´")
print("-" * 60)

class EfficientRecommender:
    """
    –≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã–π —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ç–µ–ª—å —Å –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ–º –≤—Å–µ—Ö –æ–ø–µ—Ä–∞—Ü–∏–π
    """
    
    def __init__(self, hybrid_model, book_stats, train_filtered, weights):
        self.hybrid_model = hybrid_model
        self.book_stats = book_stats
        self.train_filtered = train_filtered
        self.weights = weights
        self.recommendation_cache = {}
        
        # –°–Ω–∞—á–∞–ª–∞ —Å–æ–∑–¥–∞–µ–º —Å–ª–æ–≤–∞—Ä—å –¥–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ –¥–æ—Å—Ç—É–ø–∞ –∫ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ –∫–Ω–∏–≥–∞—Ö
        self.book_info_cache = {}
        self._build_book_info_cache()
        
        # –¢–µ–ø–µ—Ä—å –ø—Ä–µ–¥–≤—ã—á–∏—Å–ª—è–µ–º –ø–æ–ø—É–ª—è—Ä–Ω—ã–µ –∫–Ω–∏–≥–∏ –¥–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ –¥–æ—Å—Ç—É–ø–∞
        self.popular_books = self._precompute_popular_books()
        
    def _build_book_info_cache(self):
        """–°–æ–∑–¥–∞–µ–º –∫—ç—à –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ –∫–Ω–∏–≥–∞—Ö –¥–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ –¥–æ—Å—Ç—É–ø–∞"""
        print("  –ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –∫—ç—à–∞ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ –∫–Ω–∏–≥–∞—Ö...")
        for _, row in self.book_stats.iterrows():
            book_id = row['book_id']
            title = str(row.get('title', f'–ö–Ω–∏–≥–∞ {book_id}')).strip()
            authors = str(row.get('authors', '–ù–µ–∏–∑–≤–µ—Å—Ç–µ–Ω')).strip()
            
            # –û—á–∏—â–∞–µ–º –¥–∞–Ω–Ω—ã–µ
            if title == '0' or title == 'nan' or not title:
                title = f'–ö–Ω–∏–≥–∞ {book_id}'
            if authors == '0' or authors == 'nan' or not authors:
                authors = '–ù–µ–∏–∑–≤–µ—Å—Ç–µ–Ω'
            
            self.book_info_cache[book_id] = {
                'title': title,
                'authors': authors,
                'title_short': title[:40] + "..." if len(title) > 40 else title,
                'authors_short': authors[:30] + "..." if len(authors) > 30 else authors
            }
        print(f"  ‚úì –ö—ç—à –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ {len(self.book_info_cache)} –∫–Ω–∏–≥–∞—Ö –ø–æ—Å—Ç—Ä–æ–µ–Ω")
        
    def _precompute_popular_books(self, n=100):
        """–ü—Ä–µ–¥–≤—ã—á–∏—Å–ª–µ–Ω–∏–µ –ø–æ–ø—É–ª—è—Ä–Ω—ã—Ö –∫–Ω–∏–≥"""
        cache_key = "popular_books"
        if cache_key in self.recommendation_cache:
            return self.recommendation_cache[cache_key]
        
        if 'popularity_model' in self.hybrid_model.model_factory.models:
            pop_model = self.hybrid_model.model_factory.get_popularity_model()
            popular = pop_model.head(n)['book_id'].tolist()
        else:
            # –ê–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ã–π —Ä–∞—Å—á–µ—Ç
            book_counts = self.train_filtered.groupby('book_id').size()
            popular = book_counts.sort_values(ascending=False).head(n).index.tolist()
        
        # –§–∏–ª—å—Ç—Ä—É–µ–º –∫–Ω–∏–≥–∏ —Å –Ω–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã–º–∏ –¥–∞–Ω–Ω—ã–º–∏
        valid_popular = []
        for book_id in popular:
            if book_id in self.book_info_cache:
                book_info = self.book_info_cache[book_id]
                if (book_info['title'] != f'–ö–Ω–∏–≥–∞ {book_id}' and 
                    book_info['title'] != '0' and
                    book_info['authors'] != '0'):
                    valid_popular.append(book_id)
        
        self.recommendation_cache[cache_key] = valid_popular
        return valid_popular
    
    def get_user_history(self, user_id):
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –∏—Å—Ç–æ—Ä–∏–∏ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è —Å –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ–º"""
        cache_key = f"history_{user_id}"
        if cache_key in self.recommendation_cache:
            return self.recommendation_cache[cache_key]
        
        history = self.train_filtered[self.train_filtered['user_id'] == user_id]
        self.recommendation_cache[cache_key] = history
        return history
    
    def get_candidate_books(self, user_id, max_candidates=500):
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤ –¥–ª—è —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π —Å –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ–º"""
        cache_key = f"candidates_{user_id}_{max_candidates}"
        if cache_key in self.recommendation_cache:
            return self.recommendation_cache[cache_key]
        
        # –ö–Ω–∏–≥–∏, –∫–æ—Ç–æ—Ä—ã–µ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å —É–∂–µ –æ—Ü–µ–Ω–∏–≤–∞–ª
        user_history = self.get_user_history(user_id)
        rated_books = set(user_history['book_id']) if not user_history.empty else set()
        
        # –í—Å–µ –∫–Ω–∏–≥–∏ –∏–∑ –æ–±—É—á–∞—é—â–∏—Ö –¥–∞–Ω–Ω—ã—Ö
        all_books = set(self.train_filtered['book_id'].unique())
        
        # –ò—Å–∫–ª—é—á–∞–µ–º —É–∂–µ –æ—Ü–µ–Ω–µ–Ω–Ω—ã–µ
        candidate_books = list(all_books - rated_books)
        
        # –§–∏–ª—å—Ç—Ä—É–µ–º –∫–Ω–∏–≥–∏ —Å –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π
        candidate_books = [b for b in candidate_books if b in self.book_info_cache]
        candidate_books = [b for b in candidate_books if b != 0]
        
        # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤
        if len(candidate_books) > max_candidates:
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º –ø–æ–ø—É–ª—è—Ä–Ω—ã–µ –∫–Ω–∏–≥–∏ –≤ –∫–∞—á–µ—Å—Ç–≤–µ –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–Ω—ã—Ö –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤
            popular_candidates = [b for b in self.popular_books if b in candidate_books]
            if len(popular_candidates) >= max_candidates // 2:
                candidate_books = popular_candidates[:max_candidates // 2]
                # –î–æ–±–∞–≤–ª—è–µ–º —Å–ª—É—á–∞–π–Ω—ã–µ –∏–∑ –æ—Å—Ç–∞–ª—å–Ω—ã—Ö
                other_books = [b for b in candidate_books if b not in popular_candidates]
                if other_books:
                    np.random.seed(42)
                    additional = np.random.choice(other_books, 
                                                 min(len(other_books), max_candidates // 2),
                                                 replace=False)
                    # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º numpy array –≤ list
                    if isinstance(additional, np.ndarray):
                        additional = additional.tolist()
                    candidate_books.extend(additional)
            else:
                np.random.seed(42)
                candidate_books = np.random.choice(candidate_books, 
                                                  min(max_candidates, len(candidate_books)), 
                                                  replace=False)
                # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º numpy array –≤ list
                if isinstance(candidate_books, np.ndarray):
                    candidate_books = candidate_books.tolist()
        
        self.recommendation_cache[cache_key] = candidate_books
        return candidate_books
    
    def recommend_for_user(self, user_id, n=10, use_cache=True):
        """–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –¥–ª—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è —Å —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–º –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ–º"""
        cache_key = f"recommendations_{user_id}_{n}"
        
        if use_cache and cache_key in self.recommendation_cache:
            print(f"  –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω—ã –∫—ç—à–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –¥–ª—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è {user_id}")
            return self.recommendation_cache[cache_key]
        
        print(f"\nüéØ –§–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏–µ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π –¥–ª—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è {user_id}...")
        
        # –ü–æ–ª—É—á–∞–µ–º –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤
        candidate_books = self.get_candidate_books(user_id, max_candidates=300)
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ candidate_books - —Å–ø–∏—Å–æ–∫ –∏ –Ω–µ –ø—É—Å—Ç–æ–π
        if not candidate_books or len(candidate_books) == 0:
            print("  ‚ö† –ù–µ—Ç –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤ –¥–ª—è —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π")
            return []
        
        # –í—ã—á–∏—Å–ª—è–µ–º —Å–∫–æ—Ä–∏–Ω–≥ –¥–ª—è –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤
        scores = []
        batch_size = 50
        
        # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –±–∞—Ç—á–∞–º–∏ –¥–ª—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏
        for i in range(0, len(candidate_books), batch_size):
            batch = candidate_books[i:i + batch_size]
            for book_id in batch:
                # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –Ω–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã–µ book_id
                if book_id == 0 or book_id not in self.book_info_cache:
                    continue
                    
                score = self.hybrid_model.hybrid_predict(user_id, book_id, self.weights)
                scores.append((book_id, score))
        
        # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ —É–±—ã–≤–∞–Ω–∏—é —Å–∫–æ—Ä–∞
        scores.sort(key=lambda x: x[1], reverse=True)
        
        # –§–æ—Ä–º–∞—Ç–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç
        recommendations = []
        top_n = scores[:n]
        
        for i, (book_id, score) in enumerate(top_n, 1):
            # –ü–æ–ª—É—á–∞–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –∫–Ω–∏–≥–µ –∏–∑ –∫—ç—à–∞
            if book_id in self.book_info_cache:
                book_info = self.book_info_cache[book_id]
                
                # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ—Å—Ç—å –¥–∞–Ω–Ω—ã—Ö
                if (book_info['title'] == '0' or 
                    book_info['title'] == f'–ö–Ω–∏–≥–∞ {book_id}' or
                    book_info['authors'] == '0'):
                    # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –∫–Ω–∏–≥–∏ —Å –Ω–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã–º–∏ –¥–∞–Ω–Ω—ã–º–∏
                    continue
                
                recommendations.append({
                    'rank': i,
                    'book_id': book_id,
                    'title': book_info['title_short'],
                    'authors': book_info['authors_short'],
                    'score': score
                })
            else:
                # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –∫–Ω–∏–≥–∏ –±–µ–∑ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏
                continue
        
        # –ï—Å–ª–∏ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π –º–µ–Ω—å—à–µ –∑–∞–ø—Ä–æ—à–µ–Ω–Ω–æ–≥–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞, –¥–æ–±–∞–≤–ª—è–µ–º –ø–æ–ø—É–ª—è—Ä–Ω—ã–µ –∫–Ω–∏–≥–∏
        if len(recommendations) < n:
            additional_needed = n - len(recommendations)
            popular_books = [b for b in self.popular_books 
                           if b not in [r['book_id'] for r in recommendations] 
                           and b in self.book_info_cache]
            
            for book_id in popular_books[:additional_needed]:
                if book_id in self.book_info_cache:
                    book_info = self.book_info_cache[book_id]
                    if (book_info['title'] != '0' and 
                        book_info['title'] != f'–ö–Ω–∏–≥–∞ {book_id}' and
                        book_info['authors'] != '0'):
                        
                        # –í—ã—á–∏—Å–ª—è–µ–º —Å–∫–æ—Ä –¥–ª—è –ø–æ–ø—É–ª—è—Ä–Ω–æ–π –∫–Ω–∏–≥–∏
                        score = self.hybrid_model.hybrid_predict(user_id, book_id, self.weights)
                        
                        recommendations.append({
                            'rank': len(recommendations) + 1,
                            'book_id': book_id,
                            'title': book_info['title_short'],
                            'authors': book_info['authors_short'],
                            'score': score
                        })
        
        # –ö—ç—à–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç
        self.recommendation_cache[cache_key] = recommendations
        
        return recommendations
    
    def batch_recommend(self, user_ids, n=5):
        """–ü–∞–∫–µ—Ç–Ω—ã–µ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –¥–ª—è –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π"""
        print(f"\nüë• –ü–∞–∫–µ—Ç–Ω—ã–µ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –¥–ª—è {len(user_ids)} –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π...")
        
        all_recommendations = {}
        for user_id in user_ids[:10]:  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–ª—è –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–∏
            try:
                recommendations = self.recommend_for_user(user_id, n=n, use_cache=True)
                all_recommendations[user_id] = recommendations
            except Exception as e:
                print(f"  –û—à–∏–±–∫–∞ –¥–ª—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è {user_id}: {e}")
        
        return all_recommendations
    
    def visualize_recommendations(self, user_id, n=5):
        """–í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π –¥–ª—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è"""
        print(f"\nüìä –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π –¥–ª—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è {user_id}...")
    
        # –ü–æ–ª—É—á–∞–µ–º —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏
        recommendations = self.recommend_for_user(user_id, n=n, use_cache=True)
    
        if not recommendations:
            print("  ‚ö† –ù–µ—Ç —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π –¥–ª—è –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏")
            return
    
        # –ü–æ–ª—É—á–∞–µ–º –∏—Å—Ç–æ—Ä–∏—é –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
        user_history = self.get_user_history(user_id)
    
        # –°–æ–∑–¥–∞–µ–º —Ñ–∏–≥—É—Ä—É
        fig, axes = plt.subplots(1, 2, figsize=(14, 8))
    
        # 1. –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Å–∫–æ—Ä–æ–≤ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π
        scores = [rec['score'] for rec in recommendations]
        titles = [rec['title'] for rec in recommendations]
    
        # –£–∫–æ—Ä–∞—á–∏–≤–∞–µ–º –Ω–∞–∑–≤–∞–Ω–∏—è –¥–ª—è –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏—è
        short_titles = []
        for title in titles:
            if len(title) > 25:
                short_titles.append(title[:22] + "...")
            else:
                short_titles.append(title)
    
        y_pos = np.arange(len(scores))
        bars = axes[0].barh(y_pos, scores, color=plt.cm.viridis(np.linspace(0, 1, len(scores))))
        axes[0].set_yticks(y_pos)
        axes[0].set_yticklabels(short_titles, fontsize=9)
        axes[0].invert_yaxis()
        axes[0].set_title(f'–¢–æ–ø-{len(scores)} —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π –¥–ª—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è {user_id}', 
                         fontsize=12, fontweight='bold')
        axes[0].set_xlabel('–°–∫–æ—Ä —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏', fontsize=10)
        axes[0].grid(True, alpha=0.3, axis='x')
        
        # –î–æ–±–∞–≤–ª—è–µ–º –∑–Ω–∞—á–µ–Ω–∏—è —Å–∫–æ—Ä–æ–≤
        for bar, score in zip(bars, scores):
            width = bar.get_width()
            axes[0].text(width + max(scores)*0.01, bar.get_y() + bar.get_height()/2,
                       f'{score:.3f}', ha='left', va='center', fontsize=9)
    
        # 2. –í–∫–ª–∞–¥ –º–æ–¥–µ–ª–µ–π –≤ —Ç–æ–ø-—Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏
        if len(recommendations) > 0:
            # –î–ª—è –ø–µ—Ä–≤–æ–π —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –∞–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –≤–∫–ª–∞–¥ –º–æ–¥–µ–ª–µ–π
            top_book_id = recommendations[0]['book_id']
        
            model_predictions = []
            model_names = ['popularity', 'content', 'item_based', 'svd']
            model_labels = ['–ü–æ–ø—É–ª.', '–ö–æ–Ω—Ç.', 'Item', 'SVD']
        
            for model_name in model_names:
                if model_name == 'popularity':
                    pred = self.hybrid_model.predict_popularity_cached(top_book_id) * self.weights['popularity']
                elif model_name == 'content':
                    pred = self.hybrid_model.predict_content_cached(top_book_id) * self.weights['content']
                elif model_name == 'item_based':
                    pred = self.hybrid_model.predict_item_based_cached(top_book_id) * self.weights['item_based']
                elif model_name == 'svd':
                    pred = self.hybrid_model.predict_svd_cached(user_id, top_book_id) * self.weights['svd']
                else:
                    pred = 0
            
                # –£–±–µ–¥–∏–º—Å—è, —á—Ç–æ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –Ω–µ–æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω–æ–µ
                pred = max(0, pred)
                model_predictions.append(pred)
        
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –µ—Å—Ç—å –ª–∏ –ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è
            total = sum(model_predictions)
        
            if total > 0:
                # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º –¥–ª—è –∫—Ä—É–≥–æ–≤–æ–π –¥–∏–∞–≥—Ä–∞–º–º—ã
                model_percentages = [p/total*100 for p in model_predictions]
                
                # –°–æ–∑–¥–∞–µ–º –∫—Ä—É–≥–æ–≤—É—é –¥–∏–∞–≥—Ä–∞–º–º—É —Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ –µ—Å—Ç—å –ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è
                wedges, texts, autotexts = axes[1].pie(model_predictions, labels=model_labels, 
                                                      autopct='%1.1f%%', colors=plt.cm.Set3(range(len(model_predictions))))
                axes[1].set_title(f'–í–∫–ª–∞–¥ –º–æ–¥–µ–ª–µ–π –≤ —Ç–æ–ø-—Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—é\n"{recommendations[0]["title"]}"', 
                                 fontsize=12, fontweight='bold')
            
                # –î–æ–±–∞–≤–ª—è–µ–º –ª–µ–≥–µ–Ω–¥—É —Å –∞–±—Å–æ–ª—é—Ç–Ω—ã–º–∏ –∑–Ω–∞—á–µ–Ω–∏—è–º–∏
                legend_labels = []
                for label, value, perc in zip(model_labels, model_predictions, model_percentages):
                    legend_labels.append(f'{label}: {value:.3f} ({perc:.1f}%)')
            
                axes[1].legend(wedges, legend_labels, title="–ú–æ–¥–µ–ª–∏", loc="center left", 
                              bbox_to_anchor=(1, 0, 0.5, 1), fontsize=9)
            else:
                # –ï—Å–ª–∏ –≤—Å–µ –∑–Ω–∞—á–µ–Ω–∏—è –Ω—É–ª–µ–≤—ã–µ, –ø–æ–∫–∞–∑—ã–≤–∞–µ–º —Å–æ–æ–±—â–µ–Ω–∏–µ
                axes[1].text(0.5, 0.5, '–í—Å–µ –º–æ–¥–µ–ª–∏ –¥–∞–ª–∏ –Ω—É–ª–µ–≤–æ–π –≤–∫–ª–∞–¥\n–≤ —ç—Ç—É —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—é',
                            ha='center', va='center', transform=axes[1].transAxes,
                            fontsize=11, color='gray')
                axes[1].set_title(f'–í–∫–ª–∞–¥ –º–æ–¥–µ–ª–µ–π –≤ —Ç–æ–ø-—Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—é\n"{recommendations[0]["title"]}"', 
                                 fontsize=12, fontweight='bold')
    
        # –ï—Å–ª–∏ –µ—Å—Ç—å –∏—Å—Ç–æ—Ä–∏—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
        if not user_history.empty:
            # –î–æ–±–∞–≤–ª—è–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –∏—Å—Ç–æ—Ä–∏–∏
            history_text = f"–ò—Å—Ç–æ—Ä–∏—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è:\n"
            history_text += f"‚Ä¢ –û—Ü–µ–Ω–∏–ª –∫–Ω–∏–≥: {len(user_history)}\n"
            history_text += f"‚Ä¢ –°—Ä–µ–¥–Ω—è—è –æ—Ü–µ–Ω–∫–∞: {user_history['rating'].mean():.2f}\n"
            history_text += f"‚Ä¢ –ú–∏–Ω–∏–º–∞–ª—å–Ω–∞—è –æ—Ü–µ–Ω–∫–∞: {user_history['rating'].min()}\n"
            history_text += f"‚Ä¢ –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –æ—Ü–µ–Ω–∫–∞: {user_history['rating'].max()}"
        
            fig.text(0.02, 0.98, history_text, transform=fig.transFigure,
                    fontsize=9, verticalalignment='top',
                    bbox=dict(boxstyle='round', facecolor='lightblue', alpha=0.7))
    
        fig.suptitle('–ü–ï–†–°–û–ù–ê–õ–ò–ó–ò–†–û–í–ê–ù–ù–´–ï –†–ï–ö–û–ú–ï–ù–î–ê–¶–ò–ò', fontsize=14, fontweight='bold')
        plt.tight_layout()
        plt.show()
    
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π
        print(f"\nüìä –°–¢–ê–¢–ò–°–¢–ò–ö–ê –†–ï–ö–û–ú–ï–ù–î–ê–¶–ò–ô:")
        print(f"  ‚Ä¢ –í—Å–µ–≥–æ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π: {len(recommendations)}")
        print(f"  ‚Ä¢ –î–∏–∞–ø–∞–∑–æ–Ω —Å–∫–æ—Ä–æ–≤: {min(scores):.3f} - {max(scores):.3f}")
        print(f"  ‚Ä¢ –°—Ä–µ–¥–Ω–∏–π —Å–∫–æ—Ä: {np.mean(scores):.3f}")
    
        return recommendations

# –°–æ–∑–¥–∞–µ–º —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã–π —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ç–µ–ª—å
print("–°–æ–∑–¥–∞–Ω–∏–µ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–≥–æ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ç–µ–ª—è...")
efficient_recommender = EfficientRecommender(optimized_hybrid, book_stats, train_filtered, weights)



# ========================================================================
# 7. –§–ò–ù–ê–õ–¨–ù–ê–Ø –î–ï–ú–û–ù–°–¢–†–ê–¶–ò–Ø –ò –†–ï–ó–£–õ–¨–¢–ê–¢–´
# ========================================================================

print("\n\n7. –§–ò–ù–ê–õ–¨–ù–ê–Ø –î–ï–ú–û–ù–°–¢–†–ê–¶–ò–Ø –ò –†–ï–ó–£–õ–¨–¢–ê–¢–´")
print("-" * 60)

def final_demonstration():
    """–§–∏–Ω–∞–ª—å–Ω–∞—è –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è —Ä–∞–±–æ—Ç—ã —Å–∏—Å—Ç–µ–º—ã"""
    print("üöÄ –ó–∞–ø—É—Å–∫ —Ñ–∏–Ω–∞–ª—å–Ω–æ–π –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–∏ —Å–∏—Å—Ç–µ–º—ã...")
    
    # –í—ã–±–∏—Ä–∞–µ–º —Ç–µ—Å—Ç–æ–≤–æ–≥–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
    test_users = test_filtered['user_id'].unique()
    
    if len(test_users) == 0:
        print("  ‚ö† –ù–µ—Ç —Ç–µ—Å—Ç–æ–≤—ã—Ö –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π")
        return
    
    # –ë–µ—Ä–µ–º –ø–µ—Ä–≤–æ–≥–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
    demo_user = test_users[0]
    
    print(f"\nüéØ –î–ï–ú–û–ù–°–¢–†–ê–¶–ò–Ø –î–õ–Ø –ü–û–õ–¨–ó–û–í–ê–¢–ï–õ–Ø {demo_user}:")
    
    # –ü–æ–ª—É—á–∞–µ–º –∏—Å—Ç–æ—Ä–∏—é –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
    user_history = train_filtered[train_filtered['user_id'] == demo_user]
    
    if not user_history.empty:
        print(f"\nüìö –ò–°–¢–û–†–ò–Ø –ü–û–õ–¨–ó–û–í–ê–¢–ï–õ–Ø:")
        print(f"  ‚Ä¢ –û—Ü–µ–Ω–∏–ª –∫–Ω–∏–≥: {len(user_history)}")
        print(f"  ‚Ä¢ –°—Ä–µ–¥–Ω—è—è –æ—Ü–µ–Ω–∫–∞: {user_history['rating'].mean():.2f}‚òÖ")
        
        # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –Ω–µ—Å–∫–æ–ª—å–∫–æ –ø–æ—Å–ª–µ–¥–Ω–∏—Ö –æ—Ü–µ–Ω–æ–∫
        recent_books = user_history.tail(3)
        print(f"  ‚Ä¢ –ü–æ—Å–ª–µ–¥–Ω–∏–µ –æ—Ü–µ–Ω–∫–∏:")
        for _, row in recent_books.iterrows():
            book_id = row['book_id']
            rating = row['rating']
            
            if book_id in efficient_recommender.book_info_cache:
                title = efficient_recommender.book_info_cache[book_id]['title_short']
                authors = efficient_recommender.book_info_cache[book_id]['authors_short']
                print(f"    - {title} ({authors}) - {rating}‚òÖ")
            else:
                print(f"    - –ö–Ω–∏–≥–∞ {book_id} - {rating}‚òÖ")
    
    # –ü–æ–ª—É—á–∞–µ–º –∏ –≤–∏–∑—É–∞–ª–∏–∑–∏—Ä—É–µ–º —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏
    print(f"\nüéØ –ì–ï–ù–ï–†–ê–¶–ò–Ø –†–ï–ö–û–ú–ï–ù–î–ê–¶–ò–ô...")
    recommendations = efficient_recommender.visualize_recommendations(demo_user, n=5)
    
    if recommendations:
        print(f"\n‚úÖ –†–ï–ö–û–ú–ï–ù–î–ê–¶–ò–ò –°–§–û–†–ú–ò–†–û–í–ê–ù–´:")
        for rec in recommendations:
            print(f"  {rec['rank']}. {rec['title']}")
            print(f"     –ê–≤—Ç–æ—Ä: {rec['authors']}")
            print(f"     –°–∫–æ—Ä: {rec['score']:.3f}")
            print()
    
    # –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
    print(f"\n‚ö° –¢–ï–°–¢–ò–†–û–í–ê–ù–ò–ï –ü–†–û–ò–ó–í–û–î–ò–¢–ï–õ–¨–ù–û–°–¢–ò:")
    
    # –ü–µ—Ä–≤—ã–π –≤—ã–∑–æ–≤ (—Ö–æ–ª–æ–¥–Ω—ã–π –∫—ç—à)
    import time
    start_time = time.time()
    recommendations_cold = efficient_recommender.recommend_for_user(demo_user, n=5, use_cache=False)
    cold_time = time.time() - start_time
    
    # –í—Ç–æ—Ä–æ–π –≤—ã–∑–æ–≤ (–≥–æ—Ä—è—á–∏–π –∫—ç—à)
    start_time = time.time()
    recommendations_hot = efficient_recommender.recommend_for_user(demo_user, n=5, use_cache=True)
    hot_time = time.time() - start_time
    
    print(f"  ‚Ä¢ –í—Ä–µ–º—è –ø–µ—Ä–≤–æ–≥–æ –≤—ã–∑–æ–≤–∞ (—Ö–æ–ª–æ–¥–Ω—ã–π –∫—ç—à): {cold_time:.2f} —Å–µ–∫")
    print(f"  ‚Ä¢ –í—Ä–µ–º—è –≤—Ç–æ—Ä–æ–≥–æ –≤—ã–∑–æ–≤–∞ (–≥–æ—Ä—è—á–∏–π –∫—ç—à): {hot_time:.2f} —Å–µ–∫")
    
    if hot_time > 0:
        speedup = cold_time / hot_time
        print(f"  ‚Ä¢ –£—Å–∫–æ—Ä–µ–Ω–∏–µ –∑–∞ —Å—á–µ—Ç –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏—è: {speedup:.1f} —Ä–∞–∑")
    
    # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –∫—ç—à–∞
    print(f"\nüìä –°–¢–ê–¢–ò–°–¢–ò–ö–ê –°–ò–°–¢–ï–ú–´:")
    
    global_stats = global_cache.get_stats()
    print(f"  ‚Ä¢ –ì–ª–æ–±–∞–ª—å–Ω—ã–π –∫—ç—à:")
    print(f"    - –ó–∞–ø—Ä–æ—Å–æ–≤: {global_stats['total']:,}")
    print(f"    - –ü–æ–ø–∞–¥–∞–Ω–∏–π: {global_stats['hits']:,} ({global_stats['hit_rate']*100:.1f}%)")
    print(f"    - –ü—Ä–æ–º–∞—Ö–æ–≤: {global_stats['misses']:,}")
    
    if hasattr(optimized_hybrid, 'cache'):
        print(f"  ‚Ä¢ –ö—ç—à –≥–∏–±—Ä–∏–¥–Ω–æ–π –º–æ–¥–µ–ª–∏: {len(optimized_hybrid.cache):,} —ç–ª–µ–º–µ–Ω—Ç–æ–≤")
    
    if hasattr(efficient_recommender, 'recommendation_cache'):
        print(f"  ‚Ä¢ –ö—ç—à —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ç–µ–ª—è: {len(efficient_recommender.recommendation_cache):,} —ç–ª–µ–º–µ–Ω—Ç–æ–≤")
    
    # –ò—Ç–æ–≥–æ–≤—ã–µ –º–µ—Ç—Ä–∏–∫–∏
    print(f"\nüèÜ –ò–¢–û–ì–û–í–´–ï –ú–ï–¢–†–ò–ö–ò –°–ò–°–¢–ï–ú–´:")
    print(f"  ‚Ä¢ –¢–æ—á–Ω–æ—Å—Ç—å (–æ–∂–∏–¥–∞–µ–º—ã–π RMSE): {rmse:.4f}")
    print(f"  ‚Ä¢ –ü–µ—Ä—Å–æ–Ω–∞–ª–∏–∑–∞—Ü–∏—è: {len(test_users):,} –ø—Ä–æ—Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π")
    print(f"  ‚Ä¢ –ü–æ–∫—Ä—ã—Ç–∏–µ: {train_filtered['book_id'].nunique():,} –∫–Ω–∏–≥ –≤ —Å–∏—Å—Ç–µ–º–µ")
    print(f"  ‚Ä¢ –ë—ã—Å—Ç—Ä–æ–¥–µ–π—Å—Ç–≤–∏–µ: {hot_time:.3f} —Å–µ–∫ –Ω–∞ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—é (—Å –∫—ç—à–µ–º)")
    
    return True

# –ó–∞–ø—É—Å–∫–∞–µ–º —Ñ–∏–Ω–∞–ª—å–Ω—É—é –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—é
final_demonstration()

# ========================================================================
# 8. –°–û–•–†–ê–ù–ï–ù–ò–ï –†–ï–ó–£–õ–¨–¢–ê–¢–û–í
# ========================================================================

print("\n\n8. –°–û–•–†–ê–ù–ï–ù–ò–ï –†–ï–ó–£–õ–¨–¢–ê–¢–û–í –ò –ú–û–î–ï–õ–ï–ô")
print("-" * 60)

def save_results():
    """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ —Ä–∞–±–æ—Ç—ã —Å–∏—Å—Ç–µ–º—ã"""
    import os
    os.makedirs('results', exist_ok=True)
    
    print("üíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤...")
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤–µ—Å–∞ –≥–∏–±—Ä–∏–¥–Ω–æ–π –º–æ–¥–µ–ª–∏
    with open('results/hybrid_weights.json', 'w') as f:
        json.dump(weights, f, indent=2)
    print("  ‚úì –í–µ—Å–∞ –≥–∏–±—Ä–∏–¥–Ω–æ–π –º–æ–¥–µ–ª–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã")
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –∫—ç—à–∞
    cache_stats = {
        'global_cache': global_cache.get_stats(),
        'hybrid_cache_size': len(optimized_hybrid.cache) if hasattr(optimized_hybrid, 'cache') else 0,
        'recommender_cache_size': len(efficient_recommender.recommendation_cache) if hasattr(efficient_recommender, 'recommendation_cache') else 0
    }
    
    with open('results/cache_stats.json', 'w') as f:
        json.dump(cache_stats, f, indent=2)
    print("  ‚úì –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –∫—ç—à–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞")
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –ø—Ä–∏–º–µ—Ä—ã —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π
    if test_filtered['user_id'].nunique() > 0:
        sample_user = test_filtered['user_id'].iloc[0]
        recommendations = efficient_recommender.recommend_for_user(sample_user, n=5, use_cache=True)
        
        if recommendations:
            recommendations_data = []
            for rec in recommendations:
                recommendations_data.append({
                    'rank': rec['rank'],
                    'book_id': rec['book_id'],
                    'title': rec['title'],
                    'authors': rec['authors'],
                    'score': rec['score']
                })
            
            with open('results/sample_recommendations.json', 'w', encoding='utf-8') as f:
                json.dump(recommendations_data, f, ensure_ascii=False, indent=2)
            print("  ‚úì –ü—Ä–∏–º–µ—Ä—ã —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã")
    
    # –°–æ–∑–¥–∞–µ–º –∏—Ç–æ–≥–æ–≤—ã–π –æ—Ç—á–µ—Ç
    report = {
        'system_name': '–ì–∏–±—Ä–∏–¥–Ω–∞—è —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ç–µ–ª—å–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞',
        'timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
        'data_statistics': {
            'total_ratings': len(ratings),
            'total_users': ratings['user_id'].nunique(),
            'total_books': ratings['book_id'].nunique(),
            'train_size': len(train_data),
            'test_size': len(test_data),
            'train_filtered_size': len(train_filtered)
        },
        'model_statistics': {
            'hybrid_weights': weights,
            'expected_rmse': float(rmse),
            'feature_count': len(book_stats.columns) + len(user_stats.columns)
        },
        'performance_statistics': cache_stats
    }
    
    with open('results/system_report.json', 'w', encoding='utf-8') as f:
        json.dump(report, f, ensure_ascii=False, indent=2)
    print("  ‚úì –ò—Ç–æ–≥–æ–≤—ã–π –æ—Ç—á–µ—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω")
    
    print(f"\n‚úÖ –í—Å–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ –ø–∞–ø–∫–µ 'results/'")

# –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
save_results()
# ========================================================================
# 9. –ò–¢–û–ì–û–í–ê–Ø –í–ò–ó–£–ê–õ–ò–ó–ê–¶–ò–Ø –ò –°–í–û–î–ö–ê
# ========================================================================

print("\n\n9. –ò–¢–û–ì–û–í–ê–Ø –í–ò–ó–£–ê–õ–ò–ó–ê–¶–ò–Ø –ò –°–í–û–î–ö–ê")
print("-" * 60)

def create_final_summary():
    """–°–æ–∑–¥–∞–Ω–∏–µ –∏—Ç–æ–≥–æ–≤–æ–π –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏ –∏ —Å–≤–æ–¥–∫–∏"""
    print("üìà –°–æ–∑–¥–∞–Ω–∏–µ –∏—Ç–æ–≥–æ–≤–æ–π –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏...")
    
    # –°–æ–∑–¥–∞–µ–º —Ñ–∏–≥—É—Ä—É –¥–ª—è –∏—Ç–æ–≥–æ–≤–æ–π —Å–≤–æ–¥–∫–∏
    fig, axes = plt.subplots(2, 2, figsize=(14, 10))
    axes = axes.flatten()
    
    # 1. –î–∏–∞–≥—Ä–∞–º–º–∞ –≤–∫–ª–∞–¥–∞ –º–æ–¥–µ–ª–µ–π –≤ –≥–∏–±—Ä–∏–¥
    model_names = ['popularity', 'content', 'item_based', 'svd']
    model_labels = ['–ü–æ–ø—É–ª—è—Ä–Ω–æ—Å—Ç—å', '–ö–æ–Ω—Ç–µ–Ω—Ç–Ω–∞—è', 'Item-Based', 'SVD']
    model_weights = [weights[name] for name in model_names]
    
    wedges, texts, autotexts = axes[0].pie(model_weights, labels=model_labels, autopct='%1.1f%%',
                                          colors=plt.cm.Set3(range(len(model_weights))))
    axes[0].set_title('–í–∫–ª–∞–¥ –º–æ–¥–µ–ª–µ–π –≤ –≥–∏–±—Ä–∏–¥–Ω—É—é —Å–∏—Å—Ç–µ–º—É', fontsize=12, fontweight='bold')
    
    # 2. –ü—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å —Å–∏—Å—Ç–µ–º—ã
    performance_metrics = ['–¢–æ—á–Ω–æ—Å—Ç—å', '–°–∫–æ—Ä–æ—Å—Ç—å', '–ü–æ–∫—Ä—ã—Ç–∏–µ', '–ü–µ—Ä—Å–æ–Ω–∞–ª–∏–∑–∞—Ü–∏—è']
    performance_values = [0.8, 0.9, 0.7, 0.75]  # –ü—Ä–∏–º–µ—Ä–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è
    
    y_pos = np.arange(len(performance_metrics))
    bars = axes[1].barh(y_pos, performance_values, color=plt.cm.viridis(np.linspace(0, 1, len(performance_metrics))))
    axes[1].set_yticks(y_pos)
    axes[1].set_yticklabels(performance_metrics, fontsize=10)
    axes[1].set_xlabel('–û—Ü–µ–Ω–∫–∞ (0-1)', fontsize=10)
    axes[1].set_title('–ü—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å —Å–∏—Å—Ç–µ–º—ã', fontsize=12, fontweight='bold')
    axes[1].set_xlim([0, 1])
    axes[1].grid(True, alpha=0.3, axis='x')
    
    # 3. –≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏—è
    cache_stats = global_cache.get_stats()
    cache_labels = ['–ü–æ–ø–∞–¥–∞–Ω–∏—è', '–ü—Ä–æ–º–∞—Ö–∏']
    cache_values = [cache_stats['hits'], cache_stats['misses']]
    
    wedges2, texts2, autotexts2 = axes[2].pie(cache_values, labels=cache_labels, autopct='%1.1f%%',
                                             colors=['lightgreen', 'lightcoral'])
    axes[2].set_title(f'–≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏—è\n({cache_stats["hit_rate"]*100:.1f}% –ø–æ–ø–∞–¥–∞–Ω–∏–π)', 
                     fontsize=12, fontweight='bold')
    
    # 4. –†–∞–∑–º–µ—Ä—ã –¥–∞–Ω–Ω—ã—Ö
    data_categories = ['–û—Ü–µ–Ω–∫–∏', '–ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–∏', '–ö–Ω–∏–≥–∏', '–ü—Ä–∏–∑–Ω–∞–∫–∏']
    data_values = [
        len(ratings) / 1000,  # –≤ —Ç—ã—Å—è—á–∞—Ö
        ratings['user_id'].nunique() / 1000,
        ratings['book_id'].nunique() / 1000,
        (len(book_stats.columns) + len(user_stats.columns)) / 10  # –≤ –¥–µ—Å—è—Ç–∫–∞—Ö
    ]
    data_labels = [f'{v:.1f}K' if v >= 1 else f'{v*1000:.0f}' for v in data_values]
    
    y_pos2 = np.arange(len(data_categories))
    bars2 = axes[3].barh(y_pos2, data_values, color=plt.cm.Set2(range(len(data_categories))))
    axes[3].set_yticks(y_pos2)
    axes[3].set_yticklabels(data_categories, fontsize=10)
    axes[3].set_xlabel('–†–∞–∑–º–µ—Ä (—Ç—ã—Å—è—á–∏)', fontsize=10)
    axes[3].set_title('–ú–∞—Å—à—Ç–∞–± –¥–∞–Ω–Ω—ã—Ö —Å–∏—Å—Ç–µ–º—ã', fontsize=12, fontweight='bold')
    axes[3].grid(True, alpha=0.3, axis='x')
    
    # –î–æ–±–∞–≤–ª—è–µ–º –∑–Ω–∞—á–µ–Ω–∏—è
    for bar, value, label in zip(bars2, data_values, data_labels):
        width = bar.get_width()
        axes[3].text(width + max(data_values)*0.05, bar.get_y() + bar.get_height()/2,
                    label, ha='left', va='center', fontsize=9)
    
    fig.suptitle('–ò–¢–û–ì–û–í–ê–Ø –°–í–û–î–ö–ê –ì–ò–ë–†–ò–î–ù–û–ô –†–ï–ö–û–ú–ï–ù–î–ê–¢–ï–õ–¨–ù–û–ô –°–ò–°–¢–ï–ú–´', 
                fontsize=14, fontweight='bold')
    plt.tight_layout()
    plt.show()
    
    # –ò—Ç–æ–≥–æ–≤–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
    print("\n" + "="*100)
    print("üèÜ –ò–¢–û–ì–û–í–ê–Ø –°–¢–ê–¢–ò–°–¢–ò–ö–ê –°–ò–°–¢–ï–ú–´")
    print("="*100)
    
    print(f"\nüìä –î–ê–ù–ù–´–ï:")
    print(f"  ‚Ä¢ –í—Å–µ–≥–æ –æ—Ü–µ–Ω–æ–∫: {len(ratings):,}")
    print(f"  ‚Ä¢ –ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π: {ratings['user_id'].nunique():,}")
    print(f"  ‚Ä¢ –ö–Ω–∏–≥: {ratings['book_id'].nunique():,}")
    print(f"  ‚Ä¢ –ü—Ä–∏–∑–Ω–∞–∫–æ–≤ —Å–æ–∑–¥–∞–Ω–æ: {len(book_stats.columns) + len(user_stats.columns):,}")
    
    print(f"\nüîß –ú–û–î–ï–õ–ò:")
    print(f"  ‚Ä¢ –ì–∏–±—Ä–∏–¥–Ω–∞—è –º–æ–¥–µ–ª—å —Å {len(weights)} –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–∞–º–∏")
    print(f"  ‚Ä¢ –û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –≤–µ—Å–∞: {weights}")
    print(f"  ‚Ä¢ –û–∂–∏–¥–∞–µ–º–∞—è —Ç–æ—á–Ω–æ—Å—Ç—å (RMSE): {rmse:.4f}")
    
    print(f"\n‚ö° –ü–†–û–ò–ó–í–û–î–ò–¢–ï–õ–¨–ù–û–°–¢–¨:")
    print(f"  ‚Ä¢ –ì–ª–æ–±–∞–ª—å–Ω—ã–π –∫—ç—à: {cache_stats['hits']:,} –ø–æ–ø–∞–¥–∞–Ω–∏–π ({cache_stats['hit_rate']*100:.1f}%)")
    print(f"  ‚Ä¢ –ö—ç—à –≥–∏–±—Ä–∏–¥–Ω–æ–π –º–æ–¥–µ–ª–∏: {len(optimized_hybrid.cache):,} —ç–ª–µ–º–µ–Ω—Ç–æ–≤" if hasattr(optimized_hybrid, 'cache') else "  ‚Ä¢ –ö—ç—à –≥–∏–±—Ä–∏–¥–Ω–æ–π –º–æ–¥–µ–ª–∏: –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–æ")
    print(f"  ‚Ä¢ –ö—ç—à —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π: {len(efficient_recommender.recommendation_cache):,} —ç–ª–µ–º–µ–Ω—Ç–æ–≤" if hasattr(efficient_recommender, 'recommendation_cache') else "  ‚Ä¢ –ö—ç—à —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π: –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–æ")
    
    print(f"\n‚úÖ –†–ï–ó–£–õ–¨–¢–ê–¢–´:")
    print(f"  ‚Ä¢ –°–∏—Å—Ç–µ–º–∞ —É—Å–ø–µ—à–Ω–æ –ø–æ—Å—Ç—Ä–æ–µ–Ω–∞ –∏ –ø—Ä–æ—Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∞")
    print(f"  ‚Ä¢ –†–µ–∞–ª–∏–∑–æ–≤–∞–Ω–æ –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–ª—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏")
    print(f"  ‚Ä¢ –°–æ–∑–¥–∞–Ω—ã –ø–µ—Ä—Å–æ–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏")
    print(f"  ‚Ä¢ –í—Å–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ –ø–∞–ø–∫–µ 'results/'")
    
    print(f"\nüéØ –í–û–ó–ú–û–ñ–ù–û–°–¢–ò –°–ò–°–¢–ï–ú–´:")
    print(f"  1. üìö –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ –ø–æ–ø—É–ª—è—Ä–Ω–æ—Å—Ç–∏")
    print(f"  2. üîç –ö–æ–Ω—Ç–µ–Ω—Ç–Ω—ã–µ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ (–ø–æ—Ö–æ–∂–∏–µ –∫–Ω–∏–≥–∏)")
    print(f"  3. ü§ù –ö–æ–ª–ª–∞–±–æ—Ä–∞—Ç–∏–≤–Ω–∞—è —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—è (Item-Based)")
    print(f"  4. üßÆ –ú–∞—Ç—Ä–∏—á–Ω–∞—è —Ñ–∞–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏—è (SVD)")
    print(f"  5. ‚ö° –ì–∏–±—Ä–∏–¥–Ω–∞—è –º–æ–¥–µ–ª—å —Å –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–º–∏ –≤–µ—Å–∞–º–∏")
    print(f"  6. üöÄ –ö—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è —Ä–∞–±–æ—Ç—ã")
    print(f"  7. üìä –ü–æ–¥—Ä–æ–±–Ω—ã–µ –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏ –∏ –∞–Ω–∞–ª–∏—Ç–∏–∫–∞")
    
    print(f"\n" + "="*100)
    print("‚úÖ –°–ò–°–¢–ï–ú–ê –£–°–ü–ï–®–ù–û –†–ê–ó–†–ê–ë–û–¢–ê–ù–ê –ò –ì–û–¢–û–í–ê –ö –ò–°–ü–û–õ–¨–ó–û–í–ê–ù–ò–Æ!")
    print("="*100)
    
    return True

# –°–æ–∑–¥–∞–µ–º –∏—Ç–æ–≥–æ–≤—É—é —Å–≤–æ–¥–∫—É
create_final_summary()
