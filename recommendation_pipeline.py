# ========================================================================
# –ü–û–õ–ù–´–ô –°–ö–í–û–ó–ù–û–ô –ü–ê–ô–ü–õ–ê–ô–ù –†–ï–ö–û–ú–ï–ù–î–ê–¢–ï–õ–¨–ù–û–ô –°–ò–°–¢–ï–ú–´
# ========================================================================

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.model_selection import train_test_split
from surprise import Dataset, Reader, SVD
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_squared_error, mean_absolute_error
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
from collections import defaultdict, Counter
from tqdm import tqdm
import pickle
import json
import warnings
warnings.filterwarnings('ignore')

# –ù–∞—Å—Ç—Ä–æ–π–∫–∏
pd.set_option('display.max_columns', None)
plt.style.use('seaborn-v0_8-darkgrid')
sns.set_palette("husl")

# ========================================================================
# –≠–¢–ê–ü 1: –ü–û–î–ì–û–¢–û–í–ö–ê –î–ê–ù–ù–´–•
# ========================================================================

print("=" * 80)
print("–≠–¢–ê–ü 1: –ü–û–î–ì–û–¢–û–í–ö–ê –î–ê–ù–ù–´–•")
print("=" * 80)

class DataPreprocessor:
    """–ö–ª–∞—Å—Å –¥–ª—è –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∏ –¥–∞–Ω–Ω—ã—Ö"""
    
    def __init__(self):
        self.user_features = None
        self.book_features = None
        self.interaction_features = None
        
    def load_data(self):
        """–ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö"""
        print("–ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö...")
        
        try:
            self.ratings = pd.read_csv('goodbooks-10k/ratings.csv')
            self.books = pd.read_csv('goodbooks-10k/books.csv')
            self.tags = pd.read_csv('goodbooks-10k/tags.csv')
            self.book_tags = pd.read_csv('goodbooks-10k/book_tags.csv')
            
            print(f"‚úì –î–∞–Ω–Ω—ã–µ –∑–∞–≥—Ä—É–∂–µ–Ω—ã:")
            print(f"  ‚Ä¢ –û—Ü–µ–Ω–∫–∏: {self.ratings.shape}")
            print(f"  ‚Ä¢ –ö–Ω–∏–≥–∏: {self.books.shape}")
            print(f"  ‚Ä¢ –¢–µ–≥–∏: {self.tags.shape}")
            print(f"  ‚Ä¢ –¢–µ–≥–∏ –∫–Ω–∏–≥: {self.book_tags.shape}")
            
        except Exception as e:
            print(f"‚úó –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏: {e}")
            raise
    
    def create_temporal_split(self):
        """–°–æ–∑–¥–∞–Ω–∏–µ –≤—Ä–µ–º–µ–Ω–Ω–æ–≥–æ —Ä–∞–∑–¥–µ–ª–µ–Ω–∏—è –Ω–∞ train/test"""
        print("\n–°–æ–∑–¥–∞–Ω–∏–µ –≤—Ä–µ–º–µ–Ω–Ω–æ–≥–æ —Ä–∞–∑–¥–µ–ª–µ–Ω–∏—è...")
        
        # –î–æ–±–∞–≤–ª—è–µ–º –≤—Ä–µ–º–µ–Ω–Ω—ã–µ –º–µ—Ç–∫–∏ –µ—Å–ª–∏ –Ω–µ—Ç
        if 'timestamp' not in self.ratings.columns:
            np.random.seed(42)
            dates = pd.date_range('2010-01-01', '2020-12-31', periods=len(self.ratings))
            self.ratings['timestamp'] = dates
        
        # –°–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞ –ø–æ –≤—Ä–µ–º–µ–Ω–∏
        self.ratings = self.ratings.sort_values('timestamp')
        
        # –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ (80/20)
        train_size = int(0.8 * len(self.ratings))
        self.train_data = self.ratings.iloc[:train_size]
        self.test_data = self.ratings.iloc[train_size:]
        
        print(f"‚úì –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ —Å–æ–∑–¥–∞–Ω–æ:")
        print(f"  ‚Ä¢ Train: {len(self.train_data):,} –∑–∞–ø–∏—Å–µ–π")
        print(f"  ‚Ä¢ Test: {len(self.test_data):,} –∑–∞–ø–∏—Å–µ–π")
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –ø–µ—Ä–µ—Å–µ—á–µ–Ω–∏–π
        train_users = set(self.train_data['user_id'])
        train_books = set(self.train_data['book_id'])
        test_users = set(self.test_data['user_id'])
        test_books = set(self.test_data['book_id'])
        
        print(f"  ‚Ä¢ –ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π –≤ test –∏ train: {len(test_users & train_users) / len(test_users):.1%}")
        print(f"  ‚Ä¢ –ö–Ω–∏–≥ –≤ test –∏ train: {len(test_books & train_books) / len(test_books):.1%}")
    
    def create_user_features(self):
        """–°–æ–∑–¥–∞–Ω–∏–µ —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π"""
        print("\n–°–æ–∑–¥–∞–Ω–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π...")
        
        features = []
        
        for user_id in tqdm(self.train_data['user_id'].unique(), desc="–ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–∏"):
            user_ratings = self.train_data[self.train_data['user_id'] == user_id]
            
            # –ë–∞–∑–æ–≤—ã–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏
            rating_stats = {
                'user_id': user_id,
                'user_rating_count': len(user_ratings),
                'user_avg_rating': user_ratings['rating'].mean(),
                'user_rating_std': user_ratings['rating'].std(),
                'user_rating_median': user_ratings['rating'].median(),
                'user_rating_min': user_ratings['rating'].min(),
                'user_rating_max': user_ratings['rating'].max(),
            }
            
            # –í—Ä–µ–º–µ–Ω–Ω—ã–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã
            if 'timestamp' in user_ratings.columns:
                timestamps = user_ratings['timestamp'].sort_values()
                if len(timestamps) > 1:
                    intervals = np.diff(timestamps.values.astype(np.int64) // 10**9)
                    rating_stats['user_avg_time_interval'] = intervals.mean()
                    rating_stats['user_time_interval_std'] = intervals.std()
                else:
                    rating_stats['user_avg_time_interval'] = 0
                    rating_stats['user_time_interval_std'] = 0
            
            # –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –æ—Ü–µ–Ω–æ–∫
            for rating_val in [1, 2, 3, 4, 5]:
                count = (user_ratings['rating'] == rating_val).sum()
                rating_stats[f'user_rating_{rating_val}_count'] = count
                rating_stats[f'user_rating_{rating_val}_ratio'] = count / len(user_ratings) if len(user_ratings) > 0 else 0
            
            # –ê–∫—Ç–∏–≤–Ω–æ—Å—Ç—å (–∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è)
            if len(user_ratings) < 5:
                rating_stats['user_activity_level'] = 'low'
            elif len(user_ratings) < 20:
                rating_stats['user_activity_level'] = 'medium'
            else:
                rating_stats['user_activity_level'] = 'high'
            
            features.append(rating_stats)
        
        self.user_features = pd.DataFrame(features)
        print(f"‚úì –°–æ–∑–¥–∞–Ω–æ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–ª—è {len(self.user_features)} –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π")
    
    def create_book_features(self):
        """–°–æ–∑–¥–∞–Ω–∏–µ —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –∫–Ω–∏–≥"""
        print("\n–°–æ–∑–¥–∞–Ω–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –∫–Ω–∏–≥...")
        
        # –û–±—ä–µ–¥–∏–Ω—è–µ–º —Ç–µ–≥–∏ –∫–Ω–∏–≥
        book_tags_merged = pd.merge(self.book_tags, self.tags, on='tag_id', how='left')
        
        # –°–æ–∑–¥–∞–µ–º TF-IDF –≤–µ–∫—Ç–æ—Ä—ã –¥–ª—è —Ç–µ–≥–æ–≤
        print("  –°–æ–∑–¥–∞–Ω–∏–µ TF-IDF –≤–µ–∫—Ç–æ—Ä–æ–≤...")
        tag_vectors = {}
        
        for book_id in tqdm(self.books['book_id'].unique(), desc="TF-IDF –≤–µ–∫—Ç–æ—Ä—ã"):
            book_tags = book_tags_merged[book_tags_merged['goodreads_book_id'] == book_id]
            tags_text = ' '.join([str(tag) for tag in book_tags['tag_name'].fillna('').values])
            tag_vectors[book_id] = tags_text
        
        # –°–æ–∑–¥–∞–µ–º DataFrame —Å —Ç–µ–≥–∞–º–∏
        tag_df = pd.DataFrame(list(tag_vectors.items()), columns=['book_id', 'tags_text'])
        self.books = pd.merge(self.books, tag_df, on='book_id', how='left')
        
        # TF-IDF –≤–µ–∫—Ç–æ—Ä–Ω–∞—è –º–æ–¥–µ–ª—å
        self.tfidf_vectorizer = TfidfVectorizer(
            stop_words='english', 
            max_features=1000,
            ngram_range=(1, 2)
        )
        self.tfidf_matrix = self.tfidf_vectorizer.fit_transform(self.books['tags_text'].fillna(''))
        
        # –ú–∞—Ç—Ä–∏—Ü–∞ —Å—Ö–æ–¥—Å—Ç–≤–∞ –∫–Ω–∏–≥
        print("  –°–æ–∑–¥–∞–Ω–∏–µ –º–∞—Ç—Ä–∏—Ü—ã —Å—Ö–æ–¥—Å—Ç–≤–∞ –∫–Ω–∏–≥...")
        self.book_similarity_matrix = cosine_similarity(self.tfidf_matrix)
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –ø–æ –æ—Ü–µ–Ω–∫–∞–º –¥–ª—è –∫–∞–∂–¥–æ–π –∫–Ω–∏–≥–∏
        book_stats = self.train_data.groupby('book_id').agg({
            'rating': ['count', 'mean', 'std', 'median', 'min', 'max']
        }).reset_index()
        
        book_stats.columns = ['book_id', 'book_rating_count', 'book_avg_rating', 
                             'book_rating_std', 'book_rating_median', 
                             'book_rating_min', 'book_rating_max']
        
        # –î–æ–±–∞–≤–ª—è–µ–º —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏–µ –æ—Ü–µ–Ω–æ–∫ (—ç–Ω—Ç—Ä–æ–ø–∏—é)
        def calculate_rating_entropy(ratings):
            rating_counts = ratings.value_counts(normalize=True)
            return -sum(rating_counts * np.log2(rating_counts + 1e-10))
        
        book_entropy = self.train_data.groupby('book_id')['rating'].apply(calculate_rating_entropy)
        book_entropy.name = 'book_rating_entropy'
        book_stats = pd.merge(book_stats, book_entropy, on='book_id', how='left')
        
        # –ö–∞—Ç–µ–≥–æ—Ä–∏–∏ –ø–æ–ø—É–ª—è—Ä–Ω–æ—Å—Ç–∏
        def categorize_popularity(count):
            if count < 10: return 'very_low'
            elif count < 50: return 'low'
            elif count < 200: return 'medium'
            elif count < 500: return 'high'
            else: return 'very_high'
        
        book_stats['book_popularity_category'] = book_stats['book_rating_count'].apply(categorize_popularity)
        
        # –ö–∞—Ç–µ–≥–æ—Ä–∏–∏ —Ä–µ–π—Ç–∏–Ω–≥–∞
        def categorize_rating(rating):
            if rating < 2.5: return 'very_low'
            elif rating < 3.0: return 'low'
            elif rating < 3.5: return 'medium'
            elif rating < 4.0: return 'high'
            else: return 'very_high'
        
        book_stats['book_rating_category'] = book_stats['book_avg_rating'].apply(categorize_rating)
        
        # –û–±—ä–µ–¥–∏–Ω—è–µ–º —Å –æ—Å–Ω–æ–≤–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π –æ –∫–Ω–∏–≥–∞—Ö
        self.book_features = pd.merge(self.books, book_stats, on='book_id', how='left')
        
        # –ó–∞–ø–æ–ª–Ω—è–µ–º –ø—Ä–æ–ø—É—Å–∫–∏
        numeric_cols = self.book_features.select_dtypes(include=[np.number]).columns
        self.book_features[numeric_cols] = self.book_features[numeric_cols].fillna(self.book_features[numeric_cols].median())
        
        print(f"‚úì –°–æ–∑–¥–∞–Ω–æ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–ª—è {len(self.book_features)} –∫–Ω–∏–≥")
    
    def create_interaction_features(self):
        """–°–æ–∑–¥–∞–Ω–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏–π"""
        print("\n–°–æ–∑–¥–∞–Ω–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏–π...")
        
        features = []
        
        # –î–ª—è –∫–∞–∂–¥–æ–π –ø–∞—Ä—ã –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å-–∫–Ω–∏–≥–∞ –≤ —Ç—Ä–µ–π–Ω–µ
        for idx, row in tqdm(self.train_data.iterrows(), total=len(self.train_data), desc="–í–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—è"):
            user_id = row['user_id']
            book_id = row['book_id']
            rating = row['rating']
            
            # –ü–æ–ª—É—á–∞–µ–º –ø—Ä–∏–∑–Ω–∞–∫–∏ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è –∏ –∫–Ω–∏–≥–∏
            user_feat = self.user_features[self.user_features['user_id'] == user_id].iloc[0] if user_id in self.user_features['user_id'].values else None
            book_feat = self.book_features[self.book_features['book_id'] == book_id].iloc[0] if book_id in self.book_features['book_id'].values else None
            
            if user_feat is not None and book_feat is not None:
                # –í—ã—á–∏—Å–ª—è–µ–º —Å—Ö–æ–∂–µ—Å—Ç—å —Å –∏—Å—Ç–æ—Ä–∏–µ–π –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
                similarity_score = 0
                if user_id in self.train_data['user_id'].values:
                    user_books = self.train_data[self.train_data['user_id'] == user_id]['book_id'].values
                    if len(user_books) > 0:
                        # –î–ª—è –∫–∞–∂–¥–æ–π –∫–Ω–∏–≥–∏ –≤ –∏—Å—Ç–æ—Ä–∏–∏ –≤—ã—á–∏—Å–ª—è–µ–º —Å—Ö–æ–¥—Å—Ç–≤–æ
                        similarities = []
                        for ub in user_books:
                            if ub in self.book_features['book_id'].values and book_id in self.book_features['book_id'].values:
                                idx1 = self.book_features[self.book_features['book_id'] == ub].index[0]
                                idx2 = self.book_features[self.book_features['book_id'] == book_id].index[0]
                                similarities.append(self.book_similarity_matrix[idx1][idx2])
                        similarity_score = np.mean(similarities) if similarities else 0
                
                # –†–∞–∑–Ω–∏—Ü–∞ –º–µ–∂–¥—É —Å—Ä–µ–¥–Ω–µ–π –æ—Ü–µ–Ω–∫–æ–π –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è –∏ —Å—Ä–µ–¥–Ω–µ–π –æ—Ü–µ–Ω–∫–æ–π –∫–Ω–∏–≥–∏
                rating_diff = abs(user_feat['user_avg_rating'] - book_feat['book_avg_rating']) if not pd.isna(user_feat['user_avg_rating']) else 0
                
                # –í–µ—Å –∫–Ω–∏–≥–∏ (–ø–æ–ø—É–ª—è—Ä–Ω–æ—Å—Ç—å * –∫–∞—á–µ—Å—Ç–≤–æ)
                book_weight = book_feat['book_rating_count'] * book_feat['book_avg_rating'] / 100
                
                features.append({
                    'user_id': user_id,
                    'book_id': book_id,
                    'rating': rating,
                    'similarity_score': similarity_score,
                    'rating_diff': rating_diff,
                    'book_weight': book_weight,
                    'user_book_rating_std_diff': abs(user_feat['user_rating_std'] - book_feat['book_rating_std']) if not pd.isna(user_feat['user_rating_std']) else 0
                })
        
        self.interaction_features = pd.DataFrame(features)
        print(f"‚úì –°–æ–∑–¥–∞–Ω–æ {len(self.interaction_features)} –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏–π")
    
    def prepare_all_features(self):
        """–ó–∞–ø—É—Å–∫ –≤—Å–µ–π –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∏ –¥–∞–Ω–Ω—ã—Ö"""
        self.load_data()
        self.create_temporal_split()
        self.create_user_features()
        self.create_book_features()
        self.create_interaction_features()
        
        print("\n" + "="*80)
        print("–°–í–û–î–ö–ê –ü–û –ü–û–î–ì–û–¢–û–í–ö–ï –î–ê–ù–ù–´–•:")
        print("="*80)
        print(f"‚Ä¢ –ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–∏: {len(self.user_features)} –∑–∞–ø–∏—Å–µ–π")
        print(f"‚Ä¢ –ö–Ω–∏–≥–∏: {len(self.book_features)} –∑–∞–ø–∏—Å–µ–π")
        print(f"‚Ä¢ –í–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—è: {len(self.interaction_features)} –∑–∞–ø–∏—Å–µ–π")
        print(f"‚Ä¢ –ü—Ä–∏–∑–Ω–∞–∫–∏ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π: {self.user_features.shape[1]} —Å—Ç–æ–ª–±—Ü–æ–≤")
        print(f"‚Ä¢ –ü—Ä–∏–∑–Ω–∞–∫–∏ –∫–Ω–∏–≥: {self.book_features.shape[1]} —Å—Ç–æ–ª–±—Ü–æ–≤")
        
        return self.user_features, self.book_features, self.interaction_features

# ========================================================================
# –≠–¢–ê–ü 2: –ü–û–°–¢–†–û–ï–ù–ò–ï –ì–ò–ë–†–ò–î–ù–û–ô –°–ò–°–¢–ï–ú–´
# ========================================================================

print("\n" + "="*80)
print("–≠–¢–ê–ü 2: –ü–û–°–¢–†–û–ï–ù–ò–ï –ì–ò–ë–†–ò–î–ù–û–ô –°–ò–°–¢–ï–ú–´")
print("="*80)

class HybridRecommenderSystem:
    """–ì–∏–±—Ä–∏–¥–Ω–∞—è —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ç–µ–ª—å–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞"""
    
    def __init__(self, preprocessor):
        self.preprocessor = preprocessor
        self.models = {}
        self.user_segments = {}
        
    def segment_users(self):
        """–°–µ–≥–º–µ–Ω—Ç–∞—Ü–∏—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π –ø–æ —Ç–∏–ø–∞–º"""
        print("\n–°–µ–≥–º–µ–Ω—Ç–∞—Ü–∏—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π...")
        
        # –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π –ø–æ –∞–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏
        for user_id in self.preprocessor.train_data['user_id'].unique():
            user_ratings = self.preprocessor.train_data[self.preprocessor.train_data['user_id'] == user_id]
            rating_count = len(user_ratings)
            
            if rating_count < 5:
                segment = 'new_user'
            elif rating_count < 20:
                segment = 'active_user'
            else:
                segment = 'power_user'
            
            self.user_segments[user_id] = segment
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ —Å–µ–≥–º–µ–Ω—Ç–æ–≤
        segment_counts = Counter(self.user_segments.values())
        print("‚úì –°–µ–≥–º–µ–Ω—Ç–∞—Ü–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞:")
        for segment, count in segment_counts.items():
            print(f"  ‚Ä¢ {segment}: {count} –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π ({count/len(self.user_segments):.1%})")
    
    def train_popularity_model(self):
        """–ú–æ–¥–µ–ª—å –ø–æ–ø—É–ª—è—Ä–Ω–æ—Å—Ç–∏"""
        print("\n–û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ –ø–æ–ø—É–ª—è—Ä–Ω–æ—Å—Ç–∏...")
        
        # –ë–∞–π–µ—Å–æ–≤—Å–∫–æ–µ —Å—Ä–µ–¥–Ω–µ–µ –¥–ª—è —É—Å—Ç–æ–π—á–∏–≤–æ—Å—Ç–∏
        popularity = self.preprocessor.train_data.groupby('book_id').agg({
            'rating': ['mean', 'count']
        }).reset_index()
        
        popularity.columns = ['book_id', 'avg_rating', 'rating_count']
        
        # –î–æ–±–∞–≤–ª—è–µ–º —à—Ç—Ä–∞—Ñ –∑–∞ –º–∞–ª–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –æ—Ü–µ–Ω–æ–∫
        C = popularity['avg_rating'].mean()
        m = popularity['rating_count'].quantile(0.5)
        
        popularity['bayesian_score'] = (
            (popularity['rating_count'] * popularity['avg_rating'] + C * m) / 
            (popularity['rating_count'] + m)
        )
        
        popularity = popularity.sort_values('bayesian_score', ascending=False)
        self.models['popularity'] = popularity
        
        print(f"‚úì –û–±—É—á–µ–Ω–æ –Ω–∞ {len(popularity)} –∫–Ω–∏–≥–∞—Ö")
    
    def train_content_based_model(self):
        """Content-based –º–æ–¥–µ–ª—å"""
        print("\n–û–±—É—á–µ–Ω–∏–µ Content-based –º–æ–¥–µ–ª–∏...")
        
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º TF-IDF –º–∞—Ç—Ä–∏—Ü—É –∏–∑ –ø—Ä–µ–ø—Ä–æ—Ü–µ—Å—Å–æ—Ä–∞
        self.models['content_based'] = {
            'tfidf_matrix': self.preprocessor.tfidf_matrix,
            'similarity_matrix': self.preprocessor.book_similarity_matrix,
            'book_features': self.preprocessor.book_features
        }
        
        print("‚úì Content-based –º–æ–¥–µ–ª—å –≥–æ—Ç–æ–≤–∞")
    
    def train_collaborative_model(self):
        """Collaborative Filtering –º–æ–¥–µ–ª—å (SVD)"""
        print("\n–û–±—É—á–µ–Ω–∏–µ Collaborative Filtering –º–æ–¥–µ–ª–∏...")
        
        # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è Surprise
        reader = Reader(rating_scale=(1, 5))
        data = Dataset.load_from_df(
            self.preprocessor.train_data[['user_id', 'book_id', 'rating']], 
            reader
        )
        trainset = data.build_full_trainset()
        
        # –û–±—É—á–µ–Ω–∏–µ SVD —Å –Ω–∞—Å—Ç—Ä–æ–π–∫–æ–π –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
        svd = SVD(
            n_factors=150,
            n_epochs=25,
            lr_all=0.007,
            reg_all=0.03,
            random_state=42
        )
        svd.fit(trainset)
        
        self.models['collaborative'] = svd
        print("‚úì Collaborative Filtering –º–æ–¥–µ–ª—å –æ–±—É—á–µ–Ω–∞")
    
    def train_hybrid_model(self):
        """–ì–∏–±—Ä–∏–¥–Ω–∞—è –º–æ–¥–µ–ª—å —Å –æ–±—É—á–∞–µ–º—ã–º–∏ –≤–µ—Å–∞–º–∏"""
        print("\n–û–±—É—á–µ–Ω–∏–µ –≥–∏–±—Ä–∏–¥–Ω–æ–π –º–æ–¥–µ–ª–∏...")
        
        # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è
        train_samples = self.preprocessor.interaction_features.sample(
            min(50000, len(self.preprocessor.interaction_features)),
            random_state=42
        )
        
        X = []
        y = []
        
        for idx, row in tqdm(train_samples.iterrows(), total=len(train_samples), desc="–ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤"):
            user_id = row['user_id']
            book_id = row['book_id']
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º –ø—Ä–∏–∑–Ω–∞–∫–∏ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è
            features = self._extract_hybrid_features(user_id, book_id)
            X.append(features)
            y.append(row['rating'])
        
        X = np.array(X)
        y = np.array(y)
        
        # –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –Ω–∞ train/validation
        X_train, X_val, y_train, y_val = train_test_split(
            X, y, test_size=0.2, random_state=42
        )
        
        # –û–±—É—á–µ–Ω–∏–µ Random Forest
        rf = RandomForestRegressor(
            n_estimators=100,
            max_depth=15,
            min_samples_split=10,
            min_samples_leaf=5,
            random_state=42,
            n_jobs=-1
        )
        
        rf.fit(X_train, y_train)
        
        # –û—Ü–µ–Ω–∫–∞
        train_pred = rf.predict(X_train)
        val_pred = rf.predict(X_val)
        
        train_rmse = np.sqrt(mean_squared_error(y_train, train_pred))
        val_rmse = np.sqrt(mean_squared_error(y_val, val_pred))
        
        self.models['hybrid'] = rf
        
        print(f"‚úì –ì–∏–±—Ä–∏–¥–Ω–∞—è –º–æ–¥–µ–ª—å –æ–±—É—á–µ–Ω–∞:")
        print(f"  ‚Ä¢ Train RMSE: {train_rmse:.4f}")
        print(f"  ‚Ä¢ Validation RMSE: {val_rmse:.4f}")
    
    def _extract_hybrid_features(self, user_id, book_id):
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–ª—è –≥–∏–±—Ä–∏–¥–Ω–æ–π –º–æ–¥–µ–ª–∏"""
        features = []
        
        # 1. –ü—Ä–∏–∑–Ω–∞–∫–∏ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
        if user_id in self.preprocessor.user_features['user_id'].values:
            user_row = self.preprocessor.user_features[self.preprocessor.user_features['user_id'] == user_id].iloc[0]
            features.extend([
                user_row['user_rating_count'],
                user_row['user_avg_rating'],
                user_row['user_rating_std'],
                1 if user_row['user_activity_level'] == 'low' else 0,
                1 if user_row['user_activity_level'] == 'medium' else 0,
                1 if user_row['user_activity_level'] == 'high' else 0
            ])
        else:
            features.extend([0, 3.0, 0, 1, 0, 0])  # –ó–Ω–∞—á–µ–Ω–∏—è –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é –¥–ª—è –Ω–æ–≤–æ–≥–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
        
        # 2. –ü—Ä–∏–∑–Ω–∞–∫–∏ –∫–Ω–∏–≥–∏
        if book_id in self.preprocessor.book_features['book_id'].values:
            book_row = self.preprocessor.book_features[self.preprocessor.book_features['book_id'] == book_id].iloc[0]
            features.extend([
                book_row['book_rating_count'],
                book_row['book_avg_rating'],
                book_row['book_rating_std'],
                book_row['book_rating_entropy'],
                1 if book_row['book_popularity_category'] == 'very_low' else 0,
                1 if book_row['book_popularity_category'] == 'low' else 0,
                1 if book_row['book_popularity_category'] == 'medium' else 0,
                1 if book_row['book_popularity_category'] == 'high' else 0,
                1 if book_row['book_popularity_category'] == 'very_high' else 0
            ])
        else:
            features.extend([0, 3.0, 0, 0, 0, 0, 1, 0, 0])  # –ó–Ω–∞—á–µ–Ω–∏—è –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é
        
        # 3. Content-based –ø—Ä–∏–∑–Ω–∞–∫–∏ (—Å—Ö–æ–¥—Å—Ç–≤–æ)
        if user_id in self.preprocessor.train_data['user_id'].values:
            user_books = self.preprocessor.train_data[self.preprocessor.train_data['user_id'] == user_id]['book_id'].values
            if len(user_books) > 0 and book_id in self.preprocessor.book_features['book_id'].values:
                similarities = []
                for ub in user_books[:10]:  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–ª—è —Å–∫–æ—Ä–æ—Å—Ç–∏
                    if ub in self.preprocessor.book_features['book_id'].values:
                        idx1 = self.preprocessor.book_features[self.preprocessor.book_features['book_id'] == ub].index[0]
                        idx2 = self.preprocessor.book_features[self.preprocessor.book_features['book_id'] == book_id].index[0]
                        similarities.append(self.preprocessor.book_similarity_matrix[idx1][idx2])
                features.append(np.mean(similarities) if similarities else 0)
            else:
                features.append(0)
        else:
            features.append(0)
        
        # 4. Collaborative Filtering –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ
        try:
            if hasattr(self.models.get('collaborative'), 'predict'):
                pred = self.models['collaborative'].predict(user_id, book_id)
                features.append(pred.est)
            else:
                features.append(3.0)
        except:
            features.append(3.0)
        
        return np.array(features)
    
    def generate_candidate_pool(self, user_id, top_n=100):
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –ø—É–ª–∞ –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤ –∏–∑ –≤—Å–µ—Ö –º–æ–¥–µ–ª–µ–π"""
        candidates = set()
        
        # 1. –ü–æ –ø–æ–ø—É–ª—è—Ä–Ω–æ—Å—Ç–∏ (–¥–ª—è –≤—Å–µ—Ö –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π)
        popularity_rec = self.models['popularity'].head(50)['book_id'].tolist()
        candidates.update(popularity_rec)
        
        # 2. –ü–æ —Å–µ–≥–º–µ–Ω—Ç—É –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
        user_segment = self.user_segments.get(user_id, 'new_user')
        
        if user_segment == 'new_user':
            # –î–ª—è –Ω–æ–≤—ã—Ö –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π - –±–æ–ª—å—à–µ –ø–æ–ø—É–ª—è—Ä–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–Ω—Ç–∞
            trending_books = self.models['popularity'].head(100)['book_id'].tolist()
            candidates.update(trending_books)
            
        elif user_segment == 'active_user':
            # –î–ª—è –∞–∫—Ç–∏–≤–Ω—ã—Ö - –∫–æ–º–±–∏–Ω–∞—Ü–∏—è –ø–æ–¥—Ö–æ–¥–æ–≤
            # Content-based
            if user_id in self.preprocessor.train_data['user_id'].values:
                user_books = self.preprocessor.train_data[self.preprocessor.train_data['user_id'] == user_id]['book_id'].values
                if len(user_books) > 0:
                    for book_id in user_books[:5]:  # –ë–µ—Ä–µ–º 5 –ø–æ—Å–ª–µ–¥–Ω–∏—Ö –∫–Ω–∏–≥
                        if book_id in self.preprocessor.book_features['book_id'].values:
                            idx = self.preprocessor.book_features[self.preprocessor.book_features['book_id'] == book_id].index[0]
                            sim_scores = list(enumerate(self.preprocessor.book_similarity_matrix[idx]))
                            sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)[:20]
                            for sim_idx, score in sim_scores:
                                candidates.add(self.preprocessor.book_features.iloc[sim_idx]['book_id'])
            
            # Collaborative
            try:
                # –ë–µ—Ä–µ–º –ø–æ–ø—É–ª—è—Ä–Ω—ã–µ –∫–Ω–∏–≥–∏ –¥–ª—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è
                popular_books = self.models['popularity'].head(200)['book_id'].tolist()
                predictions = []
                for book_id in popular_books[:50]:
                    try:
                        pred = self.models['collaborative'].predict(user_id, book_id)
                        predictions.append((book_id, pred.est))
                    except:
                        continue
                predictions.sort(key=lambda x: x[1], reverse=True)
                candidates.update([b for b, _ in predictions[:30]])
            except:
                pass
        
        else:  # power_user
            # –î–ª—è –æ–ø—ã—Ç–Ω—ã—Ö - –ø–µ—Ä—Å–æ–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏
            # Collaborative —Å –±–æ–ª—å—à–∏–º –ø—É–ª–æ–º
            try:
                popular_books = self.models['popularity'].head(500)['book_id'].tolist()
                predictions = []
                for book_id in popular_books[:100]:
                    try:
                        pred = self.models['collaborative'].predict(user_id, book_id)
                        predictions.append((book_id, pred.est))
                    except:
                        continue
                predictions.sort(key=lambda x: x[1], reverse=True)
                candidates.update([b for b, _ in predictions[:50]])
            except:
                pass
        
        # –ò—Å–∫–ª—é—á–∞–µ–º —É–∂–µ –ø—Ä–æ—á–∏—Ç–∞–Ω–Ω—ã–µ –∫–Ω–∏–≥–∏
        if user_id in self.preprocessor.train_data['user_id'].values:
            read_books = set(self.preprocessor.train_data[self.preprocessor.train_data['user_id'] == user_id]['book_id'])
            candidates = candidates - read_books
        
        # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º —Ä–∞–∑–º–µ—Ä –ø—É–ª–∞
        return list(candidates)[:top_n]
    
    def rank_candidates(self, user_id, candidates, top_n=20, diversity_weight=0.2):
        """–†–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–∏–µ –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤ —Å —É—á–µ—Ç–æ–º —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏—è"""
        if not candidates:
            return []
        
        # –í—ã—á–∏—Å–ª—è–µ–º —Å–∫–æ—Ä—ã –¥–ª—è –≤—Å–µ—Ö –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤
        scores = []
        candidate_features = []
        
        for book_id in candidates:
            try:
                # –ü–æ–ª—É—á–∞–µ–º –ø—Ä–∏–∑–Ω–∞–∫–∏
                features = self._extract_hybrid_features(user_id, book_id)
                # –ü—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞–µ–º —Ä–µ–π—Ç–∏–Ω–≥
                score = self.models['hybrid'].predict(features.reshape(1, -1))[0]
                scores.append((book_id, score))
                candidate_features.append(features)
            except Exception as e:
                # –ï—Å–ª–∏ –æ—à–∏–±–∫–∞, –∏—Å–ø–æ–ª—å–∑—É–µ–º —Å—Ä–µ–¥–Ω–∏–π —Å–∫–æ—Ä
                scores.append((book_id, 3.0))
                candidate_features.append(np.zeros(29))  # 29 –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
        
        # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ —Å–∫–æ—Ä—É
        scores.sort(key=lambda x: x[1], reverse=True)
        
        # –ü—Ä–∏–º–µ–Ω—è–µ–º Maximal Marginal Relevance (MMR) –¥–ª—è —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏—è
        if diversity_weight > 0 and len(scores) > 1:
            selected = []
            remaining = scores.copy()
            
            # –ù–∞—á–∏–Ω–∞–µ–º —Å –ª—É—á—à–µ–≥–æ
            selected.append(remaining.pop(0))
            
            while len(selected) < min(top_n, len(scores)) and remaining:
                best_mmr = -float('inf')
                best_idx = -1
                
                for i, (candidate_id, candidate_score) in enumerate(remaining):
                    # Relevance
                    relevance = candidate_score
                    
                    # Diversity (–º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ —Å—Ö–æ–¥—Å—Ç–≤–æ —Å —É–∂–µ –≤—ã–±—Ä–∞–Ω–Ω—ã–º–∏)
                    max_similarity = 0
                    if candidate_id in self.preprocessor.book_features['book_id'].values:
                        cand_idx = self.preprocessor.book_features[self.preprocessor.book_features['book_id'] == candidate_id].index[0]
                        for sel_id, _ in selected:
                            if sel_id in self.preprocessor.book_features['book_id'].values:
                                sel_idx = self.preprocessor.book_features[self.preprocessor.book_features['book_id'] == sel_id].index[0]
                                similarity = self.preprocessor.book_similarity_matrix[cand_idx][sel_idx]
                                max_similarity = max(max_similarity, similarity)
                    
                    # MMR score
                    mmr = (1 - diversity_weight) * relevance - diversity_weight * max_similarity
                    
                    if mmr > best_mmr:
                        best_mmr = mmr
                        best_idx = i
                
                if best_idx >= 0:
                    selected.append(remaining.pop(best_idx))
                else:
                    break
            
            return [book_id for book_id, _ in selected]
        else:
            return [book_id for book_id, _ in scores[:top_n]]
    
    def recommend(self, user_id, top_n=10):
        """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π"""
        # 1. –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –ø—É–ª–∞ –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤
        candidates = self.generate_candidate_pool(user_id, top_n=100)
        
        # 2. –†–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–∏–µ –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤
        recommendations = self.rank_candidates(user_id, candidates, top_n=top_n)
        
        # 3. –ï—Å–ª–∏ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π –º–∞–ª–æ, –¥–æ–±–∞–≤–ª—è–µ–º –ø–æ–ø—É–ª—è—Ä–Ω—ã–µ
        if len(recommendations) < top_n:
            popular_books = self.models['popularity'].head(top_n * 2)['book_id'].tolist()
            for book_id in popular_books:
                if book_id not in recommendations:
                    # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –Ω–µ —á–∏—Ç–∞–ª –ª–∏ —É–∂–µ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å
                    if user_id in self.preprocessor.train_data['user_id'].values:
                        read_books = set(self.preprocessor.train_data[self.preprocessor.train_data['user_id'] == user_id]['book_id'])
                        if book_id not in read_books:
                            recommendations.append(book_id)
                    else:
                        recommendations.append(book_id)
                
                if len(recommendations) >= top_n:
                    break
        
        return recommendations[:top_n]
    
    def train_all_models(self):
        """–û–±—É—á–µ–Ω–∏–µ –≤—Å–µ—Ö –º–æ–¥–µ–ª–µ–π"""
        self.segment_users()
        self.train_popularity_model()
        self.train_content_based_model()
        self.train_collaborative_model()
        self.train_hybrid_model()

# ========================================================================
# –ü–†–û–î–í–ò–ù–£–¢–ê–Ø –ß–ê–°–¢–¨: –ù–ï–ô–†–û–°–ï–¢–ï–í–û–ô –ü–û–î–•–û–î
# ========================================================================

print("\n" + "="*80)
print("–ü–†–û–î–í–ò–ù–£–¢–ê–Ø –ß–ê–°–¢–¨: –ù–ï–ô–†–û–°–ï–¢–ï–í–û–ô –ü–û–î–•–û–î")
print("="*80)

class NeuralRecommender:
    """Two-Tower –Ω–µ–π—Ä–æ—Å–µ—Ç–µ–≤–∞—è –º–æ–¥–µ–ª—å"""
    
    def __init__(self, preprocessor, embedding_dim=64):
        self.preprocessor = preprocessor
        self.embedding_dim = embedding_dim
        self.model = None
        self.user_encoder = LabelEncoder()
        self.book_encoder = LabelEncoder()
        
    def prepare_data(self):
        """–ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –Ω–µ–π—Ä–æ—Å–µ—Ç–∏"""
        print("–ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –Ω–µ–π—Ä–æ—Å–µ—Ç–∏...")
        
        # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º user_id –∏ book_id
        all_user_ids = np.concatenate([
            self.preprocessor.train_data['user_id'].values,
            self.preprocessor.test_data['user_id'].values
        ])
        
        all_book_ids = np.concatenate([
            self.preprocessor.train_data['book_id'].values,
            self.preprocessor.test_data['book_id'].values
        ])
        
        # –ö–æ–¥–∏—Ä—É–µ–º ID
        self.user_encoder.fit(all_user_ids)
        self.book_encoder.fit(all_book_ids)
        
        # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º train –¥–∞–Ω–Ω—ã–µ
        train_users = self.user_encoder.transform(self.preprocessor.train_data['user_id'])
        train_books = self.book_encoder.transform(self.preprocessor.train_data['book_id'])
        train_ratings = self.preprocessor.train_data['rating'].values
        
        # –°–æ–∑–¥–∞–µ–º –æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω—ã–µ —Å—ç–º–ø–ª—ã
        print("–°–æ–∑–¥–∞–Ω–∏–µ –æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω—ã—Ö —Å—ç–º–ø–ª–æ–≤...")
        positive_pairs = set(zip(train_users, train_books))
        
        negative_samples = []
        n_negative = len(positive_pairs)  # –°—Ç–æ–ª—å–∫–æ –∂–µ –Ω–µ–≥–∞—Ç–∏–≤–Ω—ã—Ö
        
        unique_users = np.unique(train_users)
        unique_books = np.unique(train_books)
        
        for _ in tqdm(range(n_negative), desc="–ù–µ–≥–∞—Ç–∏–≤–Ω—ã–µ —Å—ç–º–ø–ª—ã"):
            user = np.random.choice(unique_users)
            book = np.random.choice(unique_books)
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ —ç—Ç–æ –Ω–µ –ø–æ–∑–∏—Ç–∏–≤–Ω–∞—è –ø–∞—Ä–∞
            while (user, book) in positive_pairs:
                book = np.random.choice(unique_books)
            
            negative_samples.append((user, book, 0))  # 0 - –Ω–µ–≥–∞—Ç–∏–≤–Ω—ã–π –∫–ª–∞—Å—Å
        
        # –û–±—ä–µ–¥–∏–Ω—è–µ–º –ø–æ–∑–∏—Ç–∏–≤–Ω—ã–µ –∏ –Ω–µ–≥–∞—Ç–∏–≤–Ω—ã–µ —Å—ç–º–ø–ª—ã
        positive_samples = list(zip(train_users, train_books, [1]*len(train_users)))  # 1 - –ø–æ–∑–∏—Ç–∏–≤–Ω—ã–π –∫–ª–∞—Å—Å
        all_samples = positive_samples + negative_samples
        
        np.random.shuffle(all_samples)
        
        # –†–∞–∑–¥–µ–ª—è–µ–º –Ω–∞ X –∏ y
        X_users = np.array([s[0] for s in all_samples])
        X_books = np.array([s[1] for s in all_samples])
        y = np.array([s[2] for s in all_samples])
        
        # –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –Ω–∞ train/validation
        split_idx = int(0.8 * len(X_users))
        
        self.X_train = (X_users[:split_idx], X_books[:split_idx])
        self.X_val = (X_users[split_idx:], X_books[split_idx:])
        self.y_train = y[:split_idx]
        self.y_val = y[split_idx:]
        
        print(f"‚úì –î–∞–Ω–Ω—ã–µ –ø–æ–¥–≥–æ—Ç–æ–≤–ª–µ–Ω—ã:")
        print(f"  ‚Ä¢ –£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π: {len(self.user_encoder.classes_)}")
        print(f"  ‚Ä¢ –£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –∫–Ω–∏–≥: {len(self.book_encoder.classes_)}")
        print(f"  ‚Ä¢ –¢—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω—ã—Ö —Å—ç–º–ø–ª–æ–≤: {len(self.X_train[0])}")
        print(f"  ‚Ä¢ –í–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω—ã—Ö —Å—ç–º–ø–ª–æ–≤: {len(self.X_val[0])}")
    
    def build_model(self):
        """–ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ Two-Tower –º–æ–¥–µ–ª–∏"""
        print("\n–ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ Two-Tower –º–æ–¥–µ–ª–∏...")
        
        # –í—Ö–æ–¥—ã
        user_input = keras.Input(shape=(1,), name="user_input")
        book_input = keras.Input(shape=(1,), name="book_input")
        
        # –≠–º–±–µ–¥–¥–∏–Ω–≥–∏
        n_users = len(self.user_encoder.classes_)
        n_books = len(self.book_encoder.classes_)
        
        user_embedding = layers.Embedding(
            input_dim=n_users + 1,
            output_dim=self.embedding_dim,
            embeddings_initializer='he_normal',
            name="user_embedding"
        )(user_input)
        
        book_embedding = layers.Embedding(
            input_dim=n_books + 1,
            output_dim=self.embedding_dim,
            embeddings_initializer='he_normal',
            name="book_embedding"
        )(book_input)
        
        # Flatten
        user_flat = layers.Flatten()(user_embedding)
        book_flat = layers.Flatten()(book_embedding)
        
        # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ —Å–ª–æ–∏ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —Ç–∞—É—ç—Ä–∞
        user_dense = layers.Dense(128, activation='relu')(user_flat)
        user_dense = layers.Dropout(0.3)(user_dense)
        user_dense = layers.Dense(64, activation='relu')(user_dense)
        
        book_dense = layers.Dense(128, activation='relu')(book_flat)
        book_dense = layers.Dropout(0.3)(book_dense)
        book_dense = layers.Dense(64, activation='relu')(book_dense)
        
        # –°–∫–∞–ª—è—Ä–Ω–æ–µ –ø—Ä–æ–∏–∑–≤–µ–¥–µ–Ω–∏–µ (–∫–∞–∫ –≤ Two-Tower)
        dot_product = layers.Dot(axes=1, normalize=False)([user_dense, book_dense])
        
        # –í—ã—Ö–æ–¥–Ω–æ–π —Å–ª–æ–π
        output = layers.Dense(1, activation='sigmoid')(dot_product)
        
        # –ú–æ–¥–µ–ª—å
        self.model = keras.Model(
            inputs=[user_input, book_input],
            outputs=output,
            name="two_tower_model"
        )
        
        # –ö–æ–º–ø–∏–ª—è—Ü–∏—è
        self.model.compile(
            optimizer=keras.optimizers.Adam(learning_rate=0.001),
            loss='binary_crossentropy',
            metrics=['accuracy', keras.metrics.AUC(name='auc')]
        )
        
        print("‚úì –ú–æ–¥–µ–ª—å –ø–æ—Å—Ç—Ä–æ–µ–Ω–∞")
        self.model.summary()
    
    def train(self, epochs=10, batch_size=512):
        """–û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏"""
        print("\n–û–±—É—á–µ–Ω–∏–µ Two-Tower –º–æ–¥–µ–ª–∏...")
        
        # Callbacks
        callbacks = [
            keras.callbacks.EarlyStopping(
                monitor='val_auc',
                patience=3,
                mode='max',
                restore_best_weights=True
            ),
            keras.callbacks.ReduceLROnPlateau(
                monitor='val_loss',
                factor=0.5,
                patience=2,
                min_lr=1e-6
            )
        ]
        
        # –û–±—É—á–µ–Ω–∏–µ
        history = self.model.fit(
            x=self.X_train,
            y=self.y_train,
            validation_data=(self.X_val, self.y_val),
            epochs=epochs,
            batch_size=batch_size,
            callbacks=callbacks,
            verbose=1
        )
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∏—Å—Ç–æ—Ä–∏—é
        self.history = history.history
        
        print("‚úì –ú–æ–¥–µ–ª—å –æ–±—É—á–µ–Ω–∞")
        
        # –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è
        fig, axes = plt.subplots(1, 3, figsize=(15, 4))
        
        metrics = ['loss', 'accuracy', 'auc']
        titles = ['Loss', 'Accuracy', 'AUC']
        
        for idx, (metric, title) in enumerate(zip(metrics, titles)):
            ax = axes[idx]
            ax.plot(self.history[metric], label=f'Train {title}')
            ax.plot(self.history[f'val_{metric}'], label=f'Val {title}')
            ax.set_xlabel('Epoch')
            ax.set_ylabel(title)
            ax.set_title(f'{title} over epochs')
            ax.legend()
            ax.grid(True, alpha=0.3)
        
        plt.tight_layout()
        plt.show()
        
        return history
    
    def recommend(self, user_id, top_n=10):
        """–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ —Å –ø–æ–º–æ—â—å—é –Ω–µ–π—Ä–æ—Å–µ—Ç–∏"""
        if self.model is None:
            raise ValueError("–ú–æ–¥–µ–ª—å –Ω–µ –æ–±—É—á–µ–Ω–∞!")
        
        # –ö–æ–¥–∏—Ä—É–µ–º user_id
        try:
            encoded_user = self.user_encoder.transform([user_id])[0]
        except:
            # –ï—Å–ª–∏ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å –Ω–æ–≤—ã–π, –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –ø–æ–ø—É–ª—è—Ä–Ω—ã–µ
            return []
        
        # –ü–æ–ª—É—á–∞–µ–º –≤—Å–µ book_id (–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ)
        all_books = np.arange(len(self.book_encoder.classes_))
        
        # –°–æ–∑–¥–∞–µ–º –ø–∞—Ä—ã (user, book) –¥–ª—è –≤—Å–µ—Ö –∫–Ω–∏–≥
        user_array = np.full_like(all_books, encoded_user)
        
        # –ü—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞–µ–º —Å–∫–æ—Ä—ã
        predictions = self.model.predict(
            [user_array, all_books],
            batch_size=1024,
            verbose=0
        ).flatten()
        
        # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ —É–±—ã–≤–∞–Ω–∏—é —Å–∫–æ—Ä–∞
        top_indices = np.argsort(predictions)[::-1][:top_n]
        
        # –î–µ–∫–æ–¥–∏—Ä—É–µ–º book_id
        recommended_book_ids = self.book_encoder.inverse_transform(top_indices)
        
        return recommended_book_ids.tolist()

# ========================================================================
# –ò–ù–¢–ï–ì–†–ò–†–û–í–ê–ù–ù–ê–Ø –°–ò–°–¢–ï–ú–ê
# ========================================================================

print("\n" + "="*80)
print("–ò–ù–¢–ï–ì–†–ò–†–û–í–ê–ù–ù–ê–Ø –°–ò–°–¢–ï–ú–ê")
print("="*80)

class IntegratedRecommenderSystem:
    """–ò–Ω—Ç–µ–≥—Ä–∏—Ä–æ–≤–∞–Ω–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ —Å –Ω–µ–π—Ä–æ—Å–µ—Ç–µ–≤—ã–º –ø–æ–¥—Ö–æ–¥–æ–º"""
    
    def __init__(self):
        self.preprocessor = DataPreprocessor()
        self.hybrid_system = None
        self.neural_recommender = None
        self.model_weights = {
            'hybrid': 0.4,
            'neural': 0.4,
            'popularity': 0.2
        }
    
    def run_pipeline(self):
        """–ó–∞–ø—É—Å–∫ –≤—Å–µ–≥–æ –ø–∞–π–ø–ª–∞–π–Ω–∞"""
        print("\n" + "="*80)
        print("–ó–ê–ü–£–°–ö –ü–û–õ–ù–û–ì–û –ü–ê–ô–ü–õ–ê–ô–ù–ê")
        print("="*80)
        
        # –≠—Ç–∞–ø 1: –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö
        print("\nüìä –≠–¢–ê–ü 1: –ü–û–î–ì–û–¢–û–í–ö–ê –î–ê–ù–ù–´–•")
        self.preprocessor.prepare_all_features()
        
        # –≠—Ç–∞–ø 2: –ì–∏–±—Ä–∏–¥–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞
        print("\nü§ñ –≠–¢–ê–ü 2: –ì–ò–ë–†–ò–î–ù–ê–Ø –°–ò–°–¢–ï–ú–ê")
        self.hybrid_system = HybridRecommenderSystem(self.preprocessor)
        self.hybrid_system.train_all_models()
        
        # –≠—Ç–∞–ø 3: –ù–µ–π—Ä–æ—Å–µ—Ç–µ–≤–∞—è –º–æ–¥–µ–ª—å
        print("\nüß† –≠–¢–ê–ü 3: –ù–ï–ô–†–û–°–ï–¢–ï–í–ê–Ø –ú–û–î–ï–õ–¨")
        self.neural_recommender = NeuralRecommender(self.preprocessor)
        self.neural_recommender.prepare_data()
        self.neural_recommender.build_model()
        self.neural_recommender.train(epochs=15)
        
        # –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –≤–µ—Å–æ–≤ –º–æ–¥–µ–ª–µ–π
        print("\n‚öñÔ∏è  –≠–¢–ê–ü 4: –û–ü–¢–ò–ú–ò–ó–ê–¶–ò–Ø –í–ï–°–û–í")
        self.optimize_weights()
        
        print("\n" + "="*80)
        print("–ü–ê–ô–ü–õ–ê–ô–ù –£–°–ü–ï–®–ù–û –ó–ê–í–ï–†–®–ï–ù!")
        print("="*80)
    
    def optimize_weights(self):
        """–û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –≤–µ—Å–æ–≤ –º–æ–¥–µ–ª–µ–π –Ω–∞ –≤–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö"""
        print("–û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –≤–µ—Å–æ–≤ –º–æ–¥–µ–ª–µ–π...")
        
        # –ë–µ—Ä–µ–º –ø–æ–¥–º–Ω–æ–∂–µ—Å—Ç–≤–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π –¥–ª—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏
        val_users = self.preprocessor.test_data['user_id'].unique()[:50]
        
        best_score = 0
        best_weights = self.model_weights.copy()
        
        # –ü—Ä–æ—Å—Ç–æ–π grid search –ø–æ –≤–µ—Å–∞–º
        weights_options = [
            {'hybrid': 0.5, 'neural': 0.3, 'popularity': 0.2},
            {'hybrid': 0.4, 'neural': 0.4, 'popularity': 0.2},
            {'hybrid': 0.3, 'neural': 0.5, 'popularity': 0.2},
            {'hybrid': 0.6, 'neural': 0.2, 'popularity': 0.2},
        ]
        
        for weights in weights_options:
            self.model_weights = weights
            score = self._evaluate_weight_combo(val_users[:10])
            
            print(f"  –í–µ—Å–∞ {weights}: Score = {score:.4f}")
            
            if score > best_score:
                best_score = score
                best_weights = weights
        
        self.model_weights = best_weights
        print(f"‚úì –û–ø—Ç–∏–º–∞–ª—å–Ω—ã–µ –≤–µ—Å–∞: {best_weights} (Score: {best_score:.4f})")
    
    def _evaluate_weight_combo(self, user_ids, top_n=10):
        """–û—Ü–µ–Ω–∫–∞ –∫–æ–º–±–∏–Ω–∞—Ü–∏–∏ –≤–µ—Å–æ–≤"""
        scores = []
        
        for user_id in user_ids:
            # –ü–æ–ª—É—á–∞–µ–º —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –æ—Ç –≤—Å–µ—Ö –º–æ–¥–µ–ª–µ–π
            hybrid_rec = set(self.hybrid_system.recommend(user_id, top_n=top_n*3))
            neural_rec = set(self.neural_recommender.recommend(user_id, top_n=top_n*3))
            popularity_rec = set(self.hybrid_system.models['popularity'].head(top_n*3)['book_id'].tolist())
            
            # –û–±—ä–µ–¥–∏–Ω—è–µ–º —Å –≤–µ—Å–∞–º–∏
            combined_scores = defaultdict(float)
            
            for i, book_id in enumerate(hybrid_rec):
                combined_scores[book_id] += self.model_weights['hybrid'] * (1.0 / (i + 1))
            
            for i, book_id in enumerate(neural_rec):
                combined_scores[book_id] += self.model_weights['neural'] * (1.0 / (i + 1))
            
            for i, book_id in enumerate(popularity_rec):
                combined_scores[book_id] += self.model_weights['popularity'] * (1.0 / (i + 1))
            
            # –†–∞–Ω–∂–∏—Ä—É–µ–º
            sorted_books = sorted(combined_scores.items(), key=lambda x: x[1], reverse=True)
            recommendations = [book_id for book_id, _ in sorted_books[:top_n]]
            
            # –û—Ü–µ–Ω–∏–≤–∞–µ–º
            actual_books = set(self.preprocessor.test_data[
                self.preprocessor.test_data['user_id'] == user_id
            ]['book_id'])
            
            if actual_books:
                precision = len(set(recommendations) & actual_books) / top_n
                scores.append(precision)
        
        return np.mean(scores) if scores else 0
    
    def recommend(self, user_id, top_n=10):
        """–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –∏–∑ –∏–Ω—Ç–µ–≥—Ä–∏—Ä–æ–≤–∞–Ω–Ω–æ–π —Å–∏—Å—Ç–µ–º—ã"""
        print(f"\n–ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π –¥–ª—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è {user_id}...")
        
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Å–µ–≥–º–µ–Ω—Ç –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
        if user_id in self.hybrid_system.user_segments:
            segment = self.hybrid_system.user_segments[user_id]
            print(f"  –°–µ–≥–º–µ–Ω—Ç –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è: {segment}")
        else:
            segment = 'new_user'
            print(f"  –ù–æ–≤—ã–π –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å")
        
        # –ü–æ–ª—É—á–∞–µ–º —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –æ—Ç –≤—Å–µ—Ö –º–æ–¥–µ–ª–µ–π
        recommendations = {
            'hybrid': self.hybrid_system.recommend(user_id, top_n=top_n*2),
            'neural': self.neural_recommender.recommend(user_id, top_n=top_n*2),
            'popularity': self.hybrid_system.models['popularity'].head(top_n*2)['book_id'].tolist()
        }
        
        print(f"  –ü–æ–ª—É—á–µ–Ω–æ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π:")
        print(f"    ‚Ä¢ –ì–∏–±—Ä–∏–¥–Ω–∞—è –º–æ–¥–µ–ª—å: {len(recommendations['hybrid'])}")
        print(f"    ‚Ä¢ –ù–µ–π—Ä–æ—Å–µ—Ç–µ–≤–∞—è –º–æ–¥–µ–ª—å: {len(recommendations['neural'])}")
        print(f"    ‚Ä¢ –ü–æ–ø—É–ª—è—Ä–Ω—ã–µ: {len(recommendations['popularity'])}")
        
        # –û–±—ä–µ–¥–∏–Ω—è–µ–º —Å –≤–µ—Å–∞–º–∏
        combined_scores = defaultdict(float)
        
        for model_name, recs in recommendations.items():
            weight = self.model_weights[model_name]
            for i, book_id in enumerate(recs):
                combined_scores[book_id] += weight * (1.0 / (i + 1))
        
        # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ —Å–∫–æ—Ä—É
        sorted_books = sorted(combined_scores.items(), key=lambda x: x[1], reverse=True)
        
        # –ò—Å–∫–ª—é—á–∞–µ–º —É–∂–µ –ø—Ä–æ—á–∏—Ç–∞–Ω–Ω—ã–µ
        if user_id in self.preprocessor.train_data['user_id'].values:
            read_books = set(self.preprocessor.train_data[
                self.preprocessor.train_data['user_id'] == user_id
            ]['book_id'])
            filtered_books = [(b, s) for b, s in sorted_books if b not in read_books]
        else:
            filtered_books = sorted_books
        
        # –ë–µ—Ä–µ–º —Ç–æ–ø-N
        final_recommendations = [book_id for book_id, _ in filtered_books[:top_n]]
        
        print(f"  –ò—Ç–æ–≥–æ–≤—ã—Ö —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π: {len(final_recommendations)}")
        
        return final_recommendations
    
    def evaluate_system(self, n_users=50, top_n=10):
        """–û—Ü–µ–Ω–∫–∞ –≤—Å–µ–π —Å–∏—Å—Ç–µ–º—ã"""
        print("\n" + "="*80)
        print("–û–¶–ï–ù–ö–ê –°–ò–°–¢–ï–ú–´")
        print("="*80)
        
        # –í—ã–±–∏—Ä–∞–µ–º –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π –¥–ª—è –æ—Ü–µ–Ω–∫–∏
        test_users = self.preprocessor.test_data['user_id'].unique()[:n_users]
        
        metrics = {
            'precision': [],
            'recall': [],
            'ndcg': [],
            'coverage': set(),
            'diversity': []
        }
        
        print(f"–û—Ü–µ–Ω–∫–∞ –Ω–∞ {len(test_users)} –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è—Ö...")
        
        for user_id in tqdm(test_users, desc="–û—Ü–µ–Ω–∫–∞ —Å–∏—Å—Ç–µ–º—ã"):
            # –ü–æ–ª—É—á–∞–µ–º —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏
            recommendations = self.recommend(user_id, top_n=top_n)
            
            # –ü–æ–ª—É—á–∞–µ–º —Ä–µ–∞–ª—å–Ω—ã–µ –æ—Ü–µ–Ω–∫–∏ –∏–∑ —Ç–µ—Å—Ç–∞
            actual_books = set(self.preprocessor.test_data[
                self.preprocessor.test_data['user_id'] == user_id
            ]['book_id'])
            
            if not actual_books:
                continue
            
            # Precision
            relevant = len(set(recommendations) & actual_books)
            precision = relevant / top_n if top_n > 0 else 0
            metrics['precision'].append(precision)
            
            # Recall
            recall = relevant / len(actual_books) if len(actual_books) > 0 else 0
            metrics['recall'].append(recall)
            
            # nDCG
            dcg = 0
            for i, book_id in enumerate(recommendations, 1):
                if book_id in actual_books:
                    dcg += 1 / np.log2(i + 1)
            
            ideal_rec = min(top_n, len(actual_books))
            idcg = sum(1 / np.log2(i + 1) for i in range(1, ideal_rec + 1))
            
            ndcg = dcg / idcg if idcg > 0 else 0
            metrics['ndcg'].append(ndcg)
            
            # Coverage
            metrics['coverage'].update(recommendations)
            
            # Diversity (—Å—Ä–µ–¥–Ω–µ–µ –ø–æ–ø–∞—Ä–Ω–æ–µ –Ω–µ—Å—Ö–æ–¥—Å—Ç–≤–æ)
            if len(recommendations) > 1:
                similarities = []
                for i in range(len(recommendations)):
                    for j in range(i + 1, len(recommendations)):
                        book1 = recommendations[i]
                        book2 = recommendations[j]
                        
                        if (book1 in self.preprocessor.book_features['book_id'].values and 
                            book2 in self.preprocessor.book_features['book_id'].values):
                            idx1 = self.preprocessor.book_features[
                                self.preprocessor.book_features['book_id'] == book1
                            ].index[0]
                            idx2 = self.preprocessor.book_features[
                                self.preprocessor.book_features['book_id'] == book2
                            ].index[0]
                            
                            similarity = self.preprocessor.book_similarity_matrix[idx1][idx2]
                            similarities.append(similarity)
                
                if similarities:
                    diversity = 1 - np.mean(similarities)
                    metrics['diversity'].append(diversity)
        
        # –í—ã—á–∏—Å–ª—è–µ–º —Å—Ä–µ–¥–Ω–∏–µ –º–µ—Ç—Ä–∏–∫–∏
        avg_metrics = {
            'precision@K': np.mean(metrics['precision']) if metrics['precision'] else 0,
            'recall@K': np.mean(metrics['recall']) if metrics['recall'] else 0,
            'nDCG@K': np.mean(metrics['ndcg']) if metrics['ndcg'] else 0,
            'coverage': len(metrics['coverage']) / len(self.preprocessor.book_features) if len(self.preprocessor.book_features) > 0 else 0,
            'diversity': np.mean(metrics['diversity']) if metrics['diversity'] else 0,
            'f1_score': 0
        }
        
        # F1-score
        if avg_metrics['precision@K'] + avg_metrics['recall@K'] > 0:
            avg_metrics['f1_score'] = 2 * avg_metrics['precision@K'] * avg_metrics['recall@K'] / (
                avg_metrics['precision@K'] + avg_metrics['recall@K']
            )
        
        print("\nüìà –†–ï–ó–£–õ–¨–¢–ê–¢–´ –û–¶–ï–ù–ö–ò:")
        print("-" * 40)
        for metric, value in avg_metrics.items():
            print(f"  {metric}: {value:.4f}")
        
        # –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è
        fig, axes = plt.subplots(1, 2, figsize=(12, 5))
        
        # –ú–µ—Ç—Ä–∏–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞
        quality_metrics = ['precision@K', 'recall@K', 'nDCG@K', 'f1_score']
        quality_values = [avg_metrics[m] for m in quality_metrics]
        
        axes[0].bar(quality_metrics, quality_values, color=['#3498db', '#2ecc71', '#e74c3c', '#f39c12'])
        axes[0].set_title('–ú–µ—Ç—Ä–∏–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π')
        axes[0].set_ylabel('–ó–Ω–∞—á–µ–Ω–∏–µ')
        axes[0].grid(True, alpha=0.3)
        
        for i, v in enumerate(quality_values):
            axes[0].text(i, v + 0.01, f'{v:.3f}', ha='center', va='bottom')
        
        # –ú–µ—Ç—Ä–∏–∫–∏ —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏—è
        diversity_metrics = ['coverage', 'diversity']
        diversity_values = [avg_metrics['coverage'], avg_metrics['diversity']]
        
        axes[1].bar(diversity_metrics, diversity_values, color=['#9b59b6', '#1abc9c'])
        axes[1].set_title('–ú–µ—Ç—Ä–∏–∫–∏ —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏—è –∏ –ø–æ–∫—Ä—ã—Ç–∏—è')
        axes[1].set_ylabel('–ó–Ω–∞—á–µ–Ω–∏–µ')
        axes[1].grid(True, alpha=0.3)
        
        for i, v in enumerate(diversity_values):
            axes[1].text(i, v + 0.01, f'{v:.3f}', ha='center', va='bottom')
        
        plt.tight_layout()
        plt.show()
        
        return avg_metrics
    
    def save_pipeline(self, path='recommendation_pipeline'):
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤—Å–µ–≥–æ –ø–∞–π–ø–ª–∞–π–Ω–∞"""
        print(f"\n–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –ø–∞–π–ø–ª–∞–π–Ω–∞ –≤ {path}...")
        
        # –°–æ–∑–¥–∞–µ–º –ø–∞–ø–∫—É
        import os
        os.makedirs(path, exist_ok=True)
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã
        components = {
            'preprocessor': self.preprocessor,
            'hybrid_system': self.hybrid_system,
            'neural_recommender': self.neural_recommender,
            'model_weights': self.model_weights
        }
        
        with open(f'{path}/pipeline.pkl', 'wb') as f:
            pickle.dump(components, f)
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
        metadata = {
            'n_users': len(self.preprocessor.user_features),
            'n_books': len(self.preprocessor.book_features),
            'n_interactions': len(self.preprocessor.interaction_features),
            'model_weights': self.model_weights,
            'timestamp': pd.Timestamp.now().isoformat()
        }
        
        with open(f'{path}/metadata.json', 'w') as f:
            json.dump(metadata, f, indent=2)
        
        print("‚úì –ü–∞–π–ø–ª–∞–π–Ω —Å–æ—Ö—Ä–∞–Ω–µ–Ω")
        
        return metadata
    
    def load_pipeline(self, path='recommendation_pipeline'):
        """–ó–∞–≥—Ä—É–∑–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–Ω–æ–≥–æ –ø–∞–π–ø–ª–∞–π–Ω–∞"""
        print(f"\n–ó–∞–≥—Ä—É–∑–∫–∞ –ø–∞–π–ø–ª–∞–π–Ω–∞ –∏–∑ {path}...")
        
        with open(f'{path}/pipeline.pkl', 'rb') as f:
            components = pickle.load(f)
        
        self.preprocessor = components['preprocessor']
        self.hybrid_system = components['hybrid_system']
        self.neural_recommender = components['neural_recommender']
        self.model_weights = components['model_weights']
        
        print("‚úì –ü–∞–π–ø–ª–∞–π–Ω –∑–∞–≥—Ä—É–∂–µ–Ω")
        
        return self

# ========================================================================
# –ó–ê–ü–£–°–ö –°–ö–í–û–ó–ù–û–ì–û –ü–ê–ô–ü–õ–ê–ô–ù–ê
# ========================================================================

if __name__ == "__main__":
    print("="*80)
    print("–°–ö–í–û–ó–ù–û–ô –ü–ê–ô–ü–õ–ê–ô–ù –†–ï–ö–û–ú–ï–ù–î–ê–¢–ï–õ–¨–ù–û–ô –°–ò–°–¢–ï–ú–´")
    print("="*80)
    
    # –°–æ–∑–¥–∞–µ–º –∏ –∑–∞–ø—É—Å–∫–∞–µ–º —Å–∏—Å—Ç–µ–º—É
    system = IntegratedRecommenderSystem()
    
    try:
        # –ó–∞–ø—É—Å–∫ –ø–æ–ª–Ω–æ–≥–æ –ø–∞–π–ø–ª–∞–π–Ω–∞
        system.run_pipeline()
        
        # –û—Ü–µ–Ω–∫–∞ —Å–∏—Å—Ç–µ–º—ã
        metrics = system.evaluate_system(n_users=100, top_n=10)
        
        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –ø–∞–π–ø–ª–∞–π–Ω–∞
        metadata = system.save_pipeline()
        
        # –î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π –¥–ª—è —Ç–µ—Å—Ç–æ–≤–æ–≥–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
        print("\n" + "="*80)
        print("–î–ï–ú–û–ù–°–¢–†–ê–¶–ò–Ø –†–ï–ö–û–ú–ï–ù–î–ê–¶–ò–ô")
        print("="*80)
        
        # –í—ã–±–∏—Ä–∞–µ–º —Ç–µ—Å—Ç–æ–≤–æ–≥–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
        test_user = system.preprocessor.test_data['user_id'].iloc[0]
        
        print(f"\n–ü—Ä–∏–º–µ—Ä —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π –¥–ª—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è {test_user}:")
        recommendations = system.recommend(test_user, top_n=10)
        
        if recommendations:
            print("\n–¢–æ–ø-10 —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π:")
            for i, book_id in enumerate(recommendations, 1):
                book_info = system.preprocessor.book_features[
                    system.preprocessor.book_features['book_id'] == book_id
                ]
                
                if not book_info.empty:
                    title = book_info['title'].iloc[0]
                    authors = book_info['authors'].iloc[0]
                    avg_rating = book_info['book_avg_rating'].iloc[0]
                    
                    print(f"{i}. {title}")
                    print(f"   –ê–≤—Ç–æ—Ä—ã: {authors}")
                    print(f"   –°—Ä–µ–¥–Ω–∏–π —Ä–µ–π—Ç–∏–Ω–≥: {avg_rating:.2f}")
                    print()
        
        print("\n" + "="*80)
        print("–ü–ê–ô–ü–õ–ê–ô–ù –£–°–ü–ï–®–ù–û –ó–ê–í–ï–†–®–ï–ù!")
        print("="*80)
        
        print("\nüìä –ò–¢–û–ì–û–í–´–ï –ú–ï–¢–†–ò–ö–ò –°–ò–°–¢–ï–ú–´:")
        print("-" * 40)
        for metric, value in metrics.items():
            print(f"  {metric}: {value:.4f}")
        
        print("\nüéØ –ö–õ–Æ–ß–ï–í–´–ï –•–ê–†–ê–ö–¢–ï–†–ò–°–¢–ò–ö–ò –°–ò–°–¢–ï–ú–´:")
        print("  ‚Ä¢ –ì–∏–±—Ä–∏–¥–Ω–∞—è –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ —Å –Ω–µ—Å–∫–æ–ª—å–∫–∏–º–∏ –º–æ–¥–µ–ª—è–º–∏")
        print("  ‚Ä¢ Two-Tower –Ω–µ–π—Ä–æ—Å–µ—Ç–µ–≤–∞—è –º–æ–¥–µ–ª—å")
        print("  ‚Ä¢ –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∞—è —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π")
        print("  ‚Ä¢ –ë–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫–∞ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏ –∏ —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏—è")
        print("  ‚Ä¢ –í–æ–∑–º–æ–∂–Ω–æ—Å—Ç—å –ø–æ–≤—Ç–æ—Ä–Ω–æ–≥–æ –∑–∞–ø—É—Å–∫–∞ –Ω–∞ –Ω–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö")
        
    except Exception as e:
        print(f"\n‚úó –û—à–∏–±–∫–∞ –≤ –ø–∞–π–ø–ª–∞–π–Ω–µ: {e}")
        import traceback
        traceback.print_exc()
